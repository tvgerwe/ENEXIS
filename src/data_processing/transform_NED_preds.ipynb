{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "164bdd2f",
   "metadata": {},
   "source": [
    "NEd preds'  structure changed through pivoting the table. issue of slighlty deviating fetch_datetime stamps was dealt with.\n",
    "NaN values occur for Type 59. I deleted that from dataset, for now..\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b5154933",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows where validfrom is earlier than lastupdate: 9251\n",
      "Unique values in 'type': ['PRED_1' 'PRED_2' 'PRED_17' 'PRED_20' 'PRED_26' 'PRED_21' 'PRED_59']\n",
      "          type    volume                   validto    current_datetime  \\\n",
      "1463    PRED_1   2141122 2025-04-02 09:00:00+00:00 2025-04-02 08:17:40   \n",
      "29823  PRED_17   2799828 2025-04-02 09:00:00+00:00 2025-04-02 08:17:41   \n",
      "30373   PRED_2  11265053 2025-04-02 09:00:00+00:00 2025-04-02 08:17:41   \n",
      "1997   PRED_20    485000 2025-04-02 09:00:00+00:00 2025-04-02 08:17:42   \n",
      "11076  PRED_21    365000 2025-04-02 09:00:00+00:00 2025-04-02 08:17:43   \n",
      "36932  PRED_26    221000 2025-04-02 09:00:00+00:00 2025-04-02 08:17:43   \n",
      "1464    PRED_1   2219175 2025-04-02 10:00:00+00:00 2025-04-02 08:17:40   \n",
      "29824  PRED_17   2716766 2025-04-02 10:00:00+00:00 2025-04-02 08:17:41   \n",
      "30374   PRED_2  14442681 2025-04-02 10:00:00+00:00 2025-04-02 08:17:41   \n",
      "1998   PRED_20    485000 2025-04-02 10:00:00+00:00 2025-04-02 08:17:42   \n",
      "11077  PRED_21    365000 2025-04-02 10:00:00+00:00 2025-04-02 08:17:43   \n",
      "36933  PRED_26    221000 2025-04-02 10:00:00+00:00 2025-04-02 08:17:43   \n",
      "1465    PRED_1   2379837 2025-04-02 11:00:00+00:00 2025-04-02 08:17:40   \n",
      "29825  PRED_17   2775487 2025-04-02 11:00:00+00:00 2025-04-02 08:17:41   \n",
      "30375   PRED_2  17357228 2025-04-02 11:00:00+00:00 2025-04-02 08:17:41   \n",
      "1999   PRED_20    485000 2025-04-02 11:00:00+00:00 2025-04-02 08:17:42   \n",
      "11078  PRED_21    365000 2025-04-02 11:00:00+00:00 2025-04-02 08:17:43   \n",
      "36934  PRED_26    221000 2025-04-02 11:00:00+00:00 2025-04-02 08:17:43   \n",
      "1466    PRED_1   2802929 2025-04-02 12:00:00+00:00 2025-04-02 08:17:40   \n",
      "29826  PRED_17   2580169 2025-04-02 12:00:00+00:00 2025-04-02 08:17:41   \n",
      "\n",
      "             fetch_moment  \n",
      "1463  2025-04-02 08:17:43  \n",
      "29823 2025-04-02 08:17:43  \n",
      "30373 2025-04-02 08:17:43  \n",
      "1997  2025-04-02 08:17:43  \n",
      "11076 2025-04-02 08:17:43  \n",
      "36932 2025-04-02 08:17:43  \n",
      "1464  2025-04-02 08:17:43  \n",
      "29824 2025-04-02 08:17:43  \n",
      "30374 2025-04-02 08:17:43  \n",
      "1998  2025-04-02 08:17:43  \n",
      "11077 2025-04-02 08:17:43  \n",
      "36933 2025-04-02 08:17:43  \n",
      "1465  2025-04-02 08:17:43  \n",
      "29825 2025-04-02 08:17:43  \n",
      "30375 2025-04-02 08:17:43  \n",
      "1999  2025-04-02 08:17:43  \n",
      "11078 2025-04-02 08:17:43  \n",
      "36934 2025-04-02 08:17:43  \n",
      "1466  2025-04-02 08:17:43  \n",
      "29826 2025-04-02 08:17:43  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Connect to the SQLite database\n",
    "db_path = '../data/WARP.db'\n",
    "conn = sqlite3.connect(db_path)\n",
    "\n",
    "# Read the raw_NED_preds table into a DataFrame\n",
    "df_NED_preds_processed = pd.read_sql_query(\"SELECT * FROM raw_NED_preds\", conn)\n",
    "\n",
    "# Close the database connection\n",
    "conn.close()\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Zet datumkolommen om naar datetime\n",
    "df_NED_preds_processed['validfrom'] = pd.to_datetime(df_NED_preds_processed['validfrom'])\n",
    "df_NED_preds_processed['validto'] = pd.to_datetime(df_NED_preds_processed['validto'])\n",
    "df_NED_preds_processed['lastupdate'] = pd.to_datetime(df_NED_preds_processed['lastupdate'])\n",
    "df_NED_preds_processed['current_datetime'] = pd.to_datetime(df_NED_preds_processed['current_datetime'])\n",
    "\n",
    "# Zet 'lastupdate' als index en zorg dat deze ook datetime is\n",
    "#df_NED_preds_processed.set_index('lastupdate', inplace=True)\n",
    "#df_NED_preds_processed.index = pd.to_datetime(df_NED_preds_processed.index)\n",
    "\n",
    "\n",
    "\n",
    "# Calculate rows where validfrom is earlier than lastupdate\n",
    "mask = df_NED_preds_processed['validfrom'] >= df_NED_preds_processed['lastupdate']\n",
    "# Keep only rows that match this condition, thereby dropping the rest\n",
    "df_NED_preds_processed = df_NED_preds_processed[mask]\n",
    "print(f\"Number of rows where validfrom is earlier than lastupdate: {(~mask).sum()}\")\n",
    "# Sort by current_datetime to ensure correct grouping\n",
    "df_NED_preds_processed = df_NED_preds_processed.sort_values('current_datetime')\n",
    "\n",
    "\n",
    "# dealing with fact that current_datetime (the moment of fetching the data) is not always the same across NED Types fetched 'simultaneously':\n",
    "# Group current_datetime values within 1 minute and assign the most recent (max) value in each group as 'fetch_moment':\n",
    "df_NED_preds_processed['fetch_moment'] = (\n",
    "    df_NED_preds_processed['current_datetime']\n",
    "    .diff().gt(pd.Timedelta('1min')).cumsum()\n",
    ")\n",
    "# Map each group to its max current_datetime\n",
    "fetch_moment_map = df_NED_preds_processed.groupby('fetch_moment')['current_datetime'].transform('max')\n",
    "fetch_moment_map = pd.to_datetime(fetch_moment_map)\n",
    "df_NED_preds_processed['fetch_moment'] = fetch_moment_map\n",
    "\n",
    "\n",
    "# Verwijder irrelevante kolommen\n",
    "df_NED_preds_processed = df_NED_preds_processed.drop(columns=[\n",
    "    '@id', '@type', 'id', 'point', 'granularity', 'granularitytimezone', 'activity', 'classification', 'capacity','percentage','emission','emissionfactor','validfrom','lastupdate'])\n",
    "\n",
    "\n",
    "# Verander de naam van de kolom 'type' naar 'PRED_Type' for clarity\n",
    "df_NED_preds_processed['type'] = df_NED_preds_processed['type'].str.replace('/v1/types/', 'PRED_')\n",
    "print(\"Unique values in 'type':\", df_NED_preds_processed['type'].unique())\n",
    "\n",
    "# Sort the DataFrame by the index,  by 'validto', asnd by 'type'\n",
    "\n",
    "df_NED_preds_processed = df_NED_preds_processed.sort_values(by=['fetch_moment','validto', 'type'])\n",
    "print(df_NED_preds_processed.head(20))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fbba70e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         fetch_moment                   validto  Volume_PRED_1  \\\n",
      "0 2025-04-02 08:17:43 2025-04-02 09:00:00+00:00      2141122.0   \n",
      "1 2025-04-02 08:17:43 2025-04-02 10:00:00+00:00      2219175.0   \n",
      "2 2025-04-02 08:17:43 2025-04-02 11:00:00+00:00      2379837.0   \n",
      "3 2025-04-02 08:17:43 2025-04-02 12:00:00+00:00      2802929.0   \n",
      "4 2025-04-02 08:17:43 2025-04-02 13:00:00+00:00      3373530.0   \n",
      "\n",
      "   Volume_PRED_17  Volume_PRED_2  Volume_PRED_20  \n",
      "0       2799828.0     11265053.0        485000.0  \n",
      "1       2716766.0     14442681.0        485000.0  \n",
      "2       2775487.0     17357228.0        485000.0  \n",
      "3       2580169.0     18601126.0        485000.0  \n",
      "4       2731686.0     18169871.0        485000.0  \n",
      "Number of records in pivoted_NED_preds: 22161\n"
     ]
    }
   ],
   "source": [
    "# Pivot the table\n",
    "pivoted_NED_preds = df_NED_preds_processed.pivot_table(\n",
    "    index=['fetch_moment', 'validto'],  # keep these as index\n",
    "    columns='type',                   # columns become unique values from 'type'\n",
    "    values='volume',                  # values to fill in the new columns\n",
    "    aggfunc='first'                   # if duplicates exist, take the first\n",
    ")\n",
    "\n",
    "# Optional: flatten column names and rename to match desired output\n",
    "pivoted_NED_preds.columns = [f'Volume_{col}' for col in pivoted_NED_preds.columns]\n",
    "\n",
    "\n",
    "# To avoid tuple-as-single-column issue:\n",
    "pivoted_NED_preds = pivoted_NED_preds.reset_index()\n",
    "\n",
    "# Remove the 'Volume_PRED_59' column from the pivoted DataFrame\n",
    "pivoted_NED_preds = pivoted_NED_preds.drop(columns=['Volume_PRED_59'])\n",
    "# Remove the 'Volume_PRED_21' and 'Volume_PRED_26' columns from the pivoted DataFrame, as theire values are constant\n",
    "# and do not provide additional information\n",
    "pivoted_NED_preds = pivoted_NED_preds.drop(columns=['Volume_PRED_21', 'Volume_PRED_26'])\n",
    "\n",
    "print(pivoted_NED_preds.head())\n",
    "print(\"Number of records in pivoted_NED_preds:\", len(pivoted_NED_preds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b8432783",
   "metadata": {
    "vscode": {
     "languageId": "powershell"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of records in pivoted_NED_preds: 22078\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Print rows with missing values in any of the columns\n",
    "missing_rows = pivoted_NED_preds[pivoted_NED_preds.isnull().any(axis=1)]\n",
    "# Remove rows with missing values from the pivoted DataFrame\n",
    "pivoted_NED_preds = pivoted_NED_preds.dropna()\n",
    "print(\"Number of records in pivoted_NED_preds:\", len(pivoted_NED_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8395c517",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No missing dates in fetch_moment.\n"
     ]
    }
   ],
   "source": [
    "# checking for completeness of dataset\n",
    "# Extract only date part from fetch_moment\n",
    "fetch_dates = pivoted_NED_preds['fetch_moment'].dt.date\n",
    "\n",
    "# Create full range of dates between min and max\n",
    "date_range = pd.date_range(\n",
    "    start=fetch_dates.min(),\n",
    "    end=fetch_dates.max(),\n",
    "    freq='D'\n",
    ").date  # convert to plain date\n",
    "\n",
    "# Find missing days\n",
    "missing_dates = [d for d in date_range if d not in set(fetch_dates)]\n",
    "\n",
    "# Print results\n",
    "\n",
    "if not missing_dates:\n",
    "    print(\"No missing dates in fetch_moment.\")\n",
    "else:\n",
    "    print(\"Missing dates in fetch_moment:\", missing_dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2e462aef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fetch_moment        datetime64[ns, UTC]\n",
      "validto             datetime64[ns, UTC]\n",
      "Wind_Vol                        float64\n",
      "WindOffshore_Vol                float64\n",
      "Solar_Vol                       float64\n",
      "Nuclear_Vol                     float64\n",
      "dtype: object\n",
      "               fetch_moment                   validto   Wind_Vol  \\\n",
      "0 2025-04-02 08:17:43+00:00 2025-04-02 09:00:00+00:00  2141122.0   \n",
      "1 2025-04-02 08:17:43+00:00 2025-04-02 10:00:00+00:00  2219175.0   \n",
      "2 2025-04-02 08:17:43+00:00 2025-04-02 11:00:00+00:00  2379837.0   \n",
      "3 2025-04-02 08:17:43+00:00 2025-04-02 12:00:00+00:00  2802929.0   \n",
      "4 2025-04-02 08:17:43+00:00 2025-04-02 13:00:00+00:00  3373530.0   \n",
      "\n",
      "   WindOffshore_Vol   Solar_Vol  Nuclear_Vol  \n",
      "0         2799828.0  11265053.0     485000.0  \n",
      "1         2716766.0  14442681.0     485000.0  \n",
      "2         2775487.0  17357228.0     485000.0  \n",
      "3         2580169.0  18601126.0     485000.0  \n",
      "4         2731686.0  18169871.0     485000.0  \n"
     ]
    }
   ],
   "source": [
    "# ✅ Typecasting van datumkolommen\n",
    "pivoted_NED_preds[\"validto\"] = pd.to_datetime(pivoted_NED_preds[\"validto\"], utc=True)\n",
    "pivoted_NED_preds[\"fetch_moment\"] = pd.to_datetime(pivoted_NED_preds[\"fetch_moment\"], utc=True)\n",
    "\n",
    "# ✅ Kolomnamen hernoemen\n",
    "rename_map = {\n",
    "    \"Volume_PRED_1\": \"Wind_Vol\",\n",
    "    \"Volume_PRED_17\": \"WindOffshore_Vol\",\n",
    "    \"Volume_PRED_2\": \"Solar_Vol\",\n",
    "    \"Volume_PRED_20\": \"Nuclear_Vol\"\n",
    "}\n",
    "pivoted_NED_preds = pivoted_NED_preds.rename(columns=rename_map)\n",
    "\n",
    "# ✅ Preview check\n",
    "print(pivoted_NED_preds.dtypes)\n",
    "print(pivoted_NED_preds.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "882eb0e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the processed DataFrame to a new SQLite database\n",
    "output_db_path = '../data/WARP.db'\n",
    "conn = sqlite3.connect(output_db_path)\n",
    "pivoted_NED_preds.to_sql('processed_NED_preds', conn, if_exists='replace', index=True)\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "522cf72d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.10.13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
