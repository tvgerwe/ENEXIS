{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "384b4d96",
   "metadata": {},
   "source": [
    "# Folder structuur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c2e44b78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÅ ENEXIS/\n",
      "    üìÑ enexis_master_warp.csv\n",
      "    üìÑ .cache.sqlite\n",
      "    üìÑ import pandas as pd v2.py\n",
      "    üìÑ best_prophet_model.joblib\n",
      "    üìÑ fetch_entsoe_data.py\n",
      "    üìÑ .DS_Store\n",
      "    üìÑ hourly_rmse_table.html\n",
      "    üìÑ import pandas as pd v3.py\n",
      "    üìÑ requirements.txt\n",
      "    üìÑ fetch_entsoe_data_v.py\n",
      "    üìÑ transform_entsoe.csv\n",
      "    üìÑ electricity_data_nl_2022_2024_hourly.csv\n",
      "    üìÑ electricity_data_nl_2022_2024.csv\n",
      "    üìÑ validation_results.csv\n",
      "    üìÑ test.py\n",
      "    üìÑ output.csv\n",
      "    üìÑ python code ENTSO-E v2.ipynb\n",
      "    üìÑ README.md\n",
      "    üìÑ naive_forecast_interactive.html\n",
      "    üìÑ pipeline_flow.png\n",
      "    üìÑ python code ENTSO-E.ipynb\n",
      "    üìÑ debug_.py\n",
      "    üìÑ .gitignore\n",
      "    üìÑ pip install entsoe-py.py\n",
      "    üìÑ actual_total_load_nl_2022_2024.csv\n",
      "    üìÑ naive_forecast_validation.csv\n",
      "    üìÑ electricity_data_nl_2022_2025_utc.csv\n",
      "    üìÑ historical_weekahead_forecasts.csv\n",
      "    üìÑ import pandas as pd.py\n",
      "    üìÑ .gitattributes\n",
      "    üìÑ structuur.txt\n",
      "    üìÑ raw_entsoe.csv\n",
      "    üìÑ electricity_data_nl_2022_2024 kolommen.csv\n",
      "    üìÑ electricity_data_nl_2022_2025_hourly_flow.csv\n",
      "    üìÑ import pandas as pd v4.py\n",
      "    üìÑ test.ipynb\n",
      "    üìÑ step_by_step.md\n",
      "    üìÑ electricity_data_nl_2022_2025_hourly.csv\n",
      "    üìÑ cleaned_output_2025-05-05.txt\n",
      "    üìÅ tests/\n",
      "        üìÑ evaluate_models.ipynb\n",
      "        üìÑ test_raw_ned_obs.ipynb\n",
      "        üìÑ test_transform_ned_obs.ipynb\n",
      "        üìÑ test_sarimax.ipynb\n",
      "        üìÑ Test_trainingset.ipynb\n",
      "        üìÑ test_data.py\n",
      "        üìÑ test_pipeline.ipynb\n",
      "        üìÑ test_models.ipynb\n",
      "    üìÅ the/\n",
      "        üìÑ pyvenv.cfg\n",
      "        üìÅ bin/\n",
      "            üìÑ pip3.9\n",
      "            üìÑ Activate.ps1\n",
      "            üìÑ python3\n",
      "            üìÑ python\n",
      "            üìÑ pip3\n",
      "            üìÑ activate.fish\n",
      "            üìÑ python3.9\n",
      "            üìÑ pip\n",
      "            üìÑ activate\n",
      "            üìÑ activate.csh\n",
      "        üìÅ lib/\n",
      "        üìÅ share/\n",
      "    üìÅ docs/\n",
      "        üìÑ data_dictionary.md\n",
      "        üìÑ price pred oxygent.png\n",
      "        üìÑ readme_MVP.md\n",
      "        üìÑ index.md\n",
      "        üìÑ modeling_notes.md\n",
      "        üìÑ WARP_wireframe.png\n",
      "    üìÅ workspaces/\n",
      "        üìÅ twan/\n",
      "            üìÑ proces_prices-NEW.ipynb\n",
      "            üìÑ .cache.sqlite\n",
      "            üìÑ merged_data_price_preds.csv\n",
      "            üìÑ proces_prices.ipynb\n",
      "            üìÑ get_prices_NEW.ipynb\n",
      "            üìÑ get_prices.ipynb\n",
      "            üìÑ index.md\n",
      "            üìÑ pred_accuracy.ipynb\n",
      "            üìÑ get_prices2.ipynb\n",
      "        üìÅ sharell/\n",
      "            üìÑ Code merge datasets, plots, regression.ipynb\n",
      "            üìÑ enexis_master_warp.csv\n",
      "            üìÑ correlation_matrix.html\n",
      "            üìÑ entsoe_exploration.py\n",
      "            üìÑ random forest model.py\n",
      "            üìÑ entso_load forecast.py\n",
      "            üìÑ # Pad naar de SQLite-database.py\n",
      "            üìÑ entso_load forecast attempt 2.py\n",
      "            üìÑ random forest model 3.ipynb\n",
      "            üìÑ enexis_mastepr_predictions.csv\n",
      "            üìÑ index.md\n",
      "            üìÑ entsoe_ingest.py\n",
      "            üìÑ Weather_data.ipynb\n",
      "            üìÑ entsoe_2022-2025_load\n",
      "            üìÑ ENTSO-E code plots.ipynb\n",
      "            üìÑ random forest model 2.ipynb\n",
      "            üìÑ entsoe_test.py\n",
      "            üìÑ weekahead_forecast_load_2025.csv\n",
      "        üìÅ sandeep/\n",
      "            üìÑ ned-plots 2.ipynb\n",
      "            üìÑ environment.yml\n",
      "            üìÑ ned-plots.ipynb\n",
      "            üìÑ index.md\n",
      "            üìÑ 2021 01 05 - Enexis Energy - v1.ipynb\n",
      "        üìÅ redouan/\n",
      "            üìÑ GUI_ENERGY_PRICES_202501010000-202601010000.csv\n",
      "            üìÑ index.md\n",
      "    üìÅ #/\n",
      "        üìÑ pyvenv.cfg\n",
      "        üìÅ bin/\n",
      "            üìÑ pip3.9\n",
      "            üìÑ Activate.ps1\n",
      "            üìÑ python3\n",
      "            üìÑ python\n",
      "            üìÑ pip3\n",
      "            üìÑ activate.fish\n",
      "            üìÑ python3.9\n",
      "            üìÑ pip\n",
      "            üìÑ activate\n",
      "            üìÑ activate.csh\n",
      "        üìÅ lib/\n",
      "    üìÅ .history/\n",
      "        üìÑ README_20250416111233.md\n",
      "        üìÑ README_20250416111343.md\n",
      "        üìÑ README_20250416110720.md\n",
      "        üìÑ .gitignore_20250326100045\n",
      "        üìÑ README_20250416111303.md\n",
      "        üìÑ README_20250416111408.md\n",
      "        üìÑ README_20250416111257.md\n",
      "        üìÑ .gitignore_20250326101525\n",
      "        üìÑ README_20250416110938.md\n",
      "        üìÑ test_20250228090944.py\n",
      "        üìÑ test_20250318073940.py\n",
      "        üìÑ README_20250414152007.md\n",
      "        üìÑ README_20250416111432.md\n",
      "        üìÑ README_20250416111447.md\n",
      "        üìÑ .gitignore_20250407110339\n",
      "        üìÑ .gitignore_20250509201151\n",
      "        üìÑ test_20250318073945.py\n",
      "        üìÑ test_20250318073937.py\n",
      "        üìÑ README_20250416110728.md\n",
      "        üìÑ README_20250416111444.md\n",
      "        üìÑ .gitignore_20250407110310\n",
      "        üìÑ README_20250416111435.md\n",
      "        üìÑ .gitignore_20250326101227\n",
      "        üìÑ .gitignore_20250509201836\n",
      "        üìÑ README_20250416111441.md\n",
      "        üìÑ README_20250416111430.md\n",
      "        üìÑ README_20250416110729.md\n",
      "        üìÑ README_20250416110805.md\n",
      "        üìÑ README_20250416111320.md\n",
      "        üìÑ .gitignore_20250326101526\n",
      "        üìÑ README_20250416110733.md\n",
      "        üìÑ README_20250416110746.md\n",
      "        üìÅ docs/\n",
      "            üìÑ readme_MVP_20250414100707.md\n",
      "            üìÑ readme_MVP_20250414100931.md\n",
      "            üìÑ readme_MVP_20250414165411.md\n",
      "            üìÑ readme_MVP_20250416111609.md\n",
      "            üìÑ readme_MVP_20250414100703.md\n",
      "            üìÑ readme_MVP_20250414100737.md\n",
      "            üìÑ readme_MVP_20250414165415.md\n",
      "            üìÑ readme_MVP_20250417081105.md\n",
      "            üìÑ readme_MVP_20250414100821.md\n",
      "            üìÑ readme_MVP_20250417092835.md\n",
      "            üìÑ readme_MVP_20250414165244.md\n",
      "            üìÑ readme_MVP_20250416111702.md\n",
      "            üìÑ readme_MVP_20250416111613.md\n",
      "            üìÑ readme_MVP_20250414100748.md\n",
      "            üìÑ readme_MVP_20250416111602.md\n",
      "            üìÑ readme_MVP_20250416111804.md\n",
      "            üìÑ readme_MVP_20250416111813.md\n",
      "            üìÑ readme_MVP_20250416111650.md\n",
      "            üìÑ readme_MVP_20250416111751.md\n",
      "            üìÑ readme_MVP_20250414165242.md\n",
      "            üìÑ readme_MVP_20250416111654.md\n",
      "            üìÑ readme_MVP_20250416111822.md\n",
      "            üìÑ readme_MVP_20250414165418.md\n",
      "            üìÑ readme_MVP_20250416111806.md\n",
      "            üìÑ readme_MVP_20250414165247.md\n",
      "            üìÑ readme_MVP_20250416111812.md\n",
      "            üìÑ readme_MVP_20250416111750.md\n",
      "            üìÑ readme_MVP_20250414100933.md\n",
      "            üìÑ readme_MVP_20250414100922.md\n",
      "            üìÑ readme_MVP_20250414100745.md\n",
      "            üìÑ readme_MVP_20250414100751.md\n",
      "            üìÑ readme_MVP_20250416111808.md\n",
      "        üìÅ workspaces/\n",
      "        üìÅ src/\n",
      "    üìÅ virtual/\n",
      "        üìÑ pyvenv.cfg\n",
      "        üìÅ bin/\n",
      "            üìÑ pip3.9\n",
      "            üìÑ Activate.ps1\n",
      "            üìÑ python3\n",
      "            üìÑ python\n",
      "            üìÑ pip3\n",
      "            üìÑ activate.fish\n",
      "            üìÑ python3.9\n",
      "            üìÑ pip\n",
      "            üìÑ activate\n",
      "            üìÑ activate.csh\n",
      "        üìÅ lib/\n",
      "    üìÅ .github/\n",
      "        üìÅ workflows/\n",
      "            üìÑ index.md\n",
      "            üìÑ ci.yml\n",
      "    üìÅ environment/\n",
      "        üìÑ pyvenv.cfg\n",
      "        üìÅ bin/\n",
      "            üìÑ pip3.9\n",
      "            üìÑ Activate.ps1\n",
      "            üìÑ python3\n",
      "            üìÑ python\n",
      "            üìÑ pip3\n",
      "            üìÑ activate.fish\n",
      "            üìÑ python3.9\n",
      "            üìÑ pip\n",
      "            üìÑ activate\n",
      "            üìÑ activate.csh\n",
      "        üìÅ lib/\n",
      "    üìÅ .git/\n",
      "        üìÑ ORIG_HEAD\n",
      "        üìÑ config\n",
      "        üìÑ HEAD\n",
      "        üìÑ description\n",
      "        üìÑ index\n",
      "        üìÑ .MERGE_MSG.swp\n",
      "        üìÑ packed-refs\n",
      "        üìÑ COMMIT_EDITMSG\n",
      "        üìÑ FETCH_HEAD\n",
      "        üìÅ objects/\n",
      "        üìÅ info/\n",
      "            üìÑ exclude\n",
      "        üìÅ logs/\n",
      "            üìÑ HEAD\n",
      "        üìÅ hooks/\n",
      "            üìÑ commit-msg.sample\n",
      "            üìÑ pre-rebase.sample\n",
      "            üìÑ pre-commit.sample\n",
      "            üìÑ applypatch-msg.sample\n",
      "            üìÑ fsmonitor-watchman.sample\n",
      "            üìÑ pre-receive.sample\n",
      "            üìÑ prepare-commit-msg.sample\n",
      "            üìÑ post-update.sample\n",
      "            üìÑ pre-merge-commit.sample\n",
      "            üìÑ pre-applypatch.sample\n",
      "            üìÑ pre-push.sample\n",
      "            üìÑ update.sample\n",
      "            üìÑ push-to-checkout.sample\n",
      "        üìÅ refs/\n",
      "    üìÅ create/\n",
      "        üìÑ pyvenv.cfg\n",
      "        üìÅ bin/\n",
      "            üìÑ pip3.9\n",
      "            üìÑ Activate.ps1\n",
      "            üìÑ python3\n",
      "            üìÑ python\n",
      "            üìÑ pip3\n",
      "            üìÑ activate.fish\n",
      "            üìÑ python3.9\n",
      "            üìÑ pip\n",
      "            üìÑ activate\n",
      "            üìÑ activate.csh\n",
      "        üìÅ lib/\n",
      "    üìÅ flows/\n",
      "        üìÑ full_pipeline_flow.py\n",
      "        üìÑ README.md\n",
      "    üìÅ src/\n",
      "        üìÑ .DS_Store\n",
      "        üìÑ __init__.py\n",
      "        üìÅ visualization/\n",
      "            üìÑ results.py\n",
      "            üìÑ __init__.py\n",
      "        üìÅ core/\n",
      "            üìÑ __init__.py\n",
      "            üìÑ experiment.py\n",
      "            üìÑ logging_manager.py\n",
      "            üìÑ data_manager.py\n",
      "        üìÅ config/\n",
      "            üìÑ experiment_config.py\n",
      "            üìÑ experiment_config.yaml\n",
      "            üìÑ config.py\n",
      "            üìÑ database_config.py\n",
      "            üìÑ config.json\n",
      "            üìÑ __init__.py\n",
      "            üìÑ house_style.py\n",
      "            üìÑ best_sarimax_params.json\n",
      "        üìÅ data_processing/\n",
      "            üìÑ transform_NED_obs_2.ipynb\n",
      "            üìÑ transform_meteo_obs.py\n",
      "            üìÑ transform_weather_preds.ipynb\n",
      "            üìÑ entsoe_dataprocessing.py\n",
      "            üìÑ transform_open_meteo_preds.ipynb\n",
      "            üìÑ transform_meteo_preds_history.py\n",
      "            üìÑ transform_ned.py\n",
      "            üìÑ __init__.py\n",
      "            üìÑ transform_NED_preds.ipynb\n",
      "            üìÑ transform_meteo_preds.py\n",
      "            üìÑ split.py\n",
      "            üìÑ feature_eng.py\n",
      "            üìÑ transform_weather_obs.ipynb\n",
      "            üìÑ transform_meteo_forecast_now.py\n",
      "        üìÅ utils/\n",
      "            üìÑ log_rolling_window_to_sqlite.py\n",
      "            üìÑ auto_arima_optimizer.py\n",
      "            üìÑ build_training_set_draft.py\n",
      "            üìÑ log_rmse_to_sqlite.py\n",
      "            üìÑ logger.py\n",
      "            üìÑ validation_utils.py\n",
      "            üìÑ build_training_set.py\n",
      "        üìÅ models/\n",
      "            üìÑ warp-prophet-model.py\n",
      "            üìÑ XGBoost_obs_NED_incl.ipynb\n",
      "            üìÑ benchmark_model_diagnostics.ipynb\n",
      "            üìÑ XGBoost_obs&preds_old.ipynb\n",
      "            üìÑ time_df-code_extended.ipynb\n",
      "            üìÑ correlation_matrix_obs.ipynb\n",
      "            üìÑ train-models.py\n",
      "            üìÑ sarimax.py\n",
      "            üìÑ model_results_log.csv\n",
      "            üìÑ __init__.py\n",
      "            üìÑ factory.py\n",
      "            üìÑ naive.py\n",
      "            üìÑ sarimax_model.py\n",
      "            üìÑ model_predictions_log.csv\n",
      "            üìÑ naive_model_diagnostics.ipynb\n",
      "            üìÑ naive_model.py\n",
      "            üìÑ Timeseries.ipynb\n",
      "            üìÑ warp-prophet-model-forecast.py\n",
      "            üìÑ XGBoost_evaluation.ipynb\n",
      "            üìÑ warp-prophet-model.ipynb\n",
      "            üìÑ Benchmark_model.py\n",
      "            üìÑ naive_benchmark_timeseries.ipynb\n",
      "        üìÅ __pycache__/\n",
      "            üìÑ __init__.cpython-311.pyc\n",
      "            üìÑ __init__.cpython-310.pyc\n",
      "        üìÅ load-data/\n",
      "            üìÑ load_data 2.py\n",
      "            üìÑ .cache.sqlite\n",
      "            üìÑ ned-api-get-yrs-data-0-type.ipynb\n",
      "            üìÑ NED-api-PREDs-7Types-thinclient.ipynb\n",
      "            üìÑ ned-api-get-yrs-data-0-type 2.ipynb\n",
      "            üìÑ load_data.py\n",
      "            üìÑ ned-api-get-all-types_NEW.ipynb\n",
      "            üìÑ ned-api-descriptive-analysis.ipynb\n",
      "            üìÑ ned-api-get-all-types_OLD.ipynb\n",
      "        üìÅ evaluation/\n",
      "            üìÑ validator.py\n",
      "            üìÑ metrics.py\n",
      "            üìÑ __init__.py\n",
      "        üìÅ data/\n",
      "            üìÑ optimization_logs.db\n",
      "            üìÑ WARP.db\n",
      "            üìÑ logs.db\n",
      "            üìÑ warp-csv-dataset.csv\n",
      "            üìÑ warp-csv-prediction-dataset.csv\n",
      "        üìÅ notebooks/\n",
      "            üìÑ index.md\n",
      "        üìÅ data_master/\n",
      "            üìÑ merge_obs_preds.py\n",
      "            üìÑ build_master_predictions.py\n",
      "            üìÑ build_master_observed.py\n",
      "            üìÑ check_master_WARP.ipynb\n",
      "            üìÑ master_merging\n",
      "        üìÅ data_ingestion/\n",
      "            üìÑ API_open_meteo_preds.ipynb\n",
      "            üìÑ .cache.sqlite\n",
      "            üìÑ ingest_meteo_historical_pred.py\n",
      "            üìÑ API_open_meteo_preds_readCSV.ipynb\n",
      "            üìÑ entsoe_load.py\n",
      "            üìÑ ingest_open-meteo_obs.py\n",
      "            üìÑ warp-plots.ipynb\n",
      "            üìÑ ingest_ned.py\n",
      "            üìÑ NED_preds_API_to_db.ipynb\n",
      "            üìÑ ingest_date.py\n",
      "            üìÑ NED.py\n",
      "            üìÑ ingest_meteo_obs.py\n",
      "            üìÑ ingest_entsoe.py\n",
      "            üìÑ API_open_meteo_historical.ipynb\n",
      "            üìÑ ingest_meteo_forecast_now.py\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def print_tree(startpath, max_depth=3):\n",
    "    startpath = os.path.abspath(startpath)\n",
    "    for root, dirs, files in os.walk(startpath):\n",
    "        depth = root.replace(startpath, '').count(os.sep)\n",
    "        if depth >= max_depth:\n",
    "            dirs[:] = []  # stop met verder afdalen\n",
    "            continue\n",
    "        indent = ' ' * 4 * depth\n",
    "        print(f\"{indent}üìÅ {os.path.basename(root)}/\")\n",
    "        for f in files:\n",
    "            print(f\"{indent}    üìÑ {f}\")\n",
    "\n",
    "# Pas dit pad aan indien nodig ‚Äî bijvoorbeeld \".\" of \"../\"\n",
    "print_tree(\"..\", max_depth=3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "995c527d",
   "metadata": {},
   "source": [
    "# Code base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "15606154",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìÅ src/data_ingestion (13 bestanden):\n",
      "  - src/data_ingestion/API_open_meteo_preds.ipynb\n",
      "  - src/data_ingestion/ingest_meteo_historical_pred.py\n",
      "  - src/data_ingestion/API_open_meteo_preds_readCSV.ipynb\n",
      "  - src/data_ingestion/entsoe_load.py\n",
      "  - src/data_ingestion/ingest_open-meteo_obs.py\n",
      "  - src/data_ingestion/warp-plots.ipynb\n",
      "  - src/data_ingestion/ingest_ned.py\n",
      "  - src/data_ingestion/NED_preds_API_to_db.ipynb\n",
      "  - src/data_ingestion/ingest_date.py\n",
      "  - src/data_ingestion/NED.py\n",
      "  - src/data_ingestion/ingest_meteo_obs.py\n",
      "  - src/data_ingestion/API_open_meteo_historical.ipynb\n",
      "  - src/data_ingestion/ingest_meteo_forecast_now.py\n",
      "\n",
      "üìÅ src/data_processing (14 bestanden):\n",
      "  - src/data_processing/transform_NED_obs_2.ipynb\n",
      "  - src/data_processing/transform_meteo_obs.py\n",
      "  - src/data_processing/transform_weather_preds.ipynb\n",
      "  - src/data_processing/entsoe_dataprocessing.py\n",
      "  - src/data_processing/transform_open_meteo_preds.ipynb\n",
      "  - src/data_processing/transform_meteo_preds_history.py\n",
      "  - src/data_processing/transform_ned.py\n",
      "  - src/data_processing/__init__.py\n",
      "  - src/data_processing/transform_NED_preds.ipynb\n",
      "  - src/data_processing/transform_meteo_preds.py\n",
      "  - src/data_processing/split.py\n",
      "  - src/data_processing/feature_eng.py\n",
      "  - src/data_processing/transform_weather_obs.ipynb\n",
      "  - src/data_processing/transform_meteo_forecast_now.py\n",
      "\n",
      "üìÅ src/data_master (4 bestanden):\n",
      "  - src/data_master/merge_obs_preds.py\n",
      "  - src/data_master/build_master_predictions.py\n",
      "  - src/data_master/build_master_observed.py\n",
      "  - src/data_master/check_master_WARP.ipynb\n",
      "\n",
      "üìÅ src/models (23 bestanden):\n",
      "  - src/models/warp-prophet-model.py\n",
      "  - src/models/XGBoost_obs_NED_incl.ipynb\n",
      "  - src/models/benchmark_model_diagnostics.ipynb\n",
      "  - src/models/XGBoost_obs&preds_old.ipynb\n",
      "  - src/models/time_df-code_extended.ipynb\n",
      "  - src/models/correlation_matrix_obs.ipynb\n",
      "  - src/models/train-models.py\n",
      "  - src/models/sarimax.py\n",
      "  - src/models/__init__.py\n",
      "  - src/models/factory.py\n",
      "  - src/models/naive.py\n",
      "  - src/models/sarimax_model.py\n",
      "  - src/models/naive_model_diagnostics.ipynb\n",
      "  - src/models/naive_model.py\n",
      "  - src/models/Timeseries.ipynb\n",
      "  - src/models/warp-prophet-model-forecast.py\n",
      "  - src/models/XGBoost_evaluation.ipynb\n",
      "  - src/models/warp-prophet-model.ipynb\n",
      "  - src/models/Benchmark_model.py\n",
      "  - src/models/naive_benchmark_timeseries.ipynb\n",
      "  - src/models/ned/ned-gen-prediction.ipynb\n",
      "  - src/models/ned/ned-gen-merge-data-prediction-models.ipynb\n",
      "  - src/models/model_run_results/warp-model-run-results-view.py\n",
      "\n",
      "üìÅ src/utils (7 bestanden):\n",
      "  - src/utils/log_rolling_window_to_sqlite.py\n",
      "  - src/utils/auto_arima_optimizer.py\n",
      "  - src/utils/build_training_set_draft.py\n",
      "  - src/utils/log_rmse_to_sqlite.py\n",
      "  - src/utils/logger.py\n",
      "  - src/utils/validation_utils.py\n",
      "  - src/utils/build_training_set.py\n",
      "\n",
      "üìÅ flows (1 bestanden):\n",
      "  - flows/full_pipeline_flow.py\n",
      "\n",
      "üìÅ tests (8 bestanden):\n",
      "  - tests/evaluate_models.ipynb\n",
      "  - tests/test_raw_ned_obs.ipynb\n",
      "  - tests/test_transform_ned_obs.ipynb\n",
      "  - tests/test_sarimax.ipynb\n",
      "  - tests/Test_trainingset.ipynb\n",
      "  - tests/test_data.py\n",
      "  - tests/test_pipeline.ipynb\n",
      "  - tests/test_models.ipynb\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Pas dit pad aan als notebook niet 1 niveau onder project root zit\n",
    "project_root = os.path.abspath(\"..\")\n",
    "relevante_mappen = [\n",
    "    \"src/data_ingestion\",\n",
    "    \"src/data_processing\",\n",
    "    \"src/data_master\",\n",
    "    \"src/models\",\n",
    "    \"src/utils\",\n",
    "    \"flows\",\n",
    "    \"tests\",\n",
    "    # optioneel:\n",
    "    # \"workspaces/redouan\",\n",
    "    # \"workspaces/sandeep\",\n",
    "    # \"workspaces/sharell\",\n",
    "    # \"workspaces/twan\"\n",
    "]\n",
    "\n",
    "bestand_inventaris = {}\n",
    "\n",
    "for mapnaam in relevante_mappen:\n",
    "    map_pad = os.path.join(project_root, mapnaam)\n",
    "    if os.path.exists(map_pad):\n",
    "        bestanden = []\n",
    "        for root, _, files in os.walk(map_pad):\n",
    "            for file in files:\n",
    "                if file.endswith((\".py\", \".ipynb\")):\n",
    "                    rel_path = os.path.relpath(os.path.join(root, file), project_root)\n",
    "                    bestanden.append(rel_path)\n",
    "        bestand_inventaris[mapnaam] = bestanden\n",
    "    else:\n",
    "        bestand_inventaris[mapnaam] = [\"‚ùå Map niet gevonden\"]\n",
    "\n",
    "# Print overzicht\n",
    "for mapnaam, bestanden in bestand_inventaris.items():\n",
    "    print(f\"\\nüìÅ {mapnaam} ({len(bestanden)} bestanden):\")\n",
    "    for bestand in bestanden:\n",
    "        print(f\"  - {bestand}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d440ac86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting prefect\n",
      "  Downloading prefect-3.4.4-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting aiosqlite<1.0.0,>=0.17.0 (from prefect)\n",
      "  Using cached aiosqlite-0.21.0-py3-none-any.whl.metadata (4.3 kB)\n",
      "Collecting alembic<2.0.0,>=1.7.5 (from prefect)\n",
      "  Downloading alembic-1.16.1-py3-none-any.whl.metadata (7.3 kB)\n",
      "Requirement already satisfied: anyio<5.0.0,>=4.4.0 in /Users/redouan/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from prefect) (4.6.2.post1)\n",
      "Collecting apprise<2.0.0,>=1.1.0 (from prefect)\n",
      "  Using cached apprise-1.9.3-py3-none-any.whl.metadata (53 kB)\n",
      "Collecting asgi-lifespan<3.0,>=1.0 (from prefect)\n",
      "  Using cached asgi_lifespan-2.1.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting asyncpg<1.0.0,>=0.23 (from prefect)\n",
      "  Downloading asyncpg-0.30.0-cp310-cp310-macosx_11_0_arm64.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: cachetools<7.0,>=5.3 in /Users/redouan/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from prefect) (5.5.0)\n",
      "Requirement already satisfied: click<8.2,>=8.0 in /Users/redouan/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from prefect) (8.1.7)\n",
      "Requirement already satisfied: cloudpickle<4.0,>=2.0 in /Users/redouan/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from prefect) (3.1.0)\n",
      "Collecting coolname<3.0.0,>=1.0.4 (from prefect)\n",
      "  Using cached coolname-2.2.0-py2.py3-none-any.whl.metadata (6.2 kB)\n",
      "Collecting cryptography>=36.0.1 (from prefect)\n",
      "  Downloading cryptography-45.0.3-cp37-abi3-macosx_10_9_universal2.whl.metadata (5.7 kB)\n",
      "Collecting dateparser<2.0.0,>=1.1.1 (from prefect)\n",
      "  Using cached dateparser-1.2.1-py3-none-any.whl.metadata (29 kB)\n",
      "Collecting docker<8.0,>=4.0 (from prefect)\n",
      "  Using cached docker-7.1.0-py3-none-any.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.0 in /Users/redouan/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from prefect) (1.2.2)\n",
      "Collecting fastapi<1.0.0,>=0.111.0 (from prefect)\n",
      "  Using cached fastapi-0.115.12-py3-none-any.whl.metadata (27 kB)\n",
      "Collecting fsspec>=2022.5.0 (from prefect)\n",
      "  Downloading fsspec-2025.5.1-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting graphviz>=0.20.1 (from prefect)\n",
      "  Using cached graphviz-0.20.3-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting griffe<2.0.0,>=0.49.0 (from prefect)\n",
      "  Using cached griffe-1.7.3-py3-none-any.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: httpcore<2.0.0,>=1.0.5 in /Users/redouan/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from prefect) (1.0.6)\n",
      "Requirement already satisfied: httpx!=0.23.2,>=0.23 in /Users/redouan/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from httpx[http2]!=0.23.2,>=0.23->prefect) (0.27.2)\n",
      "Collecting humanize<5.0.0,>=4.9.0 (from prefect)\n",
      "  Using cached humanize-4.12.3-py3-none-any.whl.metadata (7.8 kB)\n",
      "Collecting jinja2-humanize-extension>=0.4.0 (from prefect)\n",
      "  Using cached jinja2_humanize_extension-0.4.0-py3-none-any.whl.metadata (3.6 kB)\n",
      "Collecting jinja2<4.0.0,>=3.1.6 (from prefect)\n",
      "  Using cached jinja2-3.1.6-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting jsonpatch<2.0,>=1.32 (from prefect)\n",
      "  Using cached jsonpatch-1.33-py2.py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: jsonschema<5.0.0,>=4.18.0 in /Users/redouan/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from prefect) (4.23.0)\n",
      "Collecting opentelemetry-api<2.0.0,>=1.27.0 (from prefect)\n",
      "  Downloading opentelemetry_api-1.33.1-py3-none-any.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: orjson<4.0,>=3.7 in /Users/redouan/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from prefect) (3.10.11)\n",
      "Requirement already satisfied: packaging<25.1,>=21.3 in /Users/redouan/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from prefect) (24.2)\n",
      "Requirement already satisfied: pathspec>=0.8.0 in /Users/redouan/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from prefect) (0.12.1)\n",
      "Collecting pendulum<4,>=3.0.0 (from prefect)\n",
      "  Downloading pendulum-3.1.0-cp310-cp310-macosx_11_0_arm64.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: prometheus-client>=0.20.0 in /Users/redouan/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from prefect) (0.21.0)\n",
      "Requirement already satisfied: pydantic!=2.10.0,<3.0.0,>=2.9 in /Users/redouan/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from prefect) (2.9.2)\n",
      "Requirement already satisfied: pydantic-core<3.0.0,>=2.12.0 in /Users/redouan/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from prefect) (2.23.4)\n",
      "Collecting pydantic-extra-types<3.0.0,>=2.8.2 (from prefect)\n",
      "  Using cached pydantic_extra_types-2.10.4-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting pydantic-settings!=2.9.0,<3.0.0,>2.2.1 (from prefect)\n",
      "  Using cached pydantic_settings-2.9.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.8.2 in /Users/redouan/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from prefect) (2.9.0.post0)\n",
      "Collecting python-slugify<9.0,>=5.0 (from prefect)\n",
      "  Using cached python_slugify-8.0.4-py2.py3-none-any.whl.metadata (8.5 kB)\n",
      "Collecting python-socks<3.0,>=2.5.3 (from python-socks[asyncio]<3.0,>=2.5.3->prefect)\n",
      "  Using cached python_socks-2.7.1-py3-none-any.whl.metadata (8.0 kB)\n",
      "Requirement already satisfied: pytz<2026,>=2021.1 in /Users/redouan/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from prefect) (2024.2)\n",
      "Requirement already satisfied: pyyaml<7.0.0,>=5.4.1 in /Users/redouan/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from prefect) (6.0.2)\n",
      "Collecting readchar<5.0.0,>=4.0.0 (from prefect)\n",
      "  Using cached readchar-4.2.1-py3-none-any.whl.metadata (7.5 kB)\n",
      "Requirement already satisfied: rfc3339-validator<0.2.0,>=0.1.4 in /Users/redouan/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from prefect) (0.1.4)\n",
      "Collecting rich<15.0,>=11.0 (from prefect)\n",
      "  Using cached rich-14.0.0-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting ruamel-yaml>=0.17.0 (from prefect)\n",
      "  Downloading ruamel.yaml-0.18.11-py3-none-any.whl.metadata (23 kB)\n",
      "Requirement already satisfied: sniffio<2.0.0,>=1.3.0 in /Users/redouan/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from prefect) (1.3.1)\n",
      "Requirement already satisfied: sqlalchemy<3.0.0,>=2.0 in /Users/redouan/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from sqlalchemy[asyncio]<3.0.0,>=2.0->prefect) (2.0.36)\n",
      "Collecting toml>=0.10.0 (from prefect)\n",
      "  Using cached toml-0.10.2-py2.py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting typer!=0.12.2,<0.17.0,>=0.12.0 (from prefect)\n",
      "  Downloading typer-0.16.0-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: typing-extensions<5.0.0,>=4.10.0 in /Users/redouan/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from prefect) (4.12.2)\n",
      "Collecting ujson<6.0.0,>=5.8.0 (from prefect)\n",
      "  Downloading ujson-5.10.0-cp310-cp310-macosx_11_0_arm64.whl.metadata (9.3 kB)\n",
      "Collecting uv>=0.6.0 (from prefect)\n",
      "  Downloading uv-0.7.8-py3-none-macosx_11_0_arm64.whl.metadata (11 kB)\n",
      "Collecting uvicorn!=0.29.0,>=0.14.0 (from prefect)\n",
      "  Using cached uvicorn-0.34.2-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting websockets<16.0,>=13.0 (from prefect)\n",
      "  Downloading websockets-15.0.1-cp310-cp310-macosx_11_0_arm64.whl.metadata (6.8 kB)\n",
      "Collecting Mako (from alembic<2.0.0,>=1.7.5->prefect)\n",
      "  Using cached mako-1.3.10-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: tomli in /Users/redouan/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from alembic<2.0.0,>=1.7.5->prefect) (2.1.0)\n",
      "Requirement already satisfied: idna>=2.8 in /Users/redouan/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from anyio<5.0.0,>=4.4.0->prefect) (3.10)\n",
      "Requirement already satisfied: certifi in /Users/redouan/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from apprise<2.0.0,>=1.1.0->prefect) (2024.8.30)\n",
      "Requirement already satisfied: requests in /Users/redouan/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from apprise<2.0.0,>=1.1.0->prefect) (2.32.3)\n",
      "Collecting requests-oauthlib (from apprise<2.0.0,>=1.1.0->prefect)\n",
      "  Using cached requests_oauthlib-2.0.0-py2.py3-none-any.whl.metadata (11 kB)\n",
      "Collecting markdown (from apprise<2.0.0,>=1.1.0->prefect)\n",
      "  Using cached markdown-3.8-py3-none-any.whl.metadata (5.1 kB)\n",
      "Collecting async-timeout>=4.0.3 (from asyncpg<1.0.0,>=0.23->prefect)\n",
      "  Downloading async_timeout-5.0.1-py3-none-any.whl.metadata (5.1 kB)\n",
      "Requirement already satisfied: cffi>=1.14 in /Users/redouan/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from cryptography>=36.0.1->prefect) (1.17.1)\n",
      "Collecting regex!=2019.02.19,!=2021.8.27,>=2015.06.24 (from dateparser<2.0.0,>=1.1.1->prefect)\n",
      "  Using cached regex-2024.11.6-cp310-cp310-macosx_11_0_arm64.whl.metadata (40 kB)\n",
      "Collecting tzlocal>=0.2 (from dateparser<2.0.0,>=1.1.1->prefect)\n",
      "  Using cached tzlocal-5.3.1-py3-none-any.whl.metadata (7.6 kB)\n",
      "Requirement already satisfied: urllib3>=1.26.0 in /Users/redouan/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from docker<8.0,>=4.0->prefect) (2.2.3)\n",
      "Collecting starlette<0.47.0,>=0.40.0 (from fastapi<1.0.0,>=0.111.0->prefect)\n",
      "  Using cached starlette-0.46.2-py3-none-any.whl.metadata (6.2 kB)\n",
      "Collecting colorama>=0.4 (from griffe<2.0.0,>=0.49.0->prefect)\n",
      "  Using cached colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /Users/redouan/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from httpcore<2.0.0,>=1.0.5->prefect) (0.14.0)\n",
      "Collecting h2<5,>=3 (from httpx[http2]!=0.23.2,>=0.23->prefect)\n",
      "  Using cached h2-4.2.0-py3-none-any.whl.metadata (5.1 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/redouan/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from jinja2<4.0.0,>=3.1.6->prefect) (3.0.2)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /Users/redouan/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from jsonpatch<2.0,>=1.32->prefect) (3.0.0)\n",
      "Requirement already satisfied: attrs>=22.2.0 in /Users/redouan/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from jsonschema<5.0.0,>=4.18.0->prefect) (24.2.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /Users/redouan/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from jsonschema<5.0.0,>=4.18.0->prefect) (2024.10.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /Users/redouan/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from jsonschema<5.0.0,>=4.18.0->prefect) (0.35.1)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /Users/redouan/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from jsonschema<5.0.0,>=4.18.0->prefect) (0.21.0)\n",
      "Collecting deprecated>=1.2.6 (from opentelemetry-api<2.0.0,>=1.27.0->prefect)\n",
      "  Using cached Deprecated-1.2.18-py2.py3-none-any.whl.metadata (5.7 kB)\n",
      "Requirement already satisfied: importlib-metadata<8.7.0,>=6.0 in /Users/redouan/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from opentelemetry-api<2.0.0,>=1.27.0->prefect) (8.5.0)\n",
      "Requirement already satisfied: tzdata>=2020.1 in /Users/redouan/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from pendulum<4,>=3.0.0->prefect) (2024.2)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/redouan/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from pydantic!=2.10.0,<3.0.0,>=2.9->prefect) (0.7.0)\n",
      "Collecting python-dotenv>=0.21.0 (from pydantic-settings!=2.9.0,<3.0.0,>2.2.1->prefect)\n",
      "  Using cached python_dotenv-1.1.0-py3-none-any.whl.metadata (24 kB)\n",
      "Collecting typing-inspection>=0.4.0 (from pydantic-settings!=2.9.0,<3.0.0,>2.2.1->prefect)\n",
      "  Downloading typing_inspection-0.4.1-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: six>=1.5 in /Users/redouan/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from python-dateutil<3.0.0,>=2.8.2->prefect) (1.16.0)\n",
      "Collecting text-unidecode>=1.3 (from python-slugify<9.0,>=5.0->prefect)\n",
      "  Using cached text_unidecode-1.3-py2.py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich<15.0,>=11.0->prefect)\n",
      "  Using cached markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Users/redouan/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from rich<15.0,>=11.0->prefect) (2.18.0)\n",
      "Collecting ruamel.yaml.clib>=0.2.7 (from ruamel-yaml>=0.17.0->prefect)\n",
      "  Downloading ruamel.yaml.clib-0.2.12-cp310-cp310-macosx_13_0_arm64.whl.metadata (1.2 kB)\n",
      "Collecting greenlet!=0.4.17 (from sqlalchemy[asyncio]<3.0.0,>=2.0->prefect)\n",
      "  Downloading greenlet-3.2.2-cp310-cp310-macosx_11_0_universal2.whl.metadata (4.1 kB)\n",
      "Collecting shellingham>=1.3.0 (from typer!=0.12.2,<0.17.0,>=0.12.0->prefect)\n",
      "  Using cached shellingham-1.5.4-py2.py3-none-any.whl.metadata (3.5 kB)\n",
      "Requirement already satisfied: pycparser in /Users/redouan/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from cffi>=1.14->cryptography>=36.0.1->prefect) (2.22)\n",
      "Collecting wrapt<2,>=1.10 (from deprecated>=1.2.6->opentelemetry-api<2.0.0,>=1.27.0->prefect)\n",
      "  Downloading wrapt-1.17.2-cp310-cp310-macosx_11_0_arm64.whl.metadata (6.4 kB)\n",
      "Collecting hyperframe<7,>=6.1 (from h2<5,>=3->httpx[http2]!=0.23.2,>=0.23->prefect)\n",
      "  Using cached hyperframe-6.1.0-py3-none-any.whl.metadata (4.3 kB)\n",
      "Collecting hpack<5,>=4.1 (from h2<5,>=3->httpx[http2]!=0.23.2,>=0.23->prefect)\n",
      "  Using cached hpack-4.1.0-py3-none-any.whl.metadata (4.6 kB)\n",
      "Requirement already satisfied: zipp>=3.20 in /Users/redouan/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from importlib-metadata<8.7.0,>=6.0->opentelemetry-api<2.0.0,>=1.27.0->prefect) (3.21.0)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich<15.0,>=11.0->prefect)\n",
      "  Using cached mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/redouan/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from requests->apprise<2.0.0,>=1.1.0->prefect) (3.4.0)\n",
      "Collecting oauthlib>=3.0.0 (from requests-oauthlib->apprise<2.0.0,>=1.1.0->prefect)\n",
      "  Using cached oauthlib-3.2.2-py3-none-any.whl.metadata (7.5 kB)\n",
      "Downloading prefect-3.4.4-py3-none-any.whl (6.0 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m6.0/6.0 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached aiosqlite-0.21.0-py3-none-any.whl (15 kB)\n",
      "Downloading alembic-1.16.1-py3-none-any.whl (242 kB)\n",
      "Using cached apprise-1.9.3-py3-none-any.whl (1.4 MB)\n",
      "Using cached asgi_lifespan-2.1.0-py3-none-any.whl (10 kB)\n",
      "Downloading asyncpg-0.30.0-cp310-cp310-macosx_11_0_arm64.whl (645 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m645.0/645.0 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached coolname-2.2.0-py2.py3-none-any.whl (37 kB)\n",
      "Downloading cryptography-45.0.3-cp37-abi3-macosx_10_9_universal2.whl (7.0 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached dateparser-1.2.1-py3-none-any.whl (295 kB)\n",
      "Using cached docker-7.1.0-py3-none-any.whl (147 kB)\n",
      "Using cached fastapi-0.115.12-py3-none-any.whl (95 kB)\n",
      "Downloading fsspec-2025.5.1-py3-none-any.whl (199 kB)\n",
      "Using cached graphviz-0.20.3-py3-none-any.whl (47 kB)\n",
      "Using cached griffe-1.7.3-py3-none-any.whl (129 kB)\n",
      "Using cached humanize-4.12.3-py3-none-any.whl (128 kB)\n",
      "Using cached jinja2-3.1.6-py3-none-any.whl (134 kB)\n",
      "Using cached jinja2_humanize_extension-0.4.0-py3-none-any.whl (4.8 kB)\n",
      "Using cached jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
      "Downloading opentelemetry_api-1.33.1-py3-none-any.whl (65 kB)\n",
      "Downloading pendulum-3.1.0-cp310-cp310-macosx_11_0_arm64.whl (326 kB)\n",
      "Using cached pydantic_extra_types-2.10.4-py3-none-any.whl (37 kB)\n",
      "Using cached pydantic_settings-2.9.1-py3-none-any.whl (44 kB)\n",
      "Using cached python_slugify-8.0.4-py2.py3-none-any.whl (10 kB)\n",
      "Using cached python_socks-2.7.1-py3-none-any.whl (54 kB)\n",
      "Using cached readchar-4.2.1-py3-none-any.whl (9.3 kB)\n",
      "Using cached rich-14.0.0-py3-none-any.whl (243 kB)\n",
      "Downloading ruamel.yaml-0.18.11-py3-none-any.whl (118 kB)\n",
      "Using cached toml-0.10.2-py2.py3-none-any.whl (16 kB)\n",
      "Downloading typer-0.16.0-py3-none-any.whl (46 kB)\n",
      "Downloading ujson-5.10.0-cp310-cp310-macosx_11_0_arm64.whl (51 kB)\n",
      "Downloading uv-0.7.8-py3-none-macosx_11_0_arm64.whl (15.6 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m15.6/15.6 MB\u001b[0m \u001b[31m19.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached uvicorn-0.34.2-py3-none-any.whl (62 kB)\n",
      "Downloading websockets-15.0.1-cp310-cp310-macosx_11_0_arm64.whl (173 kB)\n",
      "Downloading async_timeout-5.0.1-py3-none-any.whl (6.2 kB)\n",
      "Using cached colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
      "Using cached Deprecated-1.2.18-py2.py3-none-any.whl (10.0 kB)\n",
      "Downloading greenlet-3.2.2-cp310-cp310-macosx_11_0_universal2.whl (267 kB)\n",
      "Using cached h2-4.2.0-py3-none-any.whl (60 kB)\n",
      "Using cached markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "Using cached python_dotenv-1.1.0-py3-none-any.whl (20 kB)\n",
      "Using cached regex-2024.11.6-cp310-cp310-macosx_11_0_arm64.whl (284 kB)\n",
      "Downloading ruamel.yaml.clib-0.2.12-cp310-cp310-macosx_13_0_arm64.whl (131 kB)\n",
      "Using cached shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
      "Using cached starlette-0.46.2-py3-none-any.whl (72 kB)\n",
      "Using cached text_unidecode-1.3-py2.py3-none-any.whl (78 kB)\n",
      "Downloading typing_inspection-0.4.1-py3-none-any.whl (14 kB)\n",
      "Using cached tzlocal-5.3.1-py3-none-any.whl (18 kB)\n",
      "Using cached mako-1.3.10-py3-none-any.whl (78 kB)\n",
      "Using cached markdown-3.8-py3-none-any.whl (106 kB)\n",
      "Using cached requests_oauthlib-2.0.0-py2.py3-none-any.whl (24 kB)\n",
      "Using cached hpack-4.1.0-py3-none-any.whl (34 kB)\n",
      "Using cached hyperframe-6.1.0-py3-none-any.whl (13 kB)\n",
      "Using cached mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Using cached oauthlib-3.2.2-py3-none-any.whl (151 kB)\n",
      "Downloading wrapt-1.17.2-cp310-cp310-macosx_11_0_arm64.whl (38 kB)\n",
      "Installing collected packages: text-unidecode, coolname, wrapt, websockets, uvicorn, uv, ujson, tzlocal, typing-inspection, toml, shellingham, ruamel.yaml.clib, regex, readchar, python-socks, python-slugify, python-dotenv, oauthlib, mdurl, markdown, Mako, jsonpatch, jinja2, hyperframe, humanize, hpack, greenlet, graphviz, fsspec, colorama, async-timeout, asgi-lifespan, aiosqlite, starlette, ruamel-yaml, requests-oauthlib, pendulum, markdown-it-py, jinja2-humanize-extension, h2, griffe, docker, deprecated, dateparser, cryptography, asyncpg, alembic, rich, pydantic-settings, pydantic-extra-types, opentelemetry-api, fastapi, apprise, typer, prefect\n",
      "  Attempting uninstall: jinja2\n",
      "    Found existing installation: Jinja2 3.1.4\n",
      "    Uninstalling Jinja2-3.1.4:\n",
      "      Successfully uninstalled Jinja2-3.1.4\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "pycaret 3.3.2 requires pandas<2.2.0, but you have pandas 2.2.3 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed Mako-1.3.10 aiosqlite-0.21.0 alembic-1.16.1 apprise-1.9.3 asgi-lifespan-2.1.0 async-timeout-5.0.1 asyncpg-0.30.0 colorama-0.4.6 coolname-2.2.0 cryptography-45.0.3 dateparser-1.2.1 deprecated-1.2.18 docker-7.1.0 fastapi-0.115.12 fsspec-2025.5.1 graphviz-0.20.3 greenlet-3.2.2 griffe-1.7.3 h2-4.2.0 hpack-4.1.0 humanize-4.12.3 hyperframe-6.1.0 jinja2-3.1.6 jinja2-humanize-extension-0.4.0 jsonpatch-1.33 markdown-3.8 markdown-it-py-3.0.0 mdurl-0.1.2 oauthlib-3.2.2 opentelemetry-api-1.33.1 pendulum-3.1.0 prefect-3.4.4 pydantic-extra-types-2.10.4 pydantic-settings-2.9.1 python-dotenv-1.1.0 python-slugify-8.0.4 python-socks-2.7.1 readchar-4.2.1 regex-2024.11.6 requests-oauthlib-2.0.0 rich-14.0.0 ruamel-yaml-0.18.11 ruamel.yaml.clib-0.2.12 shellingham-1.5.4 starlette-0.46.2 text-unidecode-1.3 toml-0.10.2 typer-0.16.0 typing-inspection-0.4.1 tzlocal-5.3.1 ujson-5.10.0 uv-0.7.8 uvicorn-0.34.2 websockets-15.0.1 wrapt-1.17.2\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install prefect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8903bfaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Testing pipeline...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">07:53:00.553 | <span style=\"color: #008080; text-decoration-color: #008080\">INFO</span>    | prefect - Starting temporary server on <span style=\"color: #0000ff; text-decoration-color: #0000ff\">http://127.0.0.1:8993</span>\n",
       "See <span style=\"color: #0000ff; text-decoration-color: #0000ff\">https://docs.prefect.io/3.0/manage/self-host#self-host-a-prefect-server</span> for more information on running a dedicated Prefect server.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "07:53:00.553 | \u001b[36mINFO\u001b[0m    | prefect - Starting temporary server on \u001b[94mhttp://127.0.0.1:8993\u001b[0m\n",
       "See \u001b[94mhttps://docs.prefect.io/3.0/manage/self-host#self-host-a-prefect-server\u001b[0m for more information on running a dedicated Prefect server.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">07:53:20.864 | <span style=\"color: #008080; text-decoration-color: #008080\">INFO</span>    | Flow run<span style=\"color: #800080; text-decoration-color: #800080\"> 'tentacled-hog'</span> - Beginning flow run<span style=\"color: #800080; text-decoration-color: #800080\"> 'tentacled-hog'</span> for flow<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> 'full-pipeline-flow'</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "07:53:20.864 | \u001b[36mINFO\u001b[0m    | Flow run\u001b[35m 'tentacled-hog'\u001b[0m - Beginning flow run\u001b[35m 'tentacled-hog'\u001b[0m for flow\u001b[1;35m 'full-pipeline-flow'\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è Run weather notebooks manually:\n",
      "1. API_open_meteo_historical.ipynb\n",
      "2. API_open_meteo_preds.ipynb\n",
      "3. transform_weather_obs.ipynb\n",
      "4. transform_weather_preds.ipynb\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">07:53:20.949 | <span style=\"color: #008080; text-decoration-color: #008080\">INFO</span>    | Task run 'ingest_and_process-2a8' - Finished in state <span style=\"color: #008000; text-decoration-color: #008000\">Completed</span>()\n",
       "</pre>\n"
      ],
      "text/plain": [
       "07:53:20.949 | \u001b[36mINFO\u001b[0m    | Task run 'ingest_and_process-2a8' - Finished in state \u001b[32mCompleted\u001b[0m()\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">07:53:21.032 | <span style=\"color: #d70000; text-decoration-color: #d70000\">ERROR</span>   | build_master_predictions - ‚ùå Fout tijdens build: 'target_datetime'\n",
       "Traceback (most recent call last):\n",
       "  File \"/Users/redouan/.pyenv/versions/3.10.13/lib/python3.10/site-packages/pandas/core/indexes/base.py\", line 3805, in get_loc\n",
       "    return self._engine.get_loc(casted_key)\n",
       "  File \"index.pyx\", line 167, in pandas._libs.index.IndexEngine.get_loc\n",
       "  File \"index.pyx\", line 196, in pandas._libs.index.IndexEngine.get_loc\n",
       "  File \"pandas/_libs/hashtable_class_helper.pxi\", line 7081, in pandas._libs.hashtable.PyObjectHashTable.get_item\n",
       "  File \"pandas/_libs/hashtable_class_helper.pxi\", line 7089, in pandas._libs.hashtable.PyObjectHashTable.get_item\n",
       "KeyError: 'target_datetime'\n",
       "\n",
       "The above exception was the direct cause of the following exception:\n",
       "\n",
       "Traceback (most recent call last):\n",
       "  File \"/Users/redouan/ENEXIS/src/data_master/build_master_predictions.py\", line 44, in build_master\n",
       "    df_weather[\"target_datetime\"] = pd.to_datetime(df_weather[\"target_datetime\"], utc=True)\n",
       "  File \"/Users/redouan/.pyenv/versions/3.10.13/lib/python3.10/site-packages/pandas/core/frame.py\", line 4102, in __getitem__\n",
       "    indexer = self.columns.get_loc(key)\n",
       "  File \"/Users/redouan/.pyenv/versions/3.10.13/lib/python3.10/site-packages/pandas/core/indexes/base.py\", line 3812, in get_loc\n",
       "    raise KeyError(key) from err\n",
       "KeyError: 'target_datetime'\n",
       "</pre>\n"
      ],
      "text/plain": [
       "07:53:21.032 | \u001b[38;5;160mERROR\u001b[0m   | build_master_predictions - ‚ùå Fout tijdens build: 'target_datetime'\n",
       "Traceback (most recent call last):\n",
       "  File \"/Users/redouan/.pyenv/versions/3.10.13/lib/python3.10/site-packages/pandas/core/indexes/base.py\", line 3805, in get_loc\n",
       "    return self._engine.get_loc(casted_key)\n",
       "  File \"index.pyx\", line 167, in pandas._libs.index.IndexEngine.get_loc\n",
       "  File \"index.pyx\", line 196, in pandas._libs.index.IndexEngine.get_loc\n",
       "  File \"pandas/_libs/hashtable_class_helper.pxi\", line 7081, in pandas._libs.hashtable.PyObjectHashTable.get_item\n",
       "  File \"pandas/_libs/hashtable_class_helper.pxi\", line 7089, in pandas._libs.hashtable.PyObjectHashTable.get_item\n",
       "KeyError: 'target_datetime'\n",
       "\n",
       "The above exception was the direct cause of the following exception:\n",
       "\n",
       "Traceback (most recent call last):\n",
       "  File \"/Users/redouan/ENEXIS/src/data_master/build_master_predictions.py\", line 44, in build_master\n",
       "    df_weather[\"target_datetime\"] = pd.to_datetime(df_weather[\"target_datetime\"], utc=True)\n",
       "  File \"/Users/redouan/.pyenv/versions/3.10.13/lib/python3.10/site-packages/pandas/core/frame.py\", line 4102, in __getitem__\n",
       "    indexer = self.columns.get_loc(key)\n",
       "  File \"/Users/redouan/.pyenv/versions/3.10.13/lib/python3.10/site-packages/pandas/core/indexes/base.py\", line 3812, in get_loc\n",
       "    raise KeyError(key) from err\n",
       "KeyError: 'target_datetime'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">07:53:21.039 | <span style=\"color: #008080; text-decoration-color: #008080\">INFO</span>    | Task run 'build_masters-bbd' - Finished in state <span style=\"color: #008000; text-decoration-color: #008000\">Completed</span>()\n",
       "</pre>\n"
      ],
      "text/plain": [
       "07:53:21.039 | \u001b[36mINFO\u001b[0m    | Task run 'build_masters-bbd' - Finished in state \u001b[32mCompleted\u001b[0m()\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">07:53:21.075 | <span style=\"color: #008080; text-decoration-color: #008080\">INFO</span>    | Flow run<span style=\"color: #800080; text-decoration-color: #800080\"> 'tentacled-hog'</span> - Finished in state <span style=\"color: #008000; text-decoration-color: #008000\">Completed</span>()\n",
       "</pre>\n"
      ],
      "text/plain": [
       "07:53:21.075 | \u001b[36mINFO\u001b[0m    | Flow run\u001b[35m 'tentacled-hog'\u001b[0m - Finished in state \u001b[32mCompleted\u001b[0m()\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Pipeline completed successfully!\n",
      "Result: (None, None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "08:45:00.953 | \u001b[31mERROR\u001b[0m   | prefect.server.services.telemetry - \u001b[31mFailed\u001b[0m to send telemetry: [Errno 8] nodename nor servname provided, or not known\n",
      "Shutting down telemetry service...\n"
     ]
    }
   ],
   "source": [
    "# Simple test to see if pipeline triggers all modules\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the project root to Python path\n",
    "sys.path.append(os.path.abspath('..'))\n",
    "\n",
    "# Import the pipeline\n",
    "from flows.full_pipeline_flow import full_pipeline_flow\n",
    "\n",
    "# Test the pipeline\n",
    "print(\"üöÄ Testing pipeline...\")\n",
    "try:\n",
    "    result = full_pipeline_flow()\n",
    "    print(\"‚úÖ Pipeline completed successfully!\")\n",
    "    print(f\"Result: {result}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Pipeline error: {str(e)}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21172b01",
   "metadata": {},
   "source": [
    "# DB-scan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "57a22014",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì¶ Gevonden tabellen in WARP.db:\n",
      "\n",
      "üîπ raw_ned_obs: 40807 records\n",
      "üîπ transform_ned_obs: 29255 records\n",
      "üîπ raw_ned_df: 26304 records\n",
      "üîπ raw_meteo_preds_history: 3192 records\n",
      "üîπ raw_meteo_obs: 3187 records\n",
      "üîπ transform_meteo_forecast_now: 173 records\n",
      "üîπ transform_weather_preds_history: 3192 records\n",
      "üîπ raw_ned_obs_2: 12764 records\n",
      "üîπ transform_weather_obs: 3202 records\n",
      "üîπ transform_ned_obs_2: 3191 records\n",
      "üîπ process_weather_preds: 8568 records\n",
      "üîπ master_warp: 3355 records\n",
      "üîπ raw_NED_preds: 153602 records\n",
      "üîπ raw_meteo_forecast_now: 192 records\n",
      "üîπ processed_NED_preds: 22078 records\n",
      "üîπ dim_datetime: 3672 records\n",
      "üîπ raw_weather_obs: 3320 records\n",
      "üîπ raw_weather_preds: 3504 records\n",
      "üîπ raw_weather_preds_test: 3504 records\n",
      "üîπ raw_meteo_obs_test: 3321 records\n",
      "üîπ raw_entsoe_obs: 13342 records\n",
      "üîπ transform_entsoe_obs: 3359 records\n",
      "üîπ master_predictions: 10008 records\n",
      "üîπ training_set: 1752 records\n"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "import os\n",
    "\n",
    "# Pad naar jouw DB (pas aan als nodig)\n",
    "db_path = os.path.abspath(\"../src/data/WARP.db\")\n",
    "\n",
    "if not os.path.exists(db_path):\n",
    "    raise FileNotFoundError(f\"Database niet gevonden op: {db_path}\")\n",
    "\n",
    "# Verbinding maken\n",
    "conn = sqlite3.connect(db_path)\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Tabellen ophalen\n",
    "cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n",
    "tables = cursor.fetchall()\n",
    "\n",
    "print(f\"üì¶ Gevonden tabellen in {os.path.basename(db_path)}:\\n\")\n",
    "\n",
    "# Voor elke tabel: print aantal rijen\n",
    "for table in tables:\n",
    "    name = table[0]\n",
    "    try:\n",
    "        cursor.execute(f\"SELECT COUNT(*) FROM {name}\")\n",
    "        count = cursor.fetchone()[0]\n",
    "        print(f\"üîπ {name}: {count} records\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Fout bij {name}: {e}\")\n",
    "\n",
    "conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "90ef107a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting to database at: ..\\src\\data\\WARP.db\n",
      "\n",
      "=== Sample from raw_weather_preds ===\n",
      "Shape: (5, 81)\n",
      "Columns:\n",
      "  - date\n",
      "  - temperature_2m\n",
      "  - temperature_2m_previous_day1\n",
      "  - temperature_2m_previous_day2\n",
      "  - temperature_2m_previous_day3\n",
      "  - temperature_2m_previous_day4\n",
      "  - temperature_2m_previous_day5\n",
      "  - temperature_2m_previous_day6\n",
      "  - temperature_2m_previous_day7\n",
      "  - wind_speed_10m\n",
      "  - wind_speed_10m_previous_day1\n",
      "  - wind_speed_10m_previous_day2\n",
      "  - wind_speed_10m_previous_day3\n",
      "  - wind_speed_10m_previous_day4\n",
      "  - wind_speed_10m_previous_day5\n",
      "  - wind_speed_10m_previous_day6\n",
      "  - wind_speed_10m_previous_day7\n",
      "  - wind_direction_10m_previous_day7\n",
      "  - wind_direction_10m_previous_day6\n",
      "  - wind_direction_10m_previous_day5\n",
      "  - cloud_cover\n",
      "  - cloud_cover_previous_day1\n",
      "  - cloud_cover_previous_day2\n",
      "  - cloud_cover_previous_day3\n",
      "  - cloud_cover_previous_day4\n",
      "  - cloud_cover_previous_day5\n",
      "  - cloud_cover_previous_day6\n",
      "  - cloud_cover_previous_day7\n",
      "  - snowfall\n",
      "  - snowfall_previous_day1\n",
      "  - snowfall_previous_day2\n",
      "  - snowfall_previous_day3\n",
      "  - snowfall_previous_day4\n",
      "  - snowfall_previous_day5\n",
      "  - snowfall_previous_day6\n",
      "  - snowfall_previous_day7\n",
      "  - apparent_temperature\n",
      "  - apparent_temperature_previous_day1\n",
      "  - apparent_temperature_previous_day2\n",
      "  - apparent_temperature_previous_day3\n",
      "  - apparent_temperature_previous_day4\n",
      "  - apparent_temperature_previous_day5\n",
      "  - apparent_temperature_previous_day6\n",
      "  - apparent_temperature_previous_day7\n",
      "  - wind_direction_10m_previous_day4\n",
      "  - wind_direction_10m_previous_day3\n",
      "  - wind_direction_10m_previous_day2\n",
      "  - wind_direction_10m_previous_day1\n",
      "  - wind_direction_10m\n",
      "  - diffuse_radiation\n",
      "  - diffuse_radiation_previous_day1\n",
      "  - diffuse_radiation_previous_day2\n",
      "  - diffuse_radiation_previous_day3\n",
      "  - diffuse_radiation_previous_day4\n",
      "  - diffuse_radiation_previous_day5\n",
      "  - diffuse_radiation_previous_day6\n",
      "  - diffuse_radiation_previous_day7\n",
      "  - direct_normal_irradiance\n",
      "  - direct_normal_irradiance_previous_day1\n",
      "  - direct_normal_irradiance_previous_day2\n",
      "  - direct_normal_irradiance_previous_day3\n",
      "  - direct_normal_irradiance_previous_day4\n",
      "  - direct_normal_irradiance_previous_day5\n",
      "  - direct_normal_irradiance_previous_day6\n",
      "  - direct_normal_irradiance_previous_day7\n",
      "  - shortwave_radiation\n",
      "  - shortwave_radiation_previous_day1\n",
      "  - shortwave_radiation_previous_day2\n",
      "  - shortwave_radiation_previous_day3\n",
      "  - shortwave_radiation_previous_day4\n",
      "  - shortwave_radiation_previous_day5\n",
      "  - shortwave_radiation_previous_day6\n",
      "  - shortwave_radiation_previous_day7\n",
      "  - direct_radiation\n",
      "  - direct_radiation_previous_day1\n",
      "  - direct_radiation_previous_day2\n",
      "  - direct_radiation_previous_day3\n",
      "  - direct_radiation_previous_day4\n",
      "  - direct_radiation_previous_day5\n",
      "  - direct_radiation_previous_day6\n",
      "  - direct_radiation_previous_day7\n",
      "\n",
      "Data Sample:\n",
      "                        date  temperature_2m  temperature_2m_previous_day1  \\\n",
      "0  2024-12-25 00:00:00+00:00        7.513000                      7.863000   \n",
      "1  2024-12-25 01:00:00+00:00        7.463000                      8.662999   \n",
      "2  2024-12-25 02:00:00+00:00        7.863000                      8.363000   \n",
      "3  2024-12-25 03:00:00+00:00        8.113000                      8.763000   \n",
      "4  2024-12-25 04:00:00+00:00        8.162999                      9.313000   \n",
      "\n",
      "   temperature_2m_previous_day2  temperature_2m_previous_day3  \\\n",
      "0                          8.95                           7.9   \n",
      "1                          8.95                           8.2   \n",
      "2                          8.85                           8.3   \n",
      "3                          8.75                           8.4   \n",
      "4                          8.70                           8.6   \n",
      "\n",
      "   temperature_2m_previous_day4  temperature_2m_previous_day5  \\\n",
      "0                          5.15                          8.70   \n",
      "1                          5.05                          8.80   \n",
      "2                          5.20                          8.95   \n",
      "3                          5.45                          9.00   \n",
      "4                          5.95                          9.00   \n",
      "\n",
      "   temperature_2m_previous_day6  temperature_2m_previous_day7  wind_speed_10m  \\\n",
      "0                          2.80                          9.65            9.00   \n",
      "1                          2.40                          9.60            7.20   \n",
      "2                          2.85                          9.50            6.84   \n",
      "3                          3.30                          9.35            7.92   \n",
      "4                          3.60                          9.25            5.04   \n",
      "\n",
      "   ...  shortwave_radiation_previous_day6  shortwave_radiation_previous_day7  \\\n",
      "0  ...                                0.0                                0.0   \n",
      "1  ...                                0.0                                0.0   \n",
      "2  ...                                0.0                                0.0   \n",
      "3  ...                                0.0                                0.0   \n",
      "4  ...                                0.0                                0.0   \n",
      "\n",
      "   direct_radiation  direct_radiation_previous_day1  \\\n",
      "0               0.0                             0.0   \n",
      "1               0.0                             0.0   \n",
      "2               0.0                             0.0   \n",
      "3               0.0                             0.0   \n",
      "4               0.0                             0.0   \n",
      "\n",
      "   direct_radiation_previous_day2  direct_radiation_previous_day3  \\\n",
      "0                             0.0                             0.0   \n",
      "1                             0.0                             0.0   \n",
      "2                             0.0                             0.0   \n",
      "3                             0.0                             0.0   \n",
      "4                             0.0                             0.0   \n",
      "\n",
      "   direct_radiation_previous_day4  direct_radiation_previous_day5  \\\n",
      "0                             0.0                             0.0   \n",
      "1                             0.0                             0.0   \n",
      "2                             0.0                             0.0   \n",
      "3                             0.0                             0.0   \n",
      "4                             0.0                             0.0   \n",
      "\n",
      "   direct_radiation_previous_day6  direct_radiation_previous_day7  \n",
      "0                             0.0                             0.0  \n",
      "1                             0.0                             0.0  \n",
      "2                             0.0                             0.0  \n",
      "3                             0.0                             0.0  \n",
      "4                             0.0                             0.0  \n",
      "\n",
      "[5 rows x 81 columns]\n",
      "\n",
      "=== Sample from process_weather_preds ===\n",
      "Shape: (5, 12)\n",
      "Columns:\n",
      "  - target_datetime\n",
      "  - temperature_2m\n",
      "  - run_date\n",
      "  - wind_speed_10m\n",
      "  - wind_direction_10m\n",
      "  - cloud_cover\n",
      "  - snowfall\n",
      "  - apparent_temperature\n",
      "  - diffuse_radiation\n",
      "  - direct_normal_irradiance\n",
      "  - shortwave_radiation\n",
      "  - direct_radiation\n",
      "\n",
      "Data Sample:\n",
      "             target_datetime  temperature_2m                   run_date  \\\n",
      "0  2024-12-25 00:00:00+00:00            9.65  2024-12-18 00:00:00+00:00   \n",
      "1  2024-12-25 00:00:00+00:00            2.80  2024-12-19 00:00:00+00:00   \n",
      "2  2024-12-25 00:00:00+00:00            8.70  2024-12-20 00:00:00+00:00   \n",
      "3  2024-12-25 00:00:00+00:00            5.15  2024-12-21 00:00:00+00:00   \n",
      "4  2024-12-25 00:00:00+00:00            7.90  2024-12-22 00:00:00+00:00   \n",
      "\n",
      "   wind_speed_10m  wind_direction_10m  cloud_cover  snowfall  \\\n",
      "0        8.121970           257.19574         93.0       0.0   \n",
      "1        8.089994           212.27562        100.0       0.0   \n",
      "2       10.787993           244.29010        100.0       0.0   \n",
      "3        9.885262           190.49142        100.0       0.0   \n",
      "4        9.422101           223.45189        100.0       0.0   \n",
      "\n",
      "   apparent_temperature  diffuse_radiation  direct_normal_irradiance  \\\n",
      "0              8.258032                0.0                       0.0   \n",
      "1             -0.161992                0.0                       0.0   \n",
      "2              6.707765                0.0                       0.0   \n",
      "3              2.277478                0.0                       0.0   \n",
      "4              5.904596                0.0                       0.0   \n",
      "\n",
      "   shortwave_radiation  direct_radiation  \n",
      "0                  0.0               0.0  \n",
      "1                  0.0               0.0  \n",
      "2                  0.0               0.0  \n",
      "3                  0.0               0.0  \n",
      "4                  0.0               0.0  \n",
      "\n",
      "=== Weather Variables in Transformed Table ===\n",
      "Weather variables: ['temperature_2m', 'wind_speed_10m', 'wind_direction_10m', 'cloud_cover', 'snowfall', 'apparent_temperature', 'diffuse_radiation', 'direct_normal_irradiance', 'shortwave_radiation', 'direct_radiation']\n",
      "\n",
      "=== Sample of Row Counts per Target Date ===\n",
      "             target_datetime  count\n",
      "0  2024-12-25 00:00:00+00:00      7\n",
      "1  2024-12-25 01:00:00+00:00      7\n",
      "2  2024-12-25 02:00:00+00:00      7\n",
      "3  2024-12-25 03:00:00+00:00      7\n",
      "4  2024-12-25 04:00:00+00:00      7\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\n",
    "import pandas as pd\n",
    "import sqlite3\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "# Try to locate the database - first using the path from your original script\n",
    "# and then trying relative paths if that doesn't work\n",
    "db_paths_to_try = [\n",
    "    Path.cwd().parent.parent / \"src\" / \"data\" / \"WARP.db\",\n",
    "    Path(\"../src/data/WARP.db\"),\n",
    "    Path(\"./src/data/WARP.db\"),\n",
    "    Path(\"./WARP.db\")\n",
    "]\n",
    "\n",
    "db_path = None\n",
    "for path in db_paths_to_try:\n",
    "    if path.exists():\n",
    "        db_path = path\n",
    "        break\n",
    "\n",
    "if db_path is None:\n",
    "    # Allow user to input path if not found\n",
    "    user_path = input(\"Database not found at expected locations. Please enter the path to WARP.db: \")\n",
    "    db_path = Path(user_path)\n",
    "    if not db_path.exists():\n",
    "        raise FileNotFoundError(f\"Database not found at: {db_path}\")\n",
    "\n",
    "# Constants\n",
    "RAW_TABLE = \"raw_weather_preds\"\n",
    "TRANSFORM_TABLE = \"process_weather_preds\"\n",
    "\n",
    "# Connect to the database\n",
    "print(f\"Connecting to database at: {db_path}\")\n",
    "conn = sqlite3.connect(db_path)\n",
    "\n",
    "# Get sample of raw data\n",
    "print(f\"\\n=== Sample from {RAW_TABLE} ===\")\n",
    "raw_df = pd.read_sql_query(f\"SELECT * FROM {RAW_TABLE} LIMIT 5\", conn)\n",
    "print(f\"Shape: {raw_df.shape}\")\n",
    "print(\"Columns:\")\n",
    "for col in raw_df.columns:\n",
    "    print(f\"  - {col}\")\n",
    "print(\"\\nData Sample:\")\n",
    "print(raw_df.head())\n",
    "\n",
    "# Get sample of transformed data\n",
    "print(f\"\\n=== Sample from {TRANSFORM_TABLE} ===\")\n",
    "transform_df = pd.read_sql_query(f\"SELECT * FROM {TRANSFORM_TABLE} LIMIT 5\", conn)\n",
    "print(f\"Shape: {transform_df.shape}\")\n",
    "print(\"Columns:\")\n",
    "for col in transform_df.columns:\n",
    "    print(f\"  - {col}\")\n",
    "print(\"\\nData Sample:\")\n",
    "print(transform_df.head())\n",
    "\n",
    "# Get unique weather variables in the transformed table\n",
    "print(\"\\n=== Weather Variables in Transformed Table ===\")\n",
    "weather_vars = [col for col in transform_df.columns \n",
    "               if col not in [\"run_date\", \"target_datetime\"]]\n",
    "print(f\"Weather variables: {weather_vars}\")\n",
    "\n",
    "# Count rows per target date in the transformed table (to understand the expansion)\n",
    "print(\"\\n=== Sample of Row Counts per Target Date ===\")\n",
    "count_query = \"\"\"\n",
    "SELECT target_datetime, COUNT(*) as count \n",
    "FROM process_weather_preds \n",
    "GROUP BY target_datetime \n",
    "ORDER BY target_datetime \n",
    "LIMIT 5\n",
    "\"\"\"\n",
    "counts_df = pd.read_sql_query(count_query, conn)\n",
    "print(counts_df)\n",
    "\n",
    "# Close the connection\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d0186dd7",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'auto_arima_order' from 'models.sarimax_model' (c:\\Users\\dai\\ENEXIS\\src\\models\\sarimax_model.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 27\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mdata_processing\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msplit\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m time_based_split\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnaive_model\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m run_naive_model\n\u001b[1;32m---> 27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msarimax_model\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m run_sarimax, auto_arima_order\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlogger\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m init_logger, log_info\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;66;03m# ‚úÖ 3. Logging starten\u001b[39;00m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'auto_arima_order' from 'models.sarimax_model' (c:\\Users\\dai\\ENEXIS\\src\\models\\sarimax_model.py)"
     ]
    }
   ],
   "source": [
    "# ----------------------------\n",
    "# ‚úÖ 1. FIX voor imports in Jupyter\n",
    "# ----------------------------\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Stel pad naar src/ in (pas aan indien je notebook elders staat)\n",
    "src_path = os.path.abspath(\"../src\")  # Als je notebook in /tests staat\n",
    "if src_path not in sys.path:\n",
    "    sys.path.append(src_path)\n",
    "\n",
    "# ----------------------------\n",
    "# ‚úÖ 2. Imports modules uit src/\n",
    "# ----------------------------\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "from data_processing.feature_eng import (\n",
    "    add_lag_features,\n",
    "    add_rolling_features,\n",
    "    add_time_features,\n",
    "    scale_features\n",
    ")\n",
    "from data_processing.split import time_based_split\n",
    "from models.naive_model import run_naive_model\n",
    "from models.sarimax_model import run_sarimax, auto_arima_order\n",
    "from utils.logger import init_logger, log_info\n",
    "\n",
    "# ----------------------------\n",
    "# ‚úÖ 3. Logging starten\n",
    "# ----------------------------\n",
    "init_logger()\n",
    "log_info(\"Starting test pipeline run\", module=\"test_pipeline\")\n",
    "\n",
    "# ----------------------------\n",
    "# ‚úÖ 4. Dummy DataFrame aanmaken\n",
    "# ----------------------------\n",
    "np.random.seed(42)\n",
    "dates = pd.date_range(start=\"2025-01-01\", periods=100, freq=\"H\")\n",
    "df = pd.DataFrame({\n",
    "    \"datetime\": dates,\n",
    "    \"price\": np.random.normal(loc=50, scale=10, size=100),\n",
    "    \"load\": np.random.normal(loc=300, scale=30, size=100)\n",
    "})\n",
    "df.set_index(\"datetime\", inplace=True)\n",
    "df.reset_index(inplace=True)\n",
    "\n",
    "# ----------------------------\n",
    "# ‚úÖ 5. Feature Engineering\n",
    "# ----------------------------\n",
    "df = add_lag_features(df, columns=[\"price\", \"load\"], lags=[1, 24])\n",
    "df = add_rolling_features(df, columns=[\"price\", \"load\"], windows=[3, 24])\n",
    "df = add_time_features(df)\n",
    "df_scaled, _ = scale_features(df, columns=[\"price\", \"load\"])\n",
    "print(\"‚úÖ Feature engineering done:\", df_scaled.columns.tolist())\n",
    "\n",
    "# ----------------------------\n",
    "# ‚úÖ 6. Train/Test splitsing\n",
    "# ----------------------------\n",
    "train_df, test_df = time_based_split(df_scaled, train_ratio=0.8)\n",
    "print(f\"‚úÖ Train size: {len(train_df)}, Test size: {len(test_df)}\")\n",
    "\n",
    "# ----------------------------\n",
    "# ‚úÖ 7. Naive model testen\n",
    "# ----------------------------\n",
    "y_pred_naive, metrics_naive = run_naive_model(test_df, target_column=\"price\", lag=24)\n",
    "print(\"‚úÖ Naive RMSE:\", metrics_naive[\"rmse\"])\n",
    "\n",
    "# ----------------------------\n",
    "# ‚úÖ 8. SARIMAX model testen\n",
    "# ----------------------------\n",
    "try:\n",
    "    order, seasonal_order = auto_arima_order(train_df, target_column=\"price\", m=24)\n",
    "    y_pred_sarimax, metrics_sarimax = run_sarimax(\n",
    "        train_df,\n",
    "        test_df,\n",
    "        target_column=\"price\",\n",
    "        order=order,\n",
    "        seasonal_order=seasonal_order\n",
    "    )\n",
    "    if y_pred_sarimax is not None:\n",
    "        print(\"‚úÖ SARIMAX RMSE:\", metrics_sarimax[\"rmse\"])\n",
    "    else:\n",
    "        print(\"‚ÑπÔ∏è SARIMAX: combination already tested\")\n",
    "except Exception as e:\n",
    "    print(\"‚ùå SARIMAX failed:\", e)\n",
    "\n",
    "print(\"üéØ Test pipeline run complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3d16b263",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log database initialized at src/data/logs.db\n"
     ]
    },
    {
     "ename": "DatabaseError",
     "evalue": "Execution failed on sql 'SELECT * FROM master_warp': no such table: master_warp",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOperationalError\u001b[0m                          Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\dai\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\io\\sql.py:2674\u001b[0m, in \u001b[0;36mSQLiteDatabase.execute\u001b[1;34m(self, sql, params)\u001b[0m\n\u001b[0;32m   2673\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 2674\u001b[0m     \u001b[43mcur\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43msql\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2675\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cur\n",
      "\u001b[1;31mOperationalError\u001b[0m: no such table: master_warp",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mDatabaseError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 247\u001b[0m\n\u001b[0;32m    245\u001b[0m \u001b[38;5;66;03m# Read the master_warp table into a pandas DataFrame\u001b[39;00m\n\u001b[0;32m    246\u001b[0m query \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSELECT * FROM master_warp\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 247\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_sql_query\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;66;03m# Convert datetime column from string to proper datetime format with UTC timezone\u001b[39;00m\n\u001b[0;32m    250\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdatetime\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mto_datetime(df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdatetime\u001b[39m\u001b[38;5;124m'\u001b[39m], utc\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\dai\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\io\\sql.py:526\u001b[0m, in \u001b[0;36mread_sql_query\u001b[1;34m(sql, con, index_col, coerce_float, params, parse_dates, chunksize, dtype, dtype_backend)\u001b[0m\n\u001b[0;32m    523\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m dtype_backend \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mno_default\n\u001b[0;32m    525\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m pandasSQL_builder(con) \u001b[38;5;28;01mas\u001b[39;00m pandas_sql:\n\u001b[1;32m--> 526\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpandas_sql\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_query\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    527\u001b[0m \u001b[43m        \u001b[49m\u001b[43msql\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    528\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindex_col\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex_col\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    529\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    530\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcoerce_float\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcoerce_float\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    531\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparse_dates\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparse_dates\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    532\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunksize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    533\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    534\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype_backend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype_backend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    535\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\dai\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\io\\sql.py:2738\u001b[0m, in \u001b[0;36mSQLiteDatabase.read_query\u001b[1;34m(self, sql, index_col, coerce_float, parse_dates, params, chunksize, dtype, dtype_backend)\u001b[0m\n\u001b[0;32m   2727\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mread_query\u001b[39m(\n\u001b[0;32m   2728\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   2729\u001b[0m     sql,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2736\u001b[0m     dtype_backend: DtypeBackend \u001b[38;5;241m|\u001b[39m Literal[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnumpy\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnumpy\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   2737\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Iterator[DataFrame]:\n\u001b[1;32m-> 2738\u001b[0m     cursor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43msql\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2739\u001b[0m     columns \u001b[38;5;241m=\u001b[39m [col_desc[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m col_desc \u001b[38;5;129;01min\u001b[39;00m cursor\u001b[38;5;241m.\u001b[39mdescription]\n\u001b[0;32m   2741\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\dai\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\io\\sql.py:2686\u001b[0m, in \u001b[0;36mSQLiteDatabase.execute\u001b[1;34m(self, sql, params)\u001b[0m\n\u001b[0;32m   2683\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ex \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01minner_exc\u001b[39;00m\n\u001b[0;32m   2685\u001b[0m ex \u001b[38;5;241m=\u001b[39m DatabaseError(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExecution failed on sql \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msql\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 2686\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m ex \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mexc\u001b[39;00m\n",
      "\u001b[1;31mDatabaseError\u001b[0m: Execution failed on sql 'SELECT * FROM master_warp': no such table: master_warp"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "import warnings\n",
    "import time\n",
    "import json\n",
    "import datetime\n",
    "import os\n",
    "from pathlib import Path\n",
    "warnings.filterwarnings('ignore')  # Suppress warnings for cleaner output\n",
    "\n",
    "# Function to initialize the log database\n",
    "def initialize_log_database(db_path='src/data/logs.db'):\n",
    "    \"\"\"Create the model_performance_log table if it doesn't exist\"\"\"\n",
    "    \n",
    "    # Ensure the directory exists\n",
    "    Path(os.path.dirname(db_path)).mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Connect to the database\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    # Create the model performance log table if it doesn't exist\n",
    "    cursor.execute('''\n",
    "    CREATE TABLE IF NOT EXISTS model_performance_log (\n",
    "        id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "        timestamp TEXT,\n",
    "        model_name TEXT,\n",
    "        model_version TEXT,\n",
    "        features_used TEXT,\n",
    "        parameters TEXT,\n",
    "        train_start_date TEXT,\n",
    "        train_end_date TEXT,\n",
    "        validation_start_date TEXT,\n",
    "        validation_end_date TEXT,\n",
    "        overall_rmse REAL,\n",
    "        min_daily_rmse REAL,\n",
    "        max_daily_rmse REAL,\n",
    "        avg_daily_rmse REAL,\n",
    "        hourly_rmse_distribution TEXT,\n",
    "        run_time_seconds REAL,\n",
    "        notes TEXT\n",
    "    )\n",
    "    ''')\n",
    "    \n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "    \n",
    "    print(f\"Log database initialized at {db_path}\")\n",
    "    return db_path\n",
    "\n",
    "# Function to log model performance\n",
    "def log_model_performance(\n",
    "    model_name,\n",
    "    model_version,\n",
    "    features_used,\n",
    "    parameters,\n",
    "    train_start_date,\n",
    "    train_end_date,\n",
    "    validation_start_date,\n",
    "    validation_end_date,\n",
    "    forecast_vs_actual,\n",
    "    daily_rmse,\n",
    "    run_time_seconds,\n",
    "    notes=\"\",\n",
    "    db_path='src/data/logs.db'\n",
    "):\n",
    "    \"\"\"\n",
    "    Log the performance of a model run to the database\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    model_name : str\n",
    "        Name of the model (e.g., 'SARIMAX', 'Naive', 'XGBoost', 'Prophet')\n",
    "    model_version : str\n",
    "        Version or configuration of the model (e.g., 'Basic', 'Enhanced', 'Full-Feature')\n",
    "    features_used : list\n",
    "        List of feature names used in the model\n",
    "    parameters : dict\n",
    "        Dictionary of model parameters\n",
    "    train_start_date, train_end_date : datetime\n",
    "        Start and end dates of the training period\n",
    "    validation_start_date, validation_end_date : datetime\n",
    "        Start and end dates of the validation period\n",
    "    forecast_vs_actual : DataFrame\n",
    "        DataFrame containing forecast and actual values with 'rmse' column\n",
    "    daily_rmse : Series\n",
    "        Series of daily RMSE values\n",
    "    run_time_seconds : float\n",
    "        Model training and prediction time in seconds\n",
    "    notes : str, optional\n",
    "        Additional notes about the model run\n",
    "    db_path : str, optional\n",
    "        Path to the database file\n",
    "    \"\"\"\n",
    "    \n",
    "    # Calculate performance metrics\n",
    "    overall_rmse = np.sqrt(mean_squared_error(\n",
    "        forecast_vs_actual['Price'], \n",
    "        forecast_vs_actual['Price_forecast']\n",
    "    ))\n",
    "    \n",
    "    min_daily_rmse = daily_rmse.min()\n",
    "    max_daily_rmse = daily_rmse.max()\n",
    "    avg_daily_rmse = daily_rmse.mean()\n",
    "    \n",
    "    # Get hourly RMSE distribution statistics\n",
    "    hourly_rmse_stats = {\n",
    "        'min': float(forecast_vs_actual['rmse'].min()),\n",
    "        'max': float(forecast_vs_actual['rmse'].max()),\n",
    "        'mean': float(forecast_vs_actual['rmse'].mean()),\n",
    "        'median': float(forecast_vs_actual['rmse'].median()),\n",
    "        'q25': float(forecast_vs_actual['rmse'].quantile(0.25)),\n",
    "        'q75': float(forecast_vs_actual['rmse'].quantile(0.75)),\n",
    "        'std': float(forecast_vs_actual['rmse'].std())\n",
    "    }\n",
    "    \n",
    "    # Connect to the database\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    # Insert the log entry\n",
    "    cursor.execute('''\n",
    "    INSERT INTO model_performance_log (\n",
    "        timestamp,\n",
    "        model_name,\n",
    "        model_version,\n",
    "        features_used,\n",
    "        parameters,\n",
    "        train_start_date,\n",
    "        train_end_date,\n",
    "        validation_start_date,\n",
    "        validation_end_date,\n",
    "        overall_rmse,\n",
    "        min_daily_rmse,\n",
    "        max_daily_rmse,\n",
    "        avg_daily_rmse,\n",
    "        hourly_rmse_distribution,\n",
    "        run_time_seconds,\n",
    "        notes\n",
    "    ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\n",
    "    ''', (\n",
    "        datetime.datetime.now().isoformat(),\n",
    "        model_name,\n",
    "        model_version,\n",
    "        json.dumps(features_used),\n",
    "        json.dumps(parameters),\n",
    "        train_start_date.isoformat(),\n",
    "        train_end_date.isoformat(),\n",
    "        validation_start_date.isoformat(),\n",
    "        validation_end_date.isoformat(),\n",
    "        overall_rmse,\n",
    "        min_daily_rmse,\n",
    "        max_daily_rmse,\n",
    "        avg_daily_rmse,\n",
    "        json.dumps(hourly_rmse_stats),\n",
    "        run_time_seconds,\n",
    "        notes\n",
    "    ))\n",
    "    \n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "    \n",
    "    print(f\"Model performance logged to {db_path}\")\n",
    "    return True\n",
    "\n",
    "# Function to get the logged model performances\n",
    "def get_model_performances(db_path='src/data/logs.db'):\n",
    "    \"\"\"Retrieve all model performance logs as a DataFrame\"\"\"\n",
    "    \n",
    "    conn = sqlite3.connect(db_path)\n",
    "    \n",
    "    # Query the database\n",
    "    query = \"SELECT * FROM model_performance_log ORDER BY timestamp DESC\"\n",
    "    logs_df = pd.read_sql_query(query, conn)\n",
    "    \n",
    "    conn.close()\n",
    "    \n",
    "    return logs_df\n",
    "\n",
    "# Function to create a comparison table of model performances\n",
    "def compare_models(db_path='src/data/logs.db', latest_only=True):\n",
    "    \"\"\"\n",
    "    Create a comparison table of model performances\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    db_path : str, optional\n",
    "        Path to the database file\n",
    "    latest_only : bool, optional\n",
    "        If True, only show the latest run of each model type\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    DataFrame\n",
    "        Comparison table of model performances\n",
    "    \"\"\"\n",
    "    \n",
    "    logs_df = get_model_performances(db_path)\n",
    "    \n",
    "    if logs_df.empty:\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # If latest_only is True, get only the latest run for each model type\n",
    "    if latest_only:\n",
    "        # Get latest run for each combination of model_name and model_version\n",
    "        logs_df['timestamp'] = pd.to_datetime(logs_df['timestamp'])\n",
    "        idx = logs_df.groupby(['model_name', 'model_version'])['timestamp'].idxmax()\n",
    "        logs_df = logs_df.loc[idx]\n",
    "    \n",
    "    # Select relevant columns for comparison\n",
    "    comparison_df = logs_df[[\n",
    "        'model_name', \n",
    "        'model_version', \n",
    "        'overall_rmse', \n",
    "        'avg_daily_rmse',\n",
    "        'min_daily_rmse',\n",
    "        'max_daily_rmse',\n",
    "        'run_time_seconds',\n",
    "        'timestamp'\n",
    "    ]].copy()\n",
    "    \n",
    "    # Sort by overall RMSE (best performing models first)\n",
    "    comparison_df = comparison_df.sort_values('overall_rmse')\n",
    "    \n",
    "    return comparison_df\n",
    "\n",
    "# Initialize the log database\n",
    "db_path = initialize_log_database()\n",
    "\n",
    "# Start timing the model run\n",
    "start_time = time.time()\n",
    "\n",
    "# Connect to the SQLite database\n",
    "conn = sqlite3.connect('src/data/WARP.db')\n",
    "\n",
    "# Read the master_warp table into a pandas DataFrame\n",
    "query = \"SELECT * FROM master_warp\"\n",
    "df = pd.read_sql_query(query, conn)\n",
    "\n",
    "# Convert datetime column from string to proper datetime format with UTC timezone\n",
    "df['datetime'] = pd.to_datetime(df['datetime'], utc=True)\n",
    "\n",
    "# Define data ranges for SARIMAX model\n",
    "train_start = pd.to_datetime('2025-01-01').tz_localize('UTC')\n",
    "observation_start = pd.to_datetime('2025-04-17').tz_localize('UTC')\n",
    "validation_start = pd.to_datetime('2025-04-24').tz_localize('UTC')\n",
    "validation_end = pd.to_datetime('2025-04-30 23:00:00').tz_localize('UTC')\n",
    "\n",
    "# Filter the data for each period\n",
    "train_data = df[(df['datetime'] >= train_start) & (df['datetime'] < observation_start)]\n",
    "observation_data = df[(df['datetime'] >= observation_start) & (df['datetime'] < validation_start)]\n",
    "validation_data = df[(df['datetime'] >= validation_start) & (df['datetime'] <= validation_end)]\n",
    "\n",
    "print(f\"Training data: {len(train_data)} hours\")\n",
    "print(f\"Observation data: {len(observation_data)} hours\")\n",
    "print(f\"Validation data: {len(validation_data)} hours\")\n",
    "\n",
    "# Sort the data by datetime\n",
    "train_data = train_data.sort_values('datetime')\n",
    "observation_data = observation_data.sort_values('datetime')\n",
    "validation_data = validation_data.sort_values('datetime')\n",
    "\n",
    "# ---- SARIMAX Model Implementation with All Features ----\n",
    "# Combine train and observation data for model fitting\n",
    "model_data = pd.concat([train_data, observation_data])\n",
    "model_data = model_data.sort_values('datetime')\n",
    "\n",
    "# Define all features we will use (excluding datetime, Price, and derived columns)\n",
    "feature_columns = [\n",
    "    # Flow features\n",
    "    'Flow_BE_to_NL', 'Flow_NL_to_BE', 'Flow_DE_to_NL', 'Flow_NL_to_DE',\n",
    "    'Flow_GB_to_NL', 'Flow_NL_to_GB', 'Flow_DK_to_NL', 'Flow_NL_to_DK',\n",
    "    'Flow_NO_to_NL', 'Flow_NL_to_NO', 'Flow_BE', 'Flow_DE', 'Flow_GB',\n",
    "    'Flow_DK', 'Flow_NO', 'Total_Flow',\n",
    "    \n",
    "    # Weather features\n",
    "    'temperature_2m', 'wind_speed_10m', 'apparent_temperature', 'cloud_cover',\n",
    "    'snowfall', 'diffuse_radiation', 'direct_normal_irradiance', 'shortwave_radiation',\n",
    "    \n",
    "    # Capacity features\n",
    "    'ned.capacity', 'ned.volume', 'ned.percentage',\n",
    "    \n",
    "    # Demand feature\n",
    "    'Load'\n",
    "]\n",
    "\n",
    "# Pre-existing time features from the dataset\n",
    "time_features = [\n",
    "    'hour_sin', 'hour_cos', 'weekday_sin', 'weekday_cos',\n",
    "    'yearday_sin', 'yearday_cos', 'is_holiday', 'is_weekend', 'is_non_working_day'\n",
    "]\n",
    "\n",
    "# Check which features are available in the data\n",
    "available_features = []\n",
    "for feature in feature_columns:\n",
    "    if feature in model_data.columns:\n",
    "        available_features.append(feature)\n",
    "\n",
    "for feature in time_features:\n",
    "    if feature in model_data.columns:\n",
    "        available_features.append(feature)\n",
    "\n",
    "print(f\"Using {len(available_features)} features in the model:\")\n",
    "print(available_features)\n",
    "\n",
    "# Set the datetime as index for the time series analysis\n",
    "model_data_ts = model_data[['datetime', 'Price'] + available_features].set_index('datetime')\n",
    "\n",
    "# Initialize the scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Scale all features except binary indicators\n",
    "non_binary_features = [f for f in available_features if f not in ['is_holiday', 'is_weekend', 'is_non_working_day']]\n",
    "model_data_ts[non_binary_features] = scaler.fit_transform(model_data_ts[non_binary_features])\n",
    "\n",
    "# Prepare exogenous variables for model\n",
    "exog = model_data_ts[available_features]\n",
    "\n",
    "# Fit SARIMAX model\n",
    "print(\"Fitting SARIMAX model with all available features...\")\n",
    "model = SARIMAX(\n",
    "    model_data_ts['Price'],\n",
    "    exog=exog,\n",
    "    order=(1, 1, 1),          # (p,d,q)\n",
    "    seasonal_order=(1, 1, 1, 24),  # (P,D,Q,s) - 24 for hourly data with daily seasonality\n",
    "    enforce_stationarity=False,\n",
    "    enforce_invertibility=False\n",
    ")\n",
    "\n",
    "# Fit the model with limited iterations to speed up the process\n",
    "results = model.fit(disp=False, maxiter=50)\n",
    "print(\"Model fitted successfully.\")\n",
    "\n",
    "# Prepare exogenous variables for forecasting the validation period\n",
    "validation_exog = validation_data[['datetime'] + available_features].copy()\n",
    "\n",
    "# Scale the validation features using the same scaler\n",
    "validation_exog[non_binary_features] = scaler.transform(validation_exog[non_binary_features])\n",
    "\n",
    "# Set the datetime as index\n",
    "validation_exog = validation_exog.set_index('datetime')[available_features]\n",
    "\n",
    "# Forecast the validation period\n",
    "print(\"Forecasting validation period...\")\n",
    "forecast = results.get_forecast(steps=len(validation_exog), exog=validation_exog)\n",
    "forecast_mean = forecast.predicted_mean\n",
    "forecast_ci = forecast.conf_int()\n",
    "\n",
    "# Create a dataframe with the forecast\n",
    "sarimax_forecast = pd.DataFrame({\n",
    "    'datetime': validation_data['datetime'],\n",
    "    'Price_forecast': forecast_mean.values\n",
    "})\n",
    "\n",
    "# Merge with actual validation data to compare\n",
    "forecast_vs_actual = sarimax_forecast.merge(\n",
    "    validation_data[['datetime', 'Price']], \n",
    "    on='datetime', \n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# ----- Calculate RMSE -----\n",
    "# Overall RMSE\n",
    "overall_rmse = np.sqrt(mean_squared_error(forecast_vs_actual['Price'], forecast_vs_actual['Price_forecast']))\n",
    "print(f\"\\nOverall RMSE: {overall_rmse:.2f}\")\n",
    "\n",
    "# RMSE per hour (for each of the 168 hours in the validation period)\n",
    "forecast_vs_actual['hour_num'] = range(1, len(forecast_vs_actual) + 1)\n",
    "forecast_vs_actual['squared_error'] = (forecast_vs_actual['Price'] - forecast_vs_actual['Price_forecast']) ** 2\n",
    "forecast_vs_actual['rmse'] = np.sqrt(forecast_vs_actual['squared_error'])\n",
    "\n",
    "# RMSE per day\n",
    "forecast_vs_actual['date'] = forecast_vs_actual['datetime'].dt.date\n",
    "daily_rmse = forecast_vs_actual.groupby('date').apply(\n",
    "    lambda x: np.sqrt(mean_squared_error(x['Price'], x['Price_forecast']))\n",
    ")\n",
    "print(\"\\nRMSE per day:\")\n",
    "print(daily_rmse)\n",
    "\n",
    "# Calculate run time\n",
    "run_time_seconds = time.time() - start_time\n",
    "\n",
    "# Log the model performance\n",
    "log_model_performance(\n",
    "    model_name=\"SARIMAX\",\n",
    "    model_version=\"Full-Feature\",\n",
    "    features_used=available_features,\n",
    "    parameters={\n",
    "        \"order\": \"(1,1,1)\",\n",
    "        \"seasonal_order\": \"(1,1,1,24)\",\n",
    "        \"enforce_stationarity\": False,\n",
    "        \"enforce_invertibility\": False,\n",
    "        \"maxiter\": 50\n",
    "    },\n",
    "    train_start_date=train_start,\n",
    "    train_end_date=observation_start - pd.Timedelta(seconds=1),\n",
    "    validation_start_date=validation_start,\n",
    "    validation_end_date=validation_end,\n",
    "    forecast_vs_actual=forecast_vs_actual,\n",
    "    daily_rmse=daily_rmse,\n",
    "    run_time_seconds=run_time_seconds,\n",
    "    notes=\"SARIMAX model with all available features\"\n",
    ")\n",
    "\n",
    "# ----- Create interactive Plotly figure without legend -----\n",
    "fig = make_subplots(\n",
    "    rows=1, cols=1,\n",
    "    specs=[[{\"secondary_y\": True}]]\n",
    ")\n",
    "\n",
    "# Add observation data (last week of training)\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=observation_data['datetime'],\n",
    "        y=observation_data['Price'],\n",
    "        mode='lines',\n",
    "        name='Observed Prices (Apr 17-23)',\n",
    "        line=dict(color='royalblue'),\n",
    "        showlegend=False\n",
    "    )\n",
    ")\n",
    "\n",
    "# Add actual validation data\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=validation_data['datetime'],\n",
    "        y=validation_data['Price'],\n",
    "        mode='lines',\n",
    "        name='Actual Prices (Apr 24-30)',\n",
    "        line=dict(color='green'),\n",
    "        showlegend=False\n",
    "    )\n",
    ")\n",
    "\n",
    "# Add SARIMAX forecast\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=sarimax_forecast['datetime'],\n",
    "        y=sarimax_forecast['Price_forecast'],\n",
    "        mode='lines',\n",
    "        name='SARIMAX Forecast (Apr 24-30)',\n",
    "        line=dict(color='red', dash='dash'),\n",
    "        showlegend=False\n",
    "    )\n",
    ")\n",
    "\n",
    "# Add a vertical line using shape\n",
    "fig.add_shape(\n",
    "    type=\"line\",\n",
    "    x0=validation_start.strftime('%Y-%m-%d %H:%M:%S+00:00'),\n",
    "    x1=validation_start.strftime('%Y-%m-%d %H:%M:%S+00:00'),\n",
    "    y0=0,\n",
    "    y1=1,\n",
    "    yref=\"paper\",\n",
    "    line=dict(color=\"black\", width=1, dash=\"dash\"),\n",
    ")\n",
    "\n",
    "# Add annotation for forecast start\n",
    "fig.add_annotation(\n",
    "    x=validation_start.strftime('%Y-%m-%d %H:%M:%S+00:00'),\n",
    "    y=1,\n",
    "    yref=\"paper\",\n",
    "    text=\"Forecast Start\",\n",
    "    showarrow=False,\n",
    "    xanchor=\"left\",\n",
    "    yanchor=\"bottom\"\n",
    ")\n",
    "\n",
    "# Update layout for white background and other styles\n",
    "fig.update_layout(\n",
    "    title='Price Time Series: Observation, Actual, and Full-Feature SARIMAX Forecast',\n",
    "    xaxis_title='Date',\n",
    "    yaxis_title='Price (‚Ç¨/MWh)',\n",
    "    hovermode='x unified',\n",
    "    plot_bgcolor='white',\n",
    "    paper_bgcolor='white',\n",
    "    font=dict(size=12),\n",
    "    height=600,\n",
    "    width=1000,\n",
    "    margin=dict(l=50, r=50, t=50, b=50),\n",
    "    showlegend=False\n",
    ")\n",
    "\n",
    "# Update x-axis to show all hours\n",
    "fig.update_xaxes(\n",
    "    tickformat='%b %d %H:%M',\n",
    "    tickangle=-45,\n",
    "    tickmode='auto',\n",
    "    nticks=24,\n",
    "    gridcolor='lightgrey'\n",
    ")\n",
    "\n",
    "fig.update_yaxes(gridcolor='lightgrey')\n",
    "\n",
    "# Show the interactive plot\n",
    "fig.show()\n",
    "\n",
    "# ----- Create a table with hourly RMSE -----\n",
    "# Prepare the data for the table\n",
    "hourly_rmse_table = forecast_vs_actual[['datetime', 'Price', 'Price_forecast', 'rmse']].copy()\n",
    "hourly_rmse_table['hour'] = hourly_rmse_table['datetime'].dt.hour\n",
    "hourly_rmse_table['date'] = hourly_rmse_table['datetime'].dt.date\n",
    "hourly_rmse_table['datetime_str'] = hourly_rmse_table['datetime'].dt.strftime('%Y-%m-%d %H:%M')\n",
    "\n",
    "# Create a nicer formatted table for display\n",
    "hourly_rmse_display = hourly_rmse_table[['datetime_str', 'Price', 'Price_forecast', 'rmse']].copy()\n",
    "hourly_rmse_display.columns = ['DateTime', 'Actual Price', 'Forecast Price', 'RMSE']\n",
    "\n",
    "# Print first few rows of the table\n",
    "print(\"\\nHourly RMSE (first 10 rows):\")\n",
    "print(hourly_rmse_display.head(10))\n",
    "\n",
    "# Create a full HTML table for all hours\n",
    "hourly_table = go.Figure(data=[go.Table(\n",
    "    header=dict(\n",
    "        values=list(hourly_rmse_display.columns),\n",
    "        fill_color='royalblue',\n",
    "        align='left',\n",
    "        font=dict(color='white', size=12)\n",
    "    ),\n",
    "    cells=dict(\n",
    "        values=[hourly_rmse_display[col] for col in hourly_rmse_display.columns],\n",
    "        fill_color='lavender',\n",
    "        align='left'\n",
    "    )\n",
    ")])\n",
    "\n",
    "hourly_table.update_layout(\n",
    "    title='Hourly RMSE for All 168 Hours - Full-Feature SARIMAX Model',\n",
    "    height=800,\n",
    "    width=1000\n",
    ")\n",
    "\n",
    "# Show the table\n",
    "hourly_table.show()\n",
    "\n",
    "# Get model comparison\n",
    "comparison_table = compare_models()\n",
    "print(\"\\nModel Comparison:\")\n",
    "print(comparison_table)\n",
    "\n",
    "print(\"\\nFull-Feature SARIMAX model analysis complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b130fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Connect to the SQLite database\n",
    "conn = sqlite3.connect('src/data/WARP.db')\n",
    "\n",
    "# Read the master_warp table into a pandas DataFrame\n",
    "query = \"SELECT * FROM master_warp\"\n",
    "df = pd.read_sql_query(query, conn)\n",
    "\n",
    "# Convert datetime column from string to proper datetime format with UTC timezone\n",
    "df['datetime'] = pd.to_datetime(df['datetime'], utc=True)\n",
    "\n",
    "# Define our time periods\n",
    "# Week for observations (training): Apr 17-23, 2025\n",
    "# Week for validation: Apr 24-30, 2025\n",
    "observation_start = pd.to_datetime('2025-04-17').tz_localize('UTC')\n",
    "validation_start = pd.to_datetime('2025-04-24').tz_localize('UTC')\n",
    "validation_end = pd.to_datetime('2025-04-30 23:00:00').tz_localize('UTC')\n",
    "\n",
    "# Filter the data for each period\n",
    "observation_data = df[(df['datetime'] >= observation_start) & (df['datetime'] < validation_start)]\n",
    "validation_data = df[(df['datetime'] >= validation_start) & (df['datetime'] <= validation_end)]\n",
    "\n",
    "print(f\"Observation data: {len(observation_data)} hours\")\n",
    "print(f\"Validation data: {len(validation_data)} hours\")\n",
    "\n",
    "# Sort the data by datetime\n",
    "observation_data = observation_data.sort_values('datetime')\n",
    "validation_data = validation_data.sort_values('datetime')\n",
    "\n",
    "# ---- Naive Model Implementation ----\n",
    "# Create a dataframe with the same structure as validation_data\n",
    "naive_forecast = pd.DataFrame({'datetime': validation_data['datetime']})\n",
    "naive_forecast['hour'] = naive_forecast['datetime'].dt.hour\n",
    "naive_forecast['day_of_week'] = naive_forecast['datetime'].dt.dayofweek\n",
    "\n",
    "# For each hour in the forecast, take the price from the same hour and day of week in the observation period\n",
    "forecast_prices = []\n",
    "\n",
    "for i, row in naive_forecast.iterrows():\n",
    "    # Find the matching hour and day of week from the observation data\n",
    "    matching_obs = observation_data[(observation_data['hour'] == row['hour']) & \n",
    "                                   (observation_data['day_of_week'] == row['day_of_week'])]\n",
    "    \n",
    "    # If we have a match, use that price; otherwise use the mean price\n",
    "    if not matching_obs.empty:\n",
    "        forecast_prices.append(matching_obs['Price'].values[0])\n",
    "    else:\n",
    "        forecast_prices.append(observation_data['Price'].mean())\n",
    "\n",
    "naive_forecast['Price_forecast'] = forecast_prices\n",
    "\n",
    "# Merge with actual validation data to compare\n",
    "forecast_vs_actual = naive_forecast.merge(\n",
    "    validation_data[['datetime', 'Price']], \n",
    "    on='datetime', \n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# ----- Calculate RMSE -----\n",
    "# Overall RMSE\n",
    "overall_rmse = np.sqrt(mean_squared_error(forecast_vs_actual['Price'], forecast_vs_actual['Price_forecast']))\n",
    "print(f\"\\nOverall RMSE: {overall_rmse:.2f}\")\n",
    "\n",
    "# RMSE per hour (for each of the 168 hours in the validation period)\n",
    "forecast_vs_actual['hour_num'] = range(1, len(forecast_vs_actual) + 1)\n",
    "forecast_vs_actual['squared_error'] = (forecast_vs_actual['Price'] - forecast_vs_actual['Price_forecast']) ** 2\n",
    "forecast_vs_actual['rmse'] = np.sqrt(forecast_vs_actual['squared_error'])\n",
    "\n",
    "# RMSE per day\n",
    "forecast_vs_actual['date'] = forecast_vs_actual['datetime'].dt.date\n",
    "daily_rmse = forecast_vs_actual.groupby('date').apply(\n",
    "    lambda x: np.sqrt(mean_squared_error(x['Price'], x['Price_forecast']))\n",
    ")\n",
    "print(\"\\nRMSE per day:\")\n",
    "print(daily_rmse)\n",
    "\n",
    "# ----- Create interactive Plotly figure without legend -----\n",
    "fig = make_subplots(\n",
    "    rows=1, cols=1,\n",
    "    specs=[[{\"secondary_y\": True}]]\n",
    ")\n",
    "\n",
    "# Add observation data\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=observation_data['datetime'],\n",
    "        y=observation_data['Price'],\n",
    "        mode='lines',\n",
    "        name='Observed Prices (Apr 17-23)',\n",
    "        line=dict(color='royalblue'),\n",
    "        showlegend=False  # Hide legend\n",
    "    )\n",
    ")\n",
    "\n",
    "# Add actual validation data\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=validation_data['datetime'],\n",
    "        y=validation_data['Price'],\n",
    "        mode='lines',\n",
    "        name='Actual Prices (Apr 24-30)',\n",
    "        line=dict(color='green'),\n",
    "        showlegend=False  # Hide legend\n",
    "    )\n",
    ")\n",
    "\n",
    "# Add naive forecast\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=naive_forecast['datetime'],\n",
    "        y=naive_forecast['Price_forecast'],\n",
    "        mode='lines',\n",
    "        name='Naive Forecast (Apr 24-30)',\n",
    "        line=dict(color='red', dash='dash'),\n",
    "        showlegend=False  # Hide legend\n",
    "    )\n",
    ")\n",
    "\n",
    "# Add a vertical line using shape\n",
    "fig.add_shape(\n",
    "    type=\"line\",\n",
    "    x0=validation_start.strftime('%Y-%m-%d %H:%M:%S+00:00'),\n",
    "    x1=validation_start.strftime('%Y-%m-%d %H:%M:%S+00:00'),\n",
    "    y0=0,\n",
    "    y1=1,\n",
    "    yref=\"paper\",\n",
    "    line=dict(color=\"black\", width=1, dash=\"dash\"),\n",
    ")\n",
    "\n",
    "# Add annotation for forecast start\n",
    "fig.add_annotation(\n",
    "    x=validation_start.strftime('%Y-%m-%d %H:%M:%S+00:00'),\n",
    "    y=1,\n",
    "    yref=\"paper\",\n",
    "    text=\"Forecast Start\",\n",
    "    showarrow=False,\n",
    "    xanchor=\"left\",\n",
    "    yanchor=\"bottom\"\n",
    ")\n",
    "\n",
    "# Update layout for white background and other styles\n",
    "fig.update_layout(\n",
    "    title='Price Time Series: Observation, Actual, and Naive Forecast',\n",
    "    xaxis_title='Date',\n",
    "    yaxis_title='Price (‚Ç¨/MWh)',\n",
    "    hovermode='x unified',\n",
    "    plot_bgcolor='white',\n",
    "    paper_bgcolor='white',\n",
    "    font=dict(size=12),\n",
    "    height=600,\n",
    "    width=1000,\n",
    "    margin=dict(l=50, r=50, t=50, b=50),\n",
    "    showlegend=False  # Ensure legend is hidden\n",
    ")\n",
    "\n",
    "# Update x-axis to show all hours\n",
    "fig.update_xaxes(\n",
    "    tickformat='%b %d %H:%M',\n",
    "    tickangle=-45,\n",
    "    tickmode='auto',\n",
    "    nticks=24,  # Show approximately 24 tick marks on the x-axis\n",
    "    gridcolor='lightgrey'\n",
    ")\n",
    "\n",
    "fig.update_yaxes(gridcolor='lightgrey')\n",
    "\n",
    "# Show the interactive plot\n",
    "fig.show()\n",
    "\n",
    "# Save the plotly figure as HTML for interactive viewing\n",
    "fig.write_html(\"naive_forecast_interactive.html\")\n",
    "\n",
    "# ----- Create a table with hourly RMSE -----\n",
    "# Prepare the data for the table\n",
    "hourly_rmse_table = forecast_vs_actual[['datetime', 'Price', 'Price_forecast', 'rmse']].copy()\n",
    "hourly_rmse_table['hour'] = hourly_rmse_table['datetime'].dt.hour\n",
    "hourly_rmse_table['date'] = hourly_rmse_table['datetime'].dt.date\n",
    "hourly_rmse_table['datetime_str'] = hourly_rmse_table['datetime'].dt.strftime('%Y-%m-%d %H:%M')\n",
    "\n",
    "# Create a nicer formatted table for display\n",
    "hourly_rmse_display = hourly_rmse_table[['datetime_str', 'Price', 'Price_forecast', 'rmse']].copy()\n",
    "hourly_rmse_display.columns = ['DateTime', 'Actual Price', 'Forecast Price', 'RMSE']\n",
    "\n",
    "# Print first few rows of the table\n",
    "print(\"\\nHourly RMSE (first 10 rows):\")\n",
    "print(hourly_rmse_display.head(10))\n",
    "\n",
    "# Create a full HTML table for all hours\n",
    "hourly_table = go.Figure(data=[go.Table(\n",
    "    header=dict(\n",
    "        values=list(hourly_rmse_display.columns),\n",
    "        fill_color='royalblue',\n",
    "        align='left',\n",
    "        font=dict(color='white', size=12)\n",
    "    ),\n",
    "    cells=dict(\n",
    "        values=[hourly_rmse_display[col] for col in hourly_rmse_display.columns],\n",
    "        fill_color='lavender',\n",
    "        align='left'\n",
    "    )\n",
    ")])\n",
    "\n",
    "hourly_table.update_layout(\n",
    "    title='Hourly RMSE for All 168 Hours',\n",
    "    height=800,  # Taller to show more rows\n",
    "    width=1000\n",
    ")\n",
    "\n",
    "# Show the table\n",
    "hourly_table.show()\n",
    "\n",
    "# Save the hourly RMSE table to HTML\n",
    "hourly_table.write_html(\"hourly_rmse_table.html\")\n",
    "\n",
    "# Save the forecast and validation results to CSV\n",
    "forecast_vs_actual[['datetime', 'Price', 'Price_forecast', 'rmse']].to_csv('naive_forecast_validation.csv', index=False)\n",
    "print(\"\\nValidation results saved to 'naive_forecast_validation.csv'\")\n",
    "print(\"Interactive plot saved to 'naive_forecast_interactive.html'\")\n",
    "print(\"Hourly RMSE table saved to 'hourly_rmse_table.html'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea6cd729",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "import warnings\n",
    "import time\n",
    "import json\n",
    "import datetime\n",
    "import os\n",
    "from pathlib import Path\n",
    "warnings.filterwarnings('ignore')  # Suppress warnings for cleaner output\n",
    "\n",
    "# Function to initialize the log database\n",
    "def initialize_log_database(db_path='src/data/logs.db'):\n",
    "    \"\"\"Create the model_performance_log table if it doesn't exist\"\"\"\n",
    "    \n",
    "    # Ensure the directory exists\n",
    "    Path(os.path.dirname(db_path)).mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Connect to the database\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    # Create the model performance log table if it doesn't exist\n",
    "    cursor.execute('''\n",
    "    CREATE TABLE IF NOT EXISTS model_performance_log (\n",
    "        id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "        timestamp TEXT,\n",
    "        model_name TEXT,\n",
    "        model_version TEXT,\n",
    "        features_used TEXT,\n",
    "        parameters TEXT,\n",
    "        train_start_date TEXT,\n",
    "        train_end_date TEXT,\n",
    "        validation_start_date TEXT,\n",
    "        validation_end_date TEXT,\n",
    "        overall_rmse REAL,\n",
    "        min_daily_rmse REAL,\n",
    "        max_daily_rmse REAL,\n",
    "        avg_daily_rmse REAL,\n",
    "        hourly_rmse_distribution TEXT,\n",
    "        run_time_seconds REAL,\n",
    "        notes TEXT\n",
    "    )\n",
    "    ''')\n",
    "    \n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "    \n",
    "    print(f\"Log database initialized at {db_path}\")\n",
    "    return db_path\n",
    "\n",
    "# Function to log model performance\n",
    "def log_model_performance(\n",
    "    model_name,\n",
    "    model_version,\n",
    "    features_used,\n",
    "    parameters,\n",
    "    train_start_date,\n",
    "    train_end_date,\n",
    "    validation_start_date,\n",
    "    validation_end_date,\n",
    "    forecast_vs_actual,\n",
    "    daily_rmse,\n",
    "    run_time_seconds,\n",
    "    notes=\"\",\n",
    "    db_path='src/data/logs.db'\n",
    "):\n",
    "    \"\"\"Log the performance of a model run to the database\"\"\"\n",
    "    \n",
    "    # Calculate performance metrics\n",
    "    overall_rmse = np.sqrt(mean_squared_error(\n",
    "        forecast_vs_actual['Price'], \n",
    "        forecast_vs_actual['Price_forecast']\n",
    "    ))\n",
    "    \n",
    "    min_daily_rmse = daily_rmse.min()\n",
    "    max_daily_rmse = daily_rmse.max()\n",
    "    avg_daily_rmse = daily_rmse.mean()\n",
    "    \n",
    "    # Get hourly RMSE distribution statistics\n",
    "    hourly_rmse_stats = {\n",
    "        'min': float(forecast_vs_actual['rmse'].min()),\n",
    "        'max': float(forecast_vs_actual['rmse'].max()),\n",
    "        'mean': float(forecast_vs_actual['rmse'].mean()),\n",
    "        'median': float(forecast_vs_actual['rmse'].median()),\n",
    "        'q25': float(forecast_vs_actual['rmse'].quantile(0.25)),\n",
    "        'q75': float(forecast_vs_actual['rmse'].quantile(0.75)),\n",
    "        'std': float(forecast_vs_actual['rmse'].std())\n",
    "    }\n",
    "    \n",
    "    # Connect to the database\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    # Insert the log entry\n",
    "    cursor.execute('''\n",
    "    INSERT INTO model_performance_log (\n",
    "        timestamp,\n",
    "        model_name,\n",
    "        model_version,\n",
    "        features_used,\n",
    "        parameters,\n",
    "        train_start_date,\n",
    "        train_end_date,\n",
    "        validation_start_date,\n",
    "        validation_end_date,\n",
    "        overall_rmse,\n",
    "        min_daily_rmse,\n",
    "        max_daily_rmse,\n",
    "        avg_daily_rmse,\n",
    "        hourly_rmse_distribution,\n",
    "        run_time_seconds,\n",
    "        notes\n",
    "    ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\n",
    "    ''', (\n",
    "        datetime.datetime.now().isoformat(),\n",
    "        model_name,\n",
    "        model_version,\n",
    "        json.dumps(features_used),\n",
    "        json.dumps(parameters),\n",
    "        train_start_date.isoformat(),\n",
    "        train_end_date.isoformat(),\n",
    "        validation_start_date.isoformat(),\n",
    "        validation_end_date.isoformat(),\n",
    "        overall_rmse,\n",
    "        min_daily_rmse,\n",
    "        max_daily_rmse,\n",
    "        avg_daily_rmse,\n",
    "        json.dumps(hourly_rmse_stats),\n",
    "        run_time_seconds,\n",
    "        notes\n",
    "    ))\n",
    "    \n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "    \n",
    "    print(f\"Model performance logged to {db_path}\")\n",
    "    return True\n",
    "\n",
    "# Function to get the logged model performances\n",
    "def get_model_performances(db_path='src/data/logs.db'):\n",
    "    \"\"\"Retrieve all model performance logs as a DataFrame\"\"\"\n",
    "    \n",
    "    conn = sqlite3.connect(db_path)\n",
    "    \n",
    "    # Query the database\n",
    "    query = \"SELECT * FROM model_performance_log ORDER BY timestamp DESC\"\n",
    "    logs_df = pd.read_sql_query(query, conn)\n",
    "    \n",
    "    conn.close()\n",
    "    \n",
    "    return logs_df\n",
    "\n",
    "# Function to create a comparison table of model performances\n",
    "def compare_models(db_path='src/data/logs.db', latest_only=True):\n",
    "    \"\"\"Create a comparison table of model performances\"\"\"\n",
    "    \n",
    "    logs_df = get_model_performances(db_path)\n",
    "    \n",
    "    if logs_df.empty:\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # If latest_only is True, get only the latest run for each model type\n",
    "    if latest_only:\n",
    "        # Get latest run for each combination of model_name and model_version\n",
    "        logs_df['timestamp'] = pd.to_datetime(logs_df['timestamp'])\n",
    "        idx = logs_df.groupby(['model_name', 'model_version'])['timestamp'].idxmax()\n",
    "        logs_df = logs_df.loc[idx]\n",
    "    \n",
    "    # Select relevant columns for comparison\n",
    "    comparison_df = logs_df[[\n",
    "        'model_name', \n",
    "        'model_version', \n",
    "        'overall_rmse', \n",
    "        'avg_daily_rmse',\n",
    "        'min_daily_rmse',\n",
    "        'max_daily_rmse',\n",
    "        'run_time_seconds',\n",
    "        'timestamp'\n",
    "    ]].copy()\n",
    "    \n",
    "    # Sort by overall RMSE (best performing models first)\n",
    "    comparison_df = comparison_df.sort_values('overall_rmse')\n",
    "    \n",
    "    return comparison_df\n",
    "\n",
    "# Initialize the log database\n",
    "db_path = initialize_log_database()\n",
    "\n",
    "# Start timing the model run\n",
    "start_time = time.time()\n",
    "\n",
    "# Connect to the SQLite database\n",
    "conn = sqlite3.connect('src/data/WARP.db')\n",
    "\n",
    "# Read the master_warp table into a pandas DataFrame\n",
    "query = \"SELECT * FROM master_warp\"\n",
    "df = pd.read_sql_query(query, conn)\n",
    "\n",
    "# Print column names to check availability\n",
    "print(\"Available columns in the dataset:\")\n",
    "print(df.columns.tolist())\n",
    "\n",
    "# Convert datetime column from string to proper datetime format with UTC timezone\n",
    "df['datetime'] = pd.to_datetime(df['datetime'], utc=True)\n",
    "\n",
    "# Define data ranges for SARIMAX model\n",
    "train_start = pd.to_datetime('2025-01-01').tz_localize('UTC')\n",
    "observation_start = pd.to_datetime('2025-04-17').tz_localize('UTC')\n",
    "validation_start = pd.to_datetime('2025-04-24').tz_localize('UTC')\n",
    "validation_end = pd.to_datetime('2025-04-30 23:00:00').tz_localize('UTC')\n",
    "\n",
    "# Filter the data for each period\n",
    "train_data = df[(df['datetime'] >= train_start) & (df['datetime'] < observation_start)]\n",
    "observation_data = df[(df['datetime'] >= observation_start) & (df['datetime'] < validation_start)]\n",
    "validation_data = df[(df['datetime'] >= validation_start) & (df['datetime'] <= validation_end)]\n",
    "\n",
    "print(f\"Training data: {len(train_data)} hours\")\n",
    "print(f\"Observation data: {len(observation_data)} hours\")\n",
    "print(f\"Validation data: {len(validation_data)} hours\")\n",
    "\n",
    "# Sort the data by datetime\n",
    "train_data = train_data.sort_values('datetime')\n",
    "observation_data = observation_data.sort_values('datetime')\n",
    "validation_data = validation_data.sort_values('datetime')\n",
    "\n",
    "# ---- SARIMAX Model Implementation with Available Features ----\n",
    "# Combine train and observation data for model fitting\n",
    "model_data = pd.concat([train_data, observation_data])\n",
    "model_data = model_data.sort_values('datetime')\n",
    "\n",
    "# Define desired features\n",
    "desired_features = [\n",
    "    # Weather and related\n",
    "    'temperature_2m', 'Total_Flow', 'ned.volume',\n",
    "    \n",
    "    # Check if we have these features\n",
    "    'wind_speed_10m', 'apparent_temperature', 'cloud_cover',\n",
    "    'snowfall', 'diffuse_radiation', 'direct_normal_irradiance', \n",
    "    'shortwave_radiation', 'ned.capacity', 'ned.percentage', 'Load'\n",
    "]\n",
    "\n",
    "# Check which features are available\n",
    "available_features = ['datetime', 'Price']\n",
    "for feature in desired_features:\n",
    "    if feature in model_data.columns:\n",
    "        available_features.append(feature)\n",
    "\n",
    "print(f\"Using {len(available_features) - 2} features in the model:\")  # -2 for datetime and Price\n",
    "print(available_features[2:])  # Skip datetime and Price\n",
    "\n",
    "# Set the datetime as index for the time series analysis\n",
    "model_data_ts = model_data[available_features].set_index('datetime')\n",
    "\n",
    "# Add basic time features\n",
    "model_data_ts['hour'] = model_data_ts.index.hour\n",
    "model_data_ts['day_of_week'] = model_data_ts.index.dayofweek\n",
    "model_data_ts['is_weekend'] = (model_data_ts.index.dayofweek >= 5).astype(int)\n",
    "\n",
    "# Add these time features to our available features list\n",
    "time_features = ['hour', 'day_of_week', 'is_weekend']\n",
    "all_exog_features = available_features[2:] + time_features  # Skip datetime and Price\n",
    "\n",
    "# Define features to scale (all except binary indicators)\n",
    "scale_features = [f for f in all_exog_features if f not in ['is_weekend']]\n",
    "\n",
    "# Initialize the scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Scale the features\n",
    "model_data_ts[scale_features] = scaler.fit_transform(model_data_ts[scale_features])\n",
    "\n",
    "# For this enhanced model, we'll use time features + scaled available variables\n",
    "exog = model_data_ts[all_exog_features]\n",
    "\n",
    "# Fit SARIMAX model\n",
    "print(\"Fitting SARIMAX model with available features...\")\n",
    "model = SARIMAX(\n",
    "    model_data_ts['Price'],\n",
    "    exog=exog,\n",
    "    order=(1, 1, 1),          # (p,d,q) - simple parameters\n",
    "    seasonal_order=(1, 1, 1, 24),  # (P,D,Q,s) - 24 for hourly data with daily seasonality\n",
    "    enforce_stationarity=False,\n",
    "    enforce_invertibility=False\n",
    ")\n",
    "\n",
    "# Fit the model\n",
    "results = model.fit(disp=False)\n",
    "print(\"Model fitted successfully.\")\n",
    "\n",
    "# Prepare exogenous variables for forecasting the validation period\n",
    "validation_exog = validation_data[['datetime'] + available_features[2:]].copy()\n",
    "validation_exog['hour'] = validation_exog['datetime'].dt.hour\n",
    "validation_exog['day_of_week'] = validation_exog['datetime'].dt.dayofweek\n",
    "validation_exog['is_weekend'] = (validation_exog['datetime'].dt.dayofweek >= 5).astype(int)\n",
    "\n",
    "# Scale the validation exogenous variables using the same scaler\n",
    "validation_exog[scale_features] = scaler.transform(validation_exog[scale_features])\n",
    "\n",
    "# Set the datetime as index\n",
    "validation_exog = validation_exog.set_index('datetime')[all_exog_features]\n",
    "\n",
    "# Forecast the validation period\n",
    "print(\"Forecasting validation period...\")\n",
    "forecast = results.get_forecast(steps=len(validation_exog), exog=validation_exog)\n",
    "forecast_mean = forecast.predicted_mean\n",
    "forecast_ci = forecast.conf_int()\n",
    "\n",
    "# Create a dataframe with the forecast\n",
    "sarimax_forecast = pd.DataFrame({\n",
    "    'datetime': validation_data['datetime'],\n",
    "    'Price_forecast': forecast_mean.values\n",
    "})\n",
    "\n",
    "# Merge with actual validation data to compare\n",
    "forecast_vs_actual = sarimax_forecast.merge(\n",
    "    validation_data[['datetime', 'Price']], \n",
    "    on='datetime', \n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# ----- Calculate RMSE -----\n",
    "# Overall RMSE\n",
    "overall_rmse = np.sqrt(mean_squared_error(forecast_vs_actual['Price'], forecast_vs_actual['Price_forecast']))\n",
    "print(f\"\\nOverall RMSE: {overall_rmse:.2f}\")\n",
    "\n",
    "# RMSE per hour (for each of the 168 hours in the validation period)\n",
    "forecast_vs_actual['hour_num'] = range(1, len(forecast_vs_actual) + 1)\n",
    "forecast_vs_actual['squared_error'] = (forecast_vs_actual['Price'] - forecast_vs_actual['Price_forecast']) ** 2\n",
    "forecast_vs_actual['rmse'] = np.sqrt(forecast_vs_actual['squared_error'])\n",
    "\n",
    "# RMSE per day\n",
    "forecast_vs_actual['date'] = forecast_vs_actual['datetime'].dt.date\n",
    "daily_rmse = forecast_vs_actual.groupby('date').apply(\n",
    "    lambda x: np.sqrt(mean_squared_error(x['Price'], x['Price_forecast']))\n",
    ")\n",
    "print(\"\\nRMSE per day:\")\n",
    "print(daily_rmse)\n",
    "\n",
    "# Calculate run time\n",
    "run_time_seconds = time.time() - start_time\n",
    "\n",
    "# Log the model performance\n",
    "log_model_performance(\n",
    "    model_name=\"SARIMAX\",\n",
    "    model_version=\"Available-Features\",\n",
    "    features_used=all_exog_features,\n",
    "    parameters={\n",
    "        \"order\": \"(1,1,1)\",\n",
    "        \"seasonal_order\": \"(1,1,1,24)\",\n",
    "        \"enforce_stationarity\": False,\n",
    "        \"enforce_invertibility\": False\n",
    "    },\n",
    "    train_start_date=train_start,\n",
    "    train_end_date=observation_start - pd.Timedelta(seconds=1),\n",
    "    validation_start_date=validation_start,\n",
    "    validation_end_date=validation_end,\n",
    "    forecast_vs_actual=forecast_vs_actual,\n",
    "    daily_rmse=daily_rmse,\n",
    "    run_time_seconds=run_time_seconds,\n",
    "    notes=f\"SARIMAX model with available features: {', '.join(all_exog_features)}\"\n",
    ")\n",
    "\n",
    "# ----- Create interactive Plotly figure without legend -----\n",
    "fig = make_subplots(\n",
    "    rows=1, cols=1,\n",
    "    specs=[[{\"secondary_y\": True}]]\n",
    ")\n",
    "\n",
    "# Add observation data (last week of training)\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=observation_data['datetime'],\n",
    "        y=observation_data['Price'],\n",
    "        mode='lines',\n",
    "        name='Observed Prices (Apr 17-23)',\n",
    "        line=dict(color='royalblue'),\n",
    "        showlegend=False\n",
    "    )\n",
    ")\n",
    "\n",
    "# Add actual validation data\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=validation_data['datetime'],\n",
    "        y=validation_data['Price'],\n",
    "        mode='lines',\n",
    "        name='Actual Prices (Apr 24-30)',\n",
    "        line=dict(color='green'),\n",
    "        showlegend=False\n",
    "    )\n",
    ")\n",
    "\n",
    "# Add SARIMAX forecast\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=sarimax_forecast['datetime'],\n",
    "        y=sarimax_forecast['Price_forecast'],\n",
    "        mode='lines',\n",
    "        name='SARIMAX Forecast (Apr 24-30)',\n",
    "        line=dict(color='red', dash='dash'),\n",
    "        showlegend=False\n",
    "    )\n",
    ")\n",
    "\n",
    "# Add a vertical line using shape\n",
    "fig.add_shape(\n",
    "    type=\"line\",\n",
    "    x0=validation_start.strftime('%Y-%m-%d %H:%M:%S+00:00'),\n",
    "    x1=validation_start.strftime('%Y-%m-%d %H:%M:%S+00:00'),\n",
    "    y0=0,\n",
    "    y1=1,\n",
    "    yref=\"paper\",\n",
    "    line=dict(color=\"black\", width=1, dash=\"dash\"),\n",
    ")\n",
    "\n",
    "# Add annotation for forecast start\n",
    "fig.add_annotation(\n",
    "    x=validation_start.strftime('%Y-%m-%d %H:%M:%S+00:00'),\n",
    "    y=1,\n",
    "    yref=\"paper\",\n",
    "    text=\"Forecast Start\",\n",
    "    showarrow=False,\n",
    "    xanchor=\"left\",\n",
    "    yanchor=\"bottom\"\n",
    ")\n",
    "\n",
    "# Update layout for white background and other styles\n",
    "fig.update_layout(\n",
    "    title='Price Time Series: Observation, Actual, and SARIMAX Forecast',\n",
    "    xaxis_title='Date',\n",
    "    yaxis_title='Price (‚Ç¨/MWh)',\n",
    "    hovermode='x unified',\n",
    "    plot_bgcolor='white',\n",
    "    paper_bgcolor='white',\n",
    "    font=dict(size=12),\n",
    "    height=600,\n",
    "    width=1000,\n",
    "    margin=dict(l=50, r=50, t=50, b=50),\n",
    "    showlegend=False\n",
    ")\n",
    "\n",
    "# Update x-axis to show all hours\n",
    "fig.update_xaxes(\n",
    "    tickformat='%b %d %H:%M',\n",
    "    tickangle=-45,\n",
    "    tickmode='auto',\n",
    "    nticks=24,\n",
    "    gridcolor='lightgrey'\n",
    ")\n",
    "\n",
    "fig.update_yaxes(gridcolor='lightgrey')\n",
    "\n",
    "# Show the interactive plot\n",
    "fig.show()\n",
    "\n",
    "# ----- Create a table with hourly RMSE -----\n",
    "# Prepare the data for the table\n",
    "hourly_rmse_table = forecast_vs_actual[['datetime', 'Price', 'Price_forecast', 'rmse']].copy()\n",
    "hourly_rmse_table['hour'] = hourly_rmse_table['datetime'].dt.hour\n",
    "hourly_rmse_table['date'] = hourly_rmse_table['datetime'].dt.date\n",
    "hourly_rmse_table['datetime_str'] = hourly_rmse_table['datetime'].dt.strftime('%Y-%m-%d %H:%M')\n",
    "\n",
    "# Create a nicer formatted table for display\n",
    "hourly_rmse_display = hourly_rmse_table[['datetime_str', 'Price', 'Price_forecast', 'rmse']].copy()\n",
    "hourly_rmse_display.columns = ['DateTime', 'Actual Price', 'Forecast Price', 'RMSE']\n",
    "\n",
    "# Print first few rows of the table\n",
    "print(\"\\nHourly RMSE (first 10 rows):\")\n",
    "print(hourly_rmse_display.head(10))\n",
    "\n",
    "# Create a full HTML table for all hours\n",
    "hourly_table = go.Figure(data=[go.Table(\n",
    "    header=dict(\n",
    "        values=list(hourly_rmse_display.columns),\n",
    "        fill_color='royalblue',\n",
    "        align='left',\n",
    "        font=dict(color='white', size=12)\n",
    "    ),\n",
    "    cells=dict(\n",
    "        values=[hourly_rmse_display[col] for col in hourly_rmse_display.columns],\n",
    "        fill_color='lavender',\n",
    "        align='left'\n",
    "    )\n",
    ")])\n",
    "\n",
    "hourly_table.update_layout(\n",
    "    title='Hourly RMSE for All 168 Hours - SARIMAX Model',\n",
    "    height=800,\n",
    "    width=1000\n",
    ")\n",
    "\n",
    "# Show the table\n",
    "hourly_table.show()\n",
    "\n",
    "# Try to get feature importance information\n",
    "try:\n",
    "    # Get the feature importance from the SARIMAX model coefficients\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'Feature': all_exog_features,\n",
    "        'Coefficient': results.params[-len(all_exog_features):]\n",
    "    })\n",
    "    \n",
    "    # Sort by absolute value of coefficients\n",
    "    feature_importance['Abs_Coefficient'] = feature_importance['Coefficient'].abs()\n",
    "    feature_importance = feature_importance.sort_values('Abs_Coefficient', ascending=False)\n",
    "    \n",
    "    print(\"\\nFeature Importance (top features):\")\n",
    "    print(feature_importance.head(10))\n",
    "except Exception as e:\n",
    "    print(f\"\\nCould not display feature importance: {e}\")\n",
    "\n",
    "# Get model comparison\n",
    "comparison_table = compare_models()\n",
    "print(\"\\nModel Comparison:\")\n",
    "print(comparison_table)\n",
    "\n",
    "print(\"\\nSARIMAX model analysis complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0f37979e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Laatste 35 rijen van transform_entsoe_obs in ../src/data/WARP.db:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Timestamp</th>\n",
       "      <th>Load</th>\n",
       "      <th>Price</th>\n",
       "      <th>Forecast_Load</th>\n",
       "      <th>Flow_GB</th>\n",
       "      <th>Flow_NO</th>\n",
       "      <th>Total_Flow</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>2025-05-29 12:00:00+00:00</td>\n",
       "      <td>7270.250000</td>\n",
       "      <td>-0.00200</td>\n",
       "      <td>6918.50</td>\n",
       "      <td>116.75</td>\n",
       "      <td>0.0</td>\n",
       "      <td>116.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>2025-05-29 13:00:00+00:00</td>\n",
       "      <td>7063.000000</td>\n",
       "      <td>-0.00200</td>\n",
       "      <td>6680.75</td>\n",
       "      <td>300.25</td>\n",
       "      <td>0.0</td>\n",
       "      <td>300.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>2025-05-29 14:00:00+00:00</td>\n",
       "      <td>6716.000000</td>\n",
       "      <td>-0.00276</td>\n",
       "      <td>6699.75</td>\n",
       "      <td>740.75</td>\n",
       "      <td>0.0</td>\n",
       "      <td>740.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>2025-05-29 15:00:00+00:00</td>\n",
       "      <td>6588.666667</td>\n",
       "      <td>-0.00159</td>\n",
       "      <td>6948.50</td>\n",
       "      <td>730.75</td>\n",
       "      <td>0.0</td>\n",
       "      <td>730.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>2025-05-29 16:00:00+00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.02000</td>\n",
       "      <td>7925.75</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>2025-05-29 17:00:00+00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.05962</td>\n",
       "      <td>9077.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>2025-05-29 18:00:00+00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.07351</td>\n",
       "      <td>10194.75</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>2025-05-29 19:00:00+00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.07520</td>\n",
       "      <td>11271.75</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>2025-05-29 20:00:00+00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.07340</td>\n",
       "      <td>11671.25</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2025-05-29 21:00:00+00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.07630</td>\n",
       "      <td>11542.25</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2025-05-29 22:00:00+00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.05700</td>\n",
       "      <td>11224.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2025-05-29 23:00:00+00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.04490</td>\n",
       "      <td>10831.75</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2025-05-30 00:00:00+00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.02746</td>\n",
       "      <td>10529.50</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2025-05-30 01:00:00+00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.03348</td>\n",
       "      <td>10226.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2025-05-30 02:00:00+00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.03395</td>\n",
       "      <td>10095.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2025-05-30 03:00:00+00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.03739</td>\n",
       "      <td>10026.75</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2025-05-30 04:00:00+00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.05508</td>\n",
       "      <td>10062.75</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2025-05-30 05:00:00+00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.06773</td>\n",
       "      <td>10139.50</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2025-05-30 06:00:00+00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.06838</td>\n",
       "      <td>10127.50</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2025-05-30 07:00:00+00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.05301</td>\n",
       "      <td>9569.25</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2025-05-30 08:00:00+00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.02913</td>\n",
       "      <td>8508.75</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2025-05-30 09:00:00+00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00183</td>\n",
       "      <td>7964.50</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2025-05-30 10:00:00+00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.00005</td>\n",
       "      <td>7567.75</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2025-05-30 11:00:00+00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.00071</td>\n",
       "      <td>7260.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2025-05-30 12:00:00+00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.00282</td>\n",
       "      <td>7145.25</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2025-05-30 13:00:00+00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.00319</td>\n",
       "      <td>7217.25</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2025-05-30 14:00:00+00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.00172</td>\n",
       "      <td>7089.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2025-05-30 15:00:00+00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.00003</td>\n",
       "      <td>7798.25</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2025-05-30 16:00:00+00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00700</td>\n",
       "      <td>8593.50</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2025-05-30 17:00:00+00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.07004</td>\n",
       "      <td>9680.50</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2025-05-30 18:00:00+00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.11000</td>\n",
       "      <td>10525.75</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2025-05-30 19:00:00+00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.13527</td>\n",
       "      <td>11285.50</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2025-05-30 20:00:00+00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.15018</td>\n",
       "      <td>11704.25</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2025-05-30 21:00:00+00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.13019</td>\n",
       "      <td>11579.75</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2025-05-30 22:00:00+00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.10815</td>\n",
       "      <td>11043.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Timestamp         Load    Price  Forecast_Load  Flow_GB  \\\n",
       "34 2025-05-29 12:00:00+00:00  7270.250000 -0.00200        6918.50   116.75   \n",
       "33 2025-05-29 13:00:00+00:00  7063.000000 -0.00200        6680.75   300.25   \n",
       "32 2025-05-29 14:00:00+00:00  6716.000000 -0.00276        6699.75   740.75   \n",
       "31 2025-05-29 15:00:00+00:00  6588.666667 -0.00159        6948.50   730.75   \n",
       "30 2025-05-29 16:00:00+00:00          NaN  0.02000        7925.75      NaN   \n",
       "29 2025-05-29 17:00:00+00:00          NaN  0.05962        9077.00      NaN   \n",
       "28 2025-05-29 18:00:00+00:00          NaN  0.07351       10194.75      NaN   \n",
       "27 2025-05-29 19:00:00+00:00          NaN  0.07520       11271.75      NaN   \n",
       "26 2025-05-29 20:00:00+00:00          NaN  0.07340       11671.25      NaN   \n",
       "25 2025-05-29 21:00:00+00:00          NaN  0.07630       11542.25      NaN   \n",
       "24 2025-05-29 22:00:00+00:00          NaN  0.05700       11224.00      NaN   \n",
       "23 2025-05-29 23:00:00+00:00          NaN  0.04490       10831.75      NaN   \n",
       "22 2025-05-30 00:00:00+00:00          NaN  0.02746       10529.50      NaN   \n",
       "21 2025-05-30 01:00:00+00:00          NaN  0.03348       10226.00      NaN   \n",
       "20 2025-05-30 02:00:00+00:00          NaN  0.03395       10095.00      NaN   \n",
       "19 2025-05-30 03:00:00+00:00          NaN  0.03739       10026.75      NaN   \n",
       "18 2025-05-30 04:00:00+00:00          NaN  0.05508       10062.75      NaN   \n",
       "17 2025-05-30 05:00:00+00:00          NaN  0.06773       10139.50      NaN   \n",
       "16 2025-05-30 06:00:00+00:00          NaN  0.06838       10127.50      NaN   \n",
       "15 2025-05-30 07:00:00+00:00          NaN  0.05301        9569.25      NaN   \n",
       "14 2025-05-30 08:00:00+00:00          NaN  0.02913        8508.75      NaN   \n",
       "13 2025-05-30 09:00:00+00:00          NaN  0.00183        7964.50      NaN   \n",
       "12 2025-05-30 10:00:00+00:00          NaN -0.00005        7567.75      NaN   \n",
       "11 2025-05-30 11:00:00+00:00          NaN -0.00071        7260.00      NaN   \n",
       "10 2025-05-30 12:00:00+00:00          NaN -0.00282        7145.25      NaN   \n",
       "9  2025-05-30 13:00:00+00:00          NaN -0.00319        7217.25      NaN   \n",
       "8  2025-05-30 14:00:00+00:00          NaN -0.00172        7089.00      NaN   \n",
       "7  2025-05-30 15:00:00+00:00          NaN -0.00003        7798.25      NaN   \n",
       "6  2025-05-30 16:00:00+00:00          NaN  0.00700        8593.50      NaN   \n",
       "5  2025-05-30 17:00:00+00:00          NaN  0.07004        9680.50      NaN   \n",
       "4  2025-05-30 18:00:00+00:00          NaN  0.11000       10525.75      NaN   \n",
       "3  2025-05-30 19:00:00+00:00          NaN  0.13527       11285.50      NaN   \n",
       "2  2025-05-30 20:00:00+00:00          NaN  0.15018       11704.25      NaN   \n",
       "1  2025-05-30 21:00:00+00:00          NaN  0.13019       11579.75      NaN   \n",
       "0  2025-05-30 22:00:00+00:00          NaN  0.10815       11043.00      NaN   \n",
       "\n",
       "    Flow_NO  Total_Flow  \n",
       "34      0.0      116.75  \n",
       "33      0.0      300.25  \n",
       "32      0.0      740.75  \n",
       "31      0.0      730.75  \n",
       "30      NaN         NaN  \n",
       "29      NaN         NaN  \n",
       "28      NaN         NaN  \n",
       "27      NaN         NaN  \n",
       "26      NaN         NaN  \n",
       "25      NaN         NaN  \n",
       "24      NaN         NaN  \n",
       "23      NaN         NaN  \n",
       "22      NaN         NaN  \n",
       "21      NaN         NaN  \n",
       "20      NaN         NaN  \n",
       "19      NaN         NaN  \n",
       "18      NaN         NaN  \n",
       "17      NaN         NaN  \n",
       "16      NaN         NaN  \n",
       "15      NaN         NaN  \n",
       "14      NaN         NaN  \n",
       "13      NaN         NaN  \n",
       "12      NaN         NaN  \n",
       "11      NaN         NaN  \n",
       "10      NaN         NaN  \n",
       "9       NaN         NaN  \n",
       "8       NaN         NaN  \n",
       "7       NaN         NaN  \n",
       "6       NaN         NaN  \n",
       "5       NaN         NaN  \n",
       "4       NaN         NaN  \n",
       "3       NaN         NaN  \n",
       "2       NaN         NaN  \n",
       "1       NaN         NaN  \n",
       "0       NaN         NaN  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import sqlite3\n",
    "\n",
    "def view_last_rows(db_path, table_name, n=35):\n",
    "    print(f\"\\nüîç Laatste {n} rijen van {table_name} in {db_path}:\")\n",
    "    try:\n",
    "        conn = sqlite3.connect(db_path)\n",
    "        query = f\"SELECT * FROM {table_name} ORDER BY Timestamp DESC LIMIT {n}\"\n",
    "        df = pd.read_sql_query(query, conn)\n",
    "        conn.close()\n",
    "\n",
    "        # Converteer Timestamp naar datetime voor leesbaarheid\n",
    "        if 'Timestamp' in df.columns:\n",
    "            df['Timestamp'] = pd.to_datetime(df['Timestamp'], errors='coerce')\n",
    "\n",
    "        # Sorteer weer oplopend zodat je de tijdlijn van oud ‚Üí nieuw ziet\n",
    "        df = df.sort_values(\"Timestamp\")\n",
    "        display(df)  # Werkt in Jupyter Notebook\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Fout bij ophalen van {table_name}: {e}\")\n",
    "\n",
    "# Run deze met je juiste pad\n",
    "view_last_rows(\"../src/data/WARP.db\", \"transform_entsoe_obs\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.10.13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
