{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "384b4d96",
   "metadata": {},
   "source": [
    "# Folder structuur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c2e44b78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📁 ENEXIS/\n",
      "    📄 enexis_master_warp.csv\n",
      "    📄 .cache.sqlite\n",
      "    📄 import pandas as pd v2.py\n",
      "    📄 best_prophet_model.joblib\n",
      "    📄 fetch_entsoe_data.py\n",
      "    📄 .DS_Store\n",
      "    📄 hourly_rmse_table.html\n",
      "    📄 import pandas as pd v3.py\n",
      "    📄 requirements.txt\n",
      "    📄 fetch_entsoe_data_v.py\n",
      "    📄 transform_entsoe.csv\n",
      "    📄 electricity_data_nl_2022_2024_hourly.csv\n",
      "    📄 electricity_data_nl_2022_2024.csv\n",
      "    📄 validation_results.csv\n",
      "    📄 test.py\n",
      "    📄 output.csv\n",
      "    📄 python code ENTSO-E v2.ipynb\n",
      "    📄 README.md\n",
      "    📄 naive_forecast_interactive.html\n",
      "    📄 pipeline_flow.png\n",
      "    📄 python code ENTSO-E.ipynb\n",
      "    📄 debug_.py\n",
      "    📄 .gitignore\n",
      "    📄 pip install entsoe-py.py\n",
      "    📄 actual_total_load_nl_2022_2024.csv\n",
      "    📄 naive_forecast_validation.csv\n",
      "    📄 electricity_data_nl_2022_2025_utc.csv\n",
      "    📄 historical_weekahead_forecasts.csv\n",
      "    📄 import pandas as pd.py\n",
      "    📄 .gitattributes\n",
      "    📄 structuur.txt\n",
      "    📄 raw_entsoe.csv\n",
      "    📄 electricity_data_nl_2022_2024 kolommen.csv\n",
      "    📄 electricity_data_nl_2022_2025_hourly_flow.csv\n",
      "    📄 import pandas as pd v4.py\n",
      "    📄 test.ipynb\n",
      "    📄 step_by_step.md\n",
      "    📄 electricity_data_nl_2022_2025_hourly.csv\n",
      "    📄 cleaned_output_2025-05-05.txt\n",
      "    📁 tests/\n",
      "        📄 evaluate_models.ipynb\n",
      "        📄 test_raw_ned_obs.ipynb\n",
      "        📄 test_transform_ned_obs.ipynb\n",
      "        📄 test_sarimax.ipynb\n",
      "        📄 Test_trainingset.ipynb\n",
      "        📄 test_data.py\n",
      "        📄 test_pipeline.ipynb\n",
      "        📄 test_models.ipynb\n",
      "    📁 the/\n",
      "        📄 pyvenv.cfg\n",
      "        📁 bin/\n",
      "            📄 pip3.9\n",
      "            📄 Activate.ps1\n",
      "            📄 python3\n",
      "            📄 python\n",
      "            📄 pip3\n",
      "            📄 activate.fish\n",
      "            📄 python3.9\n",
      "            📄 pip\n",
      "            📄 activate\n",
      "            📄 activate.csh\n",
      "        📁 lib/\n",
      "        📁 share/\n",
      "    📁 docs/\n",
      "        📄 data_dictionary.md\n",
      "        📄 price pred oxygent.png\n",
      "        📄 readme_MVP.md\n",
      "        📄 index.md\n",
      "        📄 modeling_notes.md\n",
      "        📄 WARP_wireframe.png\n",
      "    📁 workspaces/\n",
      "        📁 twan/\n",
      "            📄 proces_prices-NEW.ipynb\n",
      "            📄 .cache.sqlite\n",
      "            📄 merged_data_price_preds.csv\n",
      "            📄 proces_prices.ipynb\n",
      "            📄 get_prices_NEW.ipynb\n",
      "            📄 get_prices.ipynb\n",
      "            📄 index.md\n",
      "            📄 pred_accuracy.ipynb\n",
      "            📄 get_prices2.ipynb\n",
      "        📁 sharell/\n",
      "            📄 Code merge datasets, plots, regression.ipynb\n",
      "            📄 enexis_master_warp.csv\n",
      "            📄 correlation_matrix.html\n",
      "            📄 entsoe_exploration.py\n",
      "            📄 random forest model.py\n",
      "            📄 entso_load forecast.py\n",
      "            📄 # Pad naar de SQLite-database.py\n",
      "            📄 entso_load forecast attempt 2.py\n",
      "            📄 random forest model 3.ipynb\n",
      "            📄 enexis_mastepr_predictions.csv\n",
      "            📄 index.md\n",
      "            📄 entsoe_ingest.py\n",
      "            📄 Weather_data.ipynb\n",
      "            📄 entsoe_2022-2025_load\n",
      "            📄 ENTSO-E code plots.ipynb\n",
      "            📄 random forest model 2.ipynb\n",
      "            📄 entsoe_test.py\n",
      "            📄 weekahead_forecast_load_2025.csv\n",
      "        📁 sandeep/\n",
      "            📄 ned-plots 2.ipynb\n",
      "            📄 environment.yml\n",
      "            📄 ned-plots.ipynb\n",
      "            📄 index.md\n",
      "            📄 2021 01 05 - Enexis Energy - v1.ipynb\n",
      "        📁 redouan/\n",
      "            📄 GUI_ENERGY_PRICES_202501010000-202601010000.csv\n",
      "            📄 index.md\n",
      "    📁 #/\n",
      "        📄 pyvenv.cfg\n",
      "        📁 bin/\n",
      "            📄 pip3.9\n",
      "            📄 Activate.ps1\n",
      "            📄 python3\n",
      "            📄 python\n",
      "            📄 pip3\n",
      "            📄 activate.fish\n",
      "            📄 python3.9\n",
      "            📄 pip\n",
      "            📄 activate\n",
      "            📄 activate.csh\n",
      "        📁 lib/\n",
      "    📁 .history/\n",
      "        📄 README_20250416111233.md\n",
      "        📄 README_20250416111343.md\n",
      "        📄 README_20250416110720.md\n",
      "        📄 .gitignore_20250326100045\n",
      "        📄 README_20250416111303.md\n",
      "        📄 README_20250416111408.md\n",
      "        📄 README_20250416111257.md\n",
      "        📄 .gitignore_20250326101525\n",
      "        📄 README_20250416110938.md\n",
      "        📄 test_20250228090944.py\n",
      "        📄 test_20250318073940.py\n",
      "        📄 README_20250414152007.md\n",
      "        📄 README_20250416111432.md\n",
      "        📄 README_20250416111447.md\n",
      "        📄 .gitignore_20250407110339\n",
      "        📄 .gitignore_20250509201151\n",
      "        📄 test_20250318073945.py\n",
      "        📄 test_20250318073937.py\n",
      "        📄 README_20250416110728.md\n",
      "        📄 README_20250416111444.md\n",
      "        📄 .gitignore_20250407110310\n",
      "        📄 README_20250416111435.md\n",
      "        📄 .gitignore_20250326101227\n",
      "        📄 .gitignore_20250509201836\n",
      "        📄 README_20250416111441.md\n",
      "        📄 README_20250416111430.md\n",
      "        📄 README_20250416110729.md\n",
      "        📄 README_20250416110805.md\n",
      "        📄 README_20250416111320.md\n",
      "        📄 .gitignore_20250326101526\n",
      "        📄 README_20250416110733.md\n",
      "        📄 README_20250416110746.md\n",
      "        📁 docs/\n",
      "            📄 readme_MVP_20250414100707.md\n",
      "            📄 readme_MVP_20250414100931.md\n",
      "            📄 readme_MVP_20250414165411.md\n",
      "            📄 readme_MVP_20250416111609.md\n",
      "            📄 readme_MVP_20250414100703.md\n",
      "            📄 readme_MVP_20250414100737.md\n",
      "            📄 readme_MVP_20250414165415.md\n",
      "            📄 readme_MVP_20250417081105.md\n",
      "            📄 readme_MVP_20250414100821.md\n",
      "            📄 readme_MVP_20250417092835.md\n",
      "            📄 readme_MVP_20250414165244.md\n",
      "            📄 readme_MVP_20250416111702.md\n",
      "            📄 readme_MVP_20250416111613.md\n",
      "            📄 readme_MVP_20250414100748.md\n",
      "            📄 readme_MVP_20250416111602.md\n",
      "            📄 readme_MVP_20250416111804.md\n",
      "            📄 readme_MVP_20250416111813.md\n",
      "            📄 readme_MVP_20250416111650.md\n",
      "            📄 readme_MVP_20250416111751.md\n",
      "            📄 readme_MVP_20250414165242.md\n",
      "            📄 readme_MVP_20250416111654.md\n",
      "            📄 readme_MVP_20250416111822.md\n",
      "            📄 readme_MVP_20250414165418.md\n",
      "            📄 readme_MVP_20250416111806.md\n",
      "            📄 readme_MVP_20250414165247.md\n",
      "            📄 readme_MVP_20250416111812.md\n",
      "            📄 readme_MVP_20250416111750.md\n",
      "            📄 readme_MVP_20250414100933.md\n",
      "            📄 readme_MVP_20250414100922.md\n",
      "            📄 readme_MVP_20250414100745.md\n",
      "            📄 readme_MVP_20250414100751.md\n",
      "            📄 readme_MVP_20250416111808.md\n",
      "        📁 workspaces/\n",
      "        📁 src/\n",
      "    📁 virtual/\n",
      "        📄 pyvenv.cfg\n",
      "        📁 bin/\n",
      "            📄 pip3.9\n",
      "            📄 Activate.ps1\n",
      "            📄 python3\n",
      "            📄 python\n",
      "            📄 pip3\n",
      "            📄 activate.fish\n",
      "            📄 python3.9\n",
      "            📄 pip\n",
      "            📄 activate\n",
      "            📄 activate.csh\n",
      "        📁 lib/\n",
      "    📁 .github/\n",
      "        📁 workflows/\n",
      "            📄 index.md\n",
      "            📄 ci.yml\n",
      "    📁 environment/\n",
      "        📄 pyvenv.cfg\n",
      "        📁 bin/\n",
      "            📄 pip3.9\n",
      "            📄 Activate.ps1\n",
      "            📄 python3\n",
      "            📄 python\n",
      "            📄 pip3\n",
      "            📄 activate.fish\n",
      "            📄 python3.9\n",
      "            📄 pip\n",
      "            📄 activate\n",
      "            📄 activate.csh\n",
      "        📁 lib/\n",
      "    📁 .git/\n",
      "        📄 ORIG_HEAD\n",
      "        📄 config\n",
      "        📄 HEAD\n",
      "        📄 description\n",
      "        📄 index\n",
      "        📄 .MERGE_MSG.swp\n",
      "        📄 packed-refs\n",
      "        📄 COMMIT_EDITMSG\n",
      "        📄 FETCH_HEAD\n",
      "        📁 objects/\n",
      "        📁 info/\n",
      "            📄 exclude\n",
      "        📁 logs/\n",
      "            📄 HEAD\n",
      "        📁 hooks/\n",
      "            📄 commit-msg.sample\n",
      "            📄 pre-rebase.sample\n",
      "            📄 pre-commit.sample\n",
      "            📄 applypatch-msg.sample\n",
      "            📄 fsmonitor-watchman.sample\n",
      "            📄 pre-receive.sample\n",
      "            📄 prepare-commit-msg.sample\n",
      "            📄 post-update.sample\n",
      "            📄 pre-merge-commit.sample\n",
      "            📄 pre-applypatch.sample\n",
      "            📄 pre-push.sample\n",
      "            📄 update.sample\n",
      "            📄 push-to-checkout.sample\n",
      "        📁 refs/\n",
      "    📁 create/\n",
      "        📄 pyvenv.cfg\n",
      "        📁 bin/\n",
      "            📄 pip3.9\n",
      "            📄 Activate.ps1\n",
      "            📄 python3\n",
      "            📄 python\n",
      "            📄 pip3\n",
      "            📄 activate.fish\n",
      "            📄 python3.9\n",
      "            📄 pip\n",
      "            📄 activate\n",
      "            📄 activate.csh\n",
      "        📁 lib/\n",
      "    📁 flows/\n",
      "        📄 full_pipeline_flow.py\n",
      "        📄 README.md\n",
      "    📁 src/\n",
      "        📄 .DS_Store\n",
      "        📄 __init__.py\n",
      "        📁 visualization/\n",
      "            📄 results.py\n",
      "            📄 __init__.py\n",
      "        📁 core/\n",
      "            📄 __init__.py\n",
      "            📄 experiment.py\n",
      "            📄 logging_manager.py\n",
      "            📄 data_manager.py\n",
      "        📁 config/\n",
      "            📄 experiment_config.py\n",
      "            📄 experiment_config.yaml\n",
      "            📄 config.py\n",
      "            📄 database_config.py\n",
      "            📄 config.json\n",
      "            📄 __init__.py\n",
      "            📄 house_style.py\n",
      "            📄 best_sarimax_params.json\n",
      "        📁 data_processing/\n",
      "            📄 transform_NED_obs_2.ipynb\n",
      "            📄 transform_meteo_obs.py\n",
      "            📄 transform_weather_preds.ipynb\n",
      "            📄 entsoe_dataprocessing.py\n",
      "            📄 transform_open_meteo_preds.ipynb\n",
      "            📄 transform_meteo_preds_history.py\n",
      "            📄 transform_ned.py\n",
      "            📄 __init__.py\n",
      "            📄 transform_NED_preds.ipynb\n",
      "            📄 transform_meteo_preds.py\n",
      "            📄 split.py\n",
      "            📄 feature_eng.py\n",
      "            📄 transform_weather_obs.ipynb\n",
      "            📄 transform_meteo_forecast_now.py\n",
      "        📁 utils/\n",
      "            📄 log_rolling_window_to_sqlite.py\n",
      "            📄 auto_arima_optimizer.py\n",
      "            📄 build_training_set_draft.py\n",
      "            📄 log_rmse_to_sqlite.py\n",
      "            📄 logger.py\n",
      "            📄 validation_utils.py\n",
      "            📄 build_training_set.py\n",
      "        📁 models/\n",
      "            📄 warp-prophet-model.py\n",
      "            📄 XGBoost_obs_NED_incl.ipynb\n",
      "            📄 benchmark_model_diagnostics.ipynb\n",
      "            📄 XGBoost_obs&preds_old.ipynb\n",
      "            📄 time_df-code_extended.ipynb\n",
      "            📄 correlation_matrix_obs.ipynb\n",
      "            📄 train-models.py\n",
      "            📄 sarimax.py\n",
      "            📄 model_results_log.csv\n",
      "            📄 __init__.py\n",
      "            📄 factory.py\n",
      "            📄 naive.py\n",
      "            📄 sarimax_model.py\n",
      "            📄 model_predictions_log.csv\n",
      "            📄 naive_model_diagnostics.ipynb\n",
      "            📄 naive_model.py\n",
      "            📄 Timeseries.ipynb\n",
      "            📄 warp-prophet-model-forecast.py\n",
      "            📄 XGBoost_evaluation.ipynb\n",
      "            📄 warp-prophet-model.ipynb\n",
      "            📄 Benchmark_model.py\n",
      "            📄 naive_benchmark_timeseries.ipynb\n",
      "        📁 __pycache__/\n",
      "            📄 __init__.cpython-311.pyc\n",
      "            📄 __init__.cpython-310.pyc\n",
      "        📁 load-data/\n",
      "            📄 load_data 2.py\n",
      "            📄 .cache.sqlite\n",
      "            📄 ned-api-get-yrs-data-0-type.ipynb\n",
      "            📄 NED-api-PREDs-7Types-thinclient.ipynb\n",
      "            📄 ned-api-get-yrs-data-0-type 2.ipynb\n",
      "            📄 load_data.py\n",
      "            📄 ned-api-get-all-types_NEW.ipynb\n",
      "            📄 ned-api-descriptive-analysis.ipynb\n",
      "            📄 ned-api-get-all-types_OLD.ipynb\n",
      "        📁 evaluation/\n",
      "            📄 validator.py\n",
      "            📄 metrics.py\n",
      "            📄 __init__.py\n",
      "        📁 data/\n",
      "            📄 optimization_logs.db\n",
      "            📄 WARP.db\n",
      "            📄 logs.db\n",
      "            📄 warp-csv-dataset.csv\n",
      "            📄 warp-csv-prediction-dataset.csv\n",
      "        📁 notebooks/\n",
      "            📄 index.md\n",
      "        📁 data_master/\n",
      "            📄 merge_obs_preds.py\n",
      "            📄 build_master_predictions.py\n",
      "            📄 build_master_observed.py\n",
      "            📄 check_master_WARP.ipynb\n",
      "            📄 master_merging\n",
      "        📁 data_ingestion/\n",
      "            📄 API_open_meteo_preds.ipynb\n",
      "            📄 .cache.sqlite\n",
      "            📄 ingest_meteo_historical_pred.py\n",
      "            📄 API_open_meteo_preds_readCSV.ipynb\n",
      "            📄 entsoe_load.py\n",
      "            📄 ingest_open-meteo_obs.py\n",
      "            📄 warp-plots.ipynb\n",
      "            📄 ingest_ned.py\n",
      "            📄 NED_preds_API_to_db.ipynb\n",
      "            📄 ingest_date.py\n",
      "            📄 NED.py\n",
      "            📄 ingest_meteo_obs.py\n",
      "            📄 ingest_entsoe.py\n",
      "            📄 API_open_meteo_historical.ipynb\n",
      "            📄 ingest_meteo_forecast_now.py\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def print_tree(startpath, max_depth=3):\n",
    "    startpath = os.path.abspath(startpath)\n",
    "    for root, dirs, files in os.walk(startpath):\n",
    "        depth = root.replace(startpath, '').count(os.sep)\n",
    "        if depth >= max_depth:\n",
    "            dirs[:] = []  # stop met verder afdalen\n",
    "            continue\n",
    "        indent = ' ' * 4 * depth\n",
    "        print(f\"{indent}📁 {os.path.basename(root)}/\")\n",
    "        for f in files:\n",
    "            print(f\"{indent}    📄 {f}\")\n",
    "\n",
    "# Pas dit pad aan indien nodig — bijvoorbeeld \".\" of \"../\"\n",
    "print_tree(\"..\", max_depth=3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "995c527d",
   "metadata": {},
   "source": [
    "# Code base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "15606154",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📁 src/data_ingestion (13 bestanden):\n",
      "  - src/data_ingestion/API_open_meteo_preds.ipynb\n",
      "  - src/data_ingestion/ingest_meteo_historical_pred.py\n",
      "  - src/data_ingestion/API_open_meteo_preds_readCSV.ipynb\n",
      "  - src/data_ingestion/entsoe_load.py\n",
      "  - src/data_ingestion/ingest_open-meteo_obs.py\n",
      "  - src/data_ingestion/warp-plots.ipynb\n",
      "  - src/data_ingestion/ingest_ned.py\n",
      "  - src/data_ingestion/NED_preds_API_to_db.ipynb\n",
      "  - src/data_ingestion/ingest_date.py\n",
      "  - src/data_ingestion/NED.py\n",
      "  - src/data_ingestion/ingest_meteo_obs.py\n",
      "  - src/data_ingestion/API_open_meteo_historical.ipynb\n",
      "  - src/data_ingestion/ingest_meteo_forecast_now.py\n",
      "\n",
      "📁 src/data_processing (14 bestanden):\n",
      "  - src/data_processing/transform_NED_obs_2.ipynb\n",
      "  - src/data_processing/transform_meteo_obs.py\n",
      "  - src/data_processing/transform_weather_preds.ipynb\n",
      "  - src/data_processing/entsoe_dataprocessing.py\n",
      "  - src/data_processing/transform_open_meteo_preds.ipynb\n",
      "  - src/data_processing/transform_meteo_preds_history.py\n",
      "  - src/data_processing/transform_ned.py\n",
      "  - src/data_processing/__init__.py\n",
      "  - src/data_processing/transform_NED_preds.ipynb\n",
      "  - src/data_processing/transform_meteo_preds.py\n",
      "  - src/data_processing/split.py\n",
      "  - src/data_processing/feature_eng.py\n",
      "  - src/data_processing/transform_weather_obs.ipynb\n",
      "  - src/data_processing/transform_meteo_forecast_now.py\n",
      "\n",
      "📁 src/data_master (4 bestanden):\n",
      "  - src/data_master/merge_obs_preds.py\n",
      "  - src/data_master/build_master_predictions.py\n",
      "  - src/data_master/build_master_observed.py\n",
      "  - src/data_master/check_master_WARP.ipynb\n",
      "\n",
      "📁 src/models (23 bestanden):\n",
      "  - src/models/warp-prophet-model.py\n",
      "  - src/models/XGBoost_obs_NED_incl.ipynb\n",
      "  - src/models/benchmark_model_diagnostics.ipynb\n",
      "  - src/models/XGBoost_obs&preds_old.ipynb\n",
      "  - src/models/time_df-code_extended.ipynb\n",
      "  - src/models/correlation_matrix_obs.ipynb\n",
      "  - src/models/train-models.py\n",
      "  - src/models/sarimax.py\n",
      "  - src/models/__init__.py\n",
      "  - src/models/factory.py\n",
      "  - src/models/naive.py\n",
      "  - src/models/sarimax_model.py\n",
      "  - src/models/naive_model_diagnostics.ipynb\n",
      "  - src/models/naive_model.py\n",
      "  - src/models/Timeseries.ipynb\n",
      "  - src/models/warp-prophet-model-forecast.py\n",
      "  - src/models/XGBoost_evaluation.ipynb\n",
      "  - src/models/warp-prophet-model.ipynb\n",
      "  - src/models/Benchmark_model.py\n",
      "  - src/models/naive_benchmark_timeseries.ipynb\n",
      "  - src/models/ned/ned-gen-prediction.ipynb\n",
      "  - src/models/ned/ned-gen-merge-data-prediction-models.ipynb\n",
      "  - src/models/model_run_results/warp-model-run-results-view.py\n",
      "\n",
      "📁 src/utils (7 bestanden):\n",
      "  - src/utils/log_rolling_window_to_sqlite.py\n",
      "  - src/utils/auto_arima_optimizer.py\n",
      "  - src/utils/build_training_set_draft.py\n",
      "  - src/utils/log_rmse_to_sqlite.py\n",
      "  - src/utils/logger.py\n",
      "  - src/utils/validation_utils.py\n",
      "  - src/utils/build_training_set.py\n",
      "\n",
      "📁 flows (1 bestanden):\n",
      "  - flows/full_pipeline_flow.py\n",
      "\n",
      "📁 tests (8 bestanden):\n",
      "  - tests/evaluate_models.ipynb\n",
      "  - tests/test_raw_ned_obs.ipynb\n",
      "  - tests/test_transform_ned_obs.ipynb\n",
      "  - tests/test_sarimax.ipynb\n",
      "  - tests/Test_trainingset.ipynb\n",
      "  - tests/test_data.py\n",
      "  - tests/test_pipeline.ipynb\n",
      "  - tests/test_models.ipynb\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Pas dit pad aan als notebook niet 1 niveau onder project root zit\n",
    "project_root = os.path.abspath(\"..\")\n",
    "relevante_mappen = [\n",
    "    \"src/data_ingestion\",\n",
    "    \"src/data_processing\",\n",
    "    \"src/data_master\",\n",
    "    \"src/models\",\n",
    "    \"src/utils\",\n",
    "    \"flows\",\n",
    "    \"tests\",\n",
    "    # optioneel:\n",
    "    # \"workspaces/redouan\",\n",
    "    # \"workspaces/sandeep\",\n",
    "    # \"workspaces/sharell\",\n",
    "    # \"workspaces/twan\"\n",
    "]\n",
    "\n",
    "bestand_inventaris = {}\n",
    "\n",
    "for mapnaam in relevante_mappen:\n",
    "    map_pad = os.path.join(project_root, mapnaam)\n",
    "    if os.path.exists(map_pad):\n",
    "        bestanden = []\n",
    "        for root, _, files in os.walk(map_pad):\n",
    "            for file in files:\n",
    "                if file.endswith((\".py\", \".ipynb\")):\n",
    "                    rel_path = os.path.relpath(os.path.join(root, file), project_root)\n",
    "                    bestanden.append(rel_path)\n",
    "        bestand_inventaris[mapnaam] = bestanden\n",
    "    else:\n",
    "        bestand_inventaris[mapnaam] = [\"❌ Map niet gevonden\"]\n",
    "\n",
    "# Print overzicht\n",
    "for mapnaam, bestanden in bestand_inventaris.items():\n",
    "    print(f\"\\n📁 {mapnaam} ({len(bestanden)} bestanden):\")\n",
    "    for bestand in bestanden:\n",
    "        print(f\"  - {bestand}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d440ac86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting prefect\n",
      "  Downloading prefect-3.4.4-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting aiosqlite<1.0.0,>=0.17.0 (from prefect)\n",
      "  Using cached aiosqlite-0.21.0-py3-none-any.whl.metadata (4.3 kB)\n",
      "Collecting alembic<2.0.0,>=1.7.5 (from prefect)\n",
      "  Downloading alembic-1.16.1-py3-none-any.whl.metadata (7.3 kB)\n",
      "Requirement already satisfied: anyio<5.0.0,>=4.4.0 in /Users/redouan/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from prefect) (4.6.2.post1)\n",
      "Collecting apprise<2.0.0,>=1.1.0 (from prefect)\n",
      "  Using cached apprise-1.9.3-py3-none-any.whl.metadata (53 kB)\n",
      "Collecting asgi-lifespan<3.0,>=1.0 (from prefect)\n",
      "  Using cached asgi_lifespan-2.1.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting asyncpg<1.0.0,>=0.23 (from prefect)\n",
      "  Downloading asyncpg-0.30.0-cp310-cp310-macosx_11_0_arm64.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: cachetools<7.0,>=5.3 in /Users/redouan/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from prefect) (5.5.0)\n",
      "Requirement already satisfied: click<8.2,>=8.0 in /Users/redouan/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from prefect) (8.1.7)\n",
      "Requirement already satisfied: cloudpickle<4.0,>=2.0 in /Users/redouan/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from prefect) (3.1.0)\n",
      "Collecting coolname<3.0.0,>=1.0.4 (from prefect)\n",
      "  Using cached coolname-2.2.0-py2.py3-none-any.whl.metadata (6.2 kB)\n",
      "Collecting cryptography>=36.0.1 (from prefect)\n",
      "  Downloading cryptography-45.0.3-cp37-abi3-macosx_10_9_universal2.whl.metadata (5.7 kB)\n",
      "Collecting dateparser<2.0.0,>=1.1.1 (from prefect)\n",
      "  Using cached dateparser-1.2.1-py3-none-any.whl.metadata (29 kB)\n",
      "Collecting docker<8.0,>=4.0 (from prefect)\n",
      "  Using cached docker-7.1.0-py3-none-any.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.0 in /Users/redouan/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from prefect) (1.2.2)\n",
      "Collecting fastapi<1.0.0,>=0.111.0 (from prefect)\n",
      "  Using cached fastapi-0.115.12-py3-none-any.whl.metadata (27 kB)\n",
      "Collecting fsspec>=2022.5.0 (from prefect)\n",
      "  Downloading fsspec-2025.5.1-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting graphviz>=0.20.1 (from prefect)\n",
      "  Using cached graphviz-0.20.3-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting griffe<2.0.0,>=0.49.0 (from prefect)\n",
      "  Using cached griffe-1.7.3-py3-none-any.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: httpcore<2.0.0,>=1.0.5 in /Users/redouan/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from prefect) (1.0.6)\n",
      "Requirement already satisfied: httpx!=0.23.2,>=0.23 in /Users/redouan/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from httpx[http2]!=0.23.2,>=0.23->prefect) (0.27.2)\n",
      "Collecting humanize<5.0.0,>=4.9.0 (from prefect)\n",
      "  Using cached humanize-4.12.3-py3-none-any.whl.metadata (7.8 kB)\n",
      "Collecting jinja2-humanize-extension>=0.4.0 (from prefect)\n",
      "  Using cached jinja2_humanize_extension-0.4.0-py3-none-any.whl.metadata (3.6 kB)\n",
      "Collecting jinja2<4.0.0,>=3.1.6 (from prefect)\n",
      "  Using cached jinja2-3.1.6-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting jsonpatch<2.0,>=1.32 (from prefect)\n",
      "  Using cached jsonpatch-1.33-py2.py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: jsonschema<5.0.0,>=4.18.0 in /Users/redouan/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from prefect) (4.23.0)\n",
      "Collecting opentelemetry-api<2.0.0,>=1.27.0 (from prefect)\n",
      "  Downloading opentelemetry_api-1.33.1-py3-none-any.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: orjson<4.0,>=3.7 in /Users/redouan/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from prefect) (3.10.11)\n",
      "Requirement already satisfied: packaging<25.1,>=21.3 in /Users/redouan/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from prefect) (24.2)\n",
      "Requirement already satisfied: pathspec>=0.8.0 in /Users/redouan/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from prefect) (0.12.1)\n",
      "Collecting pendulum<4,>=3.0.0 (from prefect)\n",
      "  Downloading pendulum-3.1.0-cp310-cp310-macosx_11_0_arm64.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: prometheus-client>=0.20.0 in /Users/redouan/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from prefect) (0.21.0)\n",
      "Requirement already satisfied: pydantic!=2.10.0,<3.0.0,>=2.9 in /Users/redouan/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from prefect) (2.9.2)\n",
      "Requirement already satisfied: pydantic-core<3.0.0,>=2.12.0 in /Users/redouan/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from prefect) (2.23.4)\n",
      "Collecting pydantic-extra-types<3.0.0,>=2.8.2 (from prefect)\n",
      "  Using cached pydantic_extra_types-2.10.4-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting pydantic-settings!=2.9.0,<3.0.0,>2.2.1 (from prefect)\n",
      "  Using cached pydantic_settings-2.9.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.8.2 in /Users/redouan/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from prefect) (2.9.0.post0)\n",
      "Collecting python-slugify<9.0,>=5.0 (from prefect)\n",
      "  Using cached python_slugify-8.0.4-py2.py3-none-any.whl.metadata (8.5 kB)\n",
      "Collecting python-socks<3.0,>=2.5.3 (from python-socks[asyncio]<3.0,>=2.5.3->prefect)\n",
      "  Using cached python_socks-2.7.1-py3-none-any.whl.metadata (8.0 kB)\n",
      "Requirement already satisfied: pytz<2026,>=2021.1 in /Users/redouan/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from prefect) (2024.2)\n",
      "Requirement already satisfied: pyyaml<7.0.0,>=5.4.1 in /Users/redouan/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from prefect) (6.0.2)\n",
      "Collecting readchar<5.0.0,>=4.0.0 (from prefect)\n",
      "  Using cached readchar-4.2.1-py3-none-any.whl.metadata (7.5 kB)\n",
      "Requirement already satisfied: rfc3339-validator<0.2.0,>=0.1.4 in /Users/redouan/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from prefect) (0.1.4)\n",
      "Collecting rich<15.0,>=11.0 (from prefect)\n",
      "  Using cached rich-14.0.0-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting ruamel-yaml>=0.17.0 (from prefect)\n",
      "  Downloading ruamel.yaml-0.18.11-py3-none-any.whl.metadata (23 kB)\n",
      "Requirement already satisfied: sniffio<2.0.0,>=1.3.0 in /Users/redouan/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from prefect) (1.3.1)\n",
      "Requirement already satisfied: sqlalchemy<3.0.0,>=2.0 in /Users/redouan/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from sqlalchemy[asyncio]<3.0.0,>=2.0->prefect) (2.0.36)\n",
      "Collecting toml>=0.10.0 (from prefect)\n",
      "  Using cached toml-0.10.2-py2.py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting typer!=0.12.2,<0.17.0,>=0.12.0 (from prefect)\n",
      "  Downloading typer-0.16.0-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: typing-extensions<5.0.0,>=4.10.0 in /Users/redouan/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from prefect) (4.12.2)\n",
      "Collecting ujson<6.0.0,>=5.8.0 (from prefect)\n",
      "  Downloading ujson-5.10.0-cp310-cp310-macosx_11_0_arm64.whl.metadata (9.3 kB)\n",
      "Collecting uv>=0.6.0 (from prefect)\n",
      "  Downloading uv-0.7.8-py3-none-macosx_11_0_arm64.whl.metadata (11 kB)\n",
      "Collecting uvicorn!=0.29.0,>=0.14.0 (from prefect)\n",
      "  Using cached uvicorn-0.34.2-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting websockets<16.0,>=13.0 (from prefect)\n",
      "  Downloading websockets-15.0.1-cp310-cp310-macosx_11_0_arm64.whl.metadata (6.8 kB)\n",
      "Collecting Mako (from alembic<2.0.0,>=1.7.5->prefect)\n",
      "  Using cached mako-1.3.10-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: tomli in /Users/redouan/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from alembic<2.0.0,>=1.7.5->prefect) (2.1.0)\n",
      "Requirement already satisfied: idna>=2.8 in /Users/redouan/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from anyio<5.0.0,>=4.4.0->prefect) (3.10)\n",
      "Requirement already satisfied: certifi in /Users/redouan/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from apprise<2.0.0,>=1.1.0->prefect) (2024.8.30)\n",
      "Requirement already satisfied: requests in /Users/redouan/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from apprise<2.0.0,>=1.1.0->prefect) (2.32.3)\n",
      "Collecting requests-oauthlib (from apprise<2.0.0,>=1.1.0->prefect)\n",
      "  Using cached requests_oauthlib-2.0.0-py2.py3-none-any.whl.metadata (11 kB)\n",
      "Collecting markdown (from apprise<2.0.0,>=1.1.0->prefect)\n",
      "  Using cached markdown-3.8-py3-none-any.whl.metadata (5.1 kB)\n",
      "Collecting async-timeout>=4.0.3 (from asyncpg<1.0.0,>=0.23->prefect)\n",
      "  Downloading async_timeout-5.0.1-py3-none-any.whl.metadata (5.1 kB)\n",
      "Requirement already satisfied: cffi>=1.14 in /Users/redouan/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from cryptography>=36.0.1->prefect) (1.17.1)\n",
      "Collecting regex!=2019.02.19,!=2021.8.27,>=2015.06.24 (from dateparser<2.0.0,>=1.1.1->prefect)\n",
      "  Using cached regex-2024.11.6-cp310-cp310-macosx_11_0_arm64.whl.metadata (40 kB)\n",
      "Collecting tzlocal>=0.2 (from dateparser<2.0.0,>=1.1.1->prefect)\n",
      "  Using cached tzlocal-5.3.1-py3-none-any.whl.metadata (7.6 kB)\n",
      "Requirement already satisfied: urllib3>=1.26.0 in /Users/redouan/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from docker<8.0,>=4.0->prefect) (2.2.3)\n",
      "Collecting starlette<0.47.0,>=0.40.0 (from fastapi<1.0.0,>=0.111.0->prefect)\n",
      "  Using cached starlette-0.46.2-py3-none-any.whl.metadata (6.2 kB)\n",
      "Collecting colorama>=0.4 (from griffe<2.0.0,>=0.49.0->prefect)\n",
      "  Using cached colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /Users/redouan/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from httpcore<2.0.0,>=1.0.5->prefect) (0.14.0)\n",
      "Collecting h2<5,>=3 (from httpx[http2]!=0.23.2,>=0.23->prefect)\n",
      "  Using cached h2-4.2.0-py3-none-any.whl.metadata (5.1 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/redouan/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from jinja2<4.0.0,>=3.1.6->prefect) (3.0.2)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /Users/redouan/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from jsonpatch<2.0,>=1.32->prefect) (3.0.0)\n",
      "Requirement already satisfied: attrs>=22.2.0 in /Users/redouan/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from jsonschema<5.0.0,>=4.18.0->prefect) (24.2.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /Users/redouan/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from jsonschema<5.0.0,>=4.18.0->prefect) (2024.10.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /Users/redouan/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from jsonschema<5.0.0,>=4.18.0->prefect) (0.35.1)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /Users/redouan/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from jsonschema<5.0.0,>=4.18.0->prefect) (0.21.0)\n",
      "Collecting deprecated>=1.2.6 (from opentelemetry-api<2.0.0,>=1.27.0->prefect)\n",
      "  Using cached Deprecated-1.2.18-py2.py3-none-any.whl.metadata (5.7 kB)\n",
      "Requirement already satisfied: importlib-metadata<8.7.0,>=6.0 in /Users/redouan/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from opentelemetry-api<2.0.0,>=1.27.0->prefect) (8.5.0)\n",
      "Requirement already satisfied: tzdata>=2020.1 in /Users/redouan/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from pendulum<4,>=3.0.0->prefect) (2024.2)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/redouan/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from pydantic!=2.10.0,<3.0.0,>=2.9->prefect) (0.7.0)\n",
      "Collecting python-dotenv>=0.21.0 (from pydantic-settings!=2.9.0,<3.0.0,>2.2.1->prefect)\n",
      "  Using cached python_dotenv-1.1.0-py3-none-any.whl.metadata (24 kB)\n",
      "Collecting typing-inspection>=0.4.0 (from pydantic-settings!=2.9.0,<3.0.0,>2.2.1->prefect)\n",
      "  Downloading typing_inspection-0.4.1-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: six>=1.5 in /Users/redouan/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from python-dateutil<3.0.0,>=2.8.2->prefect) (1.16.0)\n",
      "Collecting text-unidecode>=1.3 (from python-slugify<9.0,>=5.0->prefect)\n",
      "  Using cached text_unidecode-1.3-py2.py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich<15.0,>=11.0->prefect)\n",
      "  Using cached markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Users/redouan/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from rich<15.0,>=11.0->prefect) (2.18.0)\n",
      "Collecting ruamel.yaml.clib>=0.2.7 (from ruamel-yaml>=0.17.0->prefect)\n",
      "  Downloading ruamel.yaml.clib-0.2.12-cp310-cp310-macosx_13_0_arm64.whl.metadata (1.2 kB)\n",
      "Collecting greenlet!=0.4.17 (from sqlalchemy[asyncio]<3.0.0,>=2.0->prefect)\n",
      "  Downloading greenlet-3.2.2-cp310-cp310-macosx_11_0_universal2.whl.metadata (4.1 kB)\n",
      "Collecting shellingham>=1.3.0 (from typer!=0.12.2,<0.17.0,>=0.12.0->prefect)\n",
      "  Using cached shellingham-1.5.4-py2.py3-none-any.whl.metadata (3.5 kB)\n",
      "Requirement already satisfied: pycparser in /Users/redouan/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from cffi>=1.14->cryptography>=36.0.1->prefect) (2.22)\n",
      "Collecting wrapt<2,>=1.10 (from deprecated>=1.2.6->opentelemetry-api<2.0.0,>=1.27.0->prefect)\n",
      "  Downloading wrapt-1.17.2-cp310-cp310-macosx_11_0_arm64.whl.metadata (6.4 kB)\n",
      "Collecting hyperframe<7,>=6.1 (from h2<5,>=3->httpx[http2]!=0.23.2,>=0.23->prefect)\n",
      "  Using cached hyperframe-6.1.0-py3-none-any.whl.metadata (4.3 kB)\n",
      "Collecting hpack<5,>=4.1 (from h2<5,>=3->httpx[http2]!=0.23.2,>=0.23->prefect)\n",
      "  Using cached hpack-4.1.0-py3-none-any.whl.metadata (4.6 kB)\n",
      "Requirement already satisfied: zipp>=3.20 in /Users/redouan/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from importlib-metadata<8.7.0,>=6.0->opentelemetry-api<2.0.0,>=1.27.0->prefect) (3.21.0)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich<15.0,>=11.0->prefect)\n",
      "  Using cached mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/redouan/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from requests->apprise<2.0.0,>=1.1.0->prefect) (3.4.0)\n",
      "Collecting oauthlib>=3.0.0 (from requests-oauthlib->apprise<2.0.0,>=1.1.0->prefect)\n",
      "  Using cached oauthlib-3.2.2-py3-none-any.whl.metadata (7.5 kB)\n",
      "Downloading prefect-3.4.4-py3-none-any.whl (6.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.0/6.0 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached aiosqlite-0.21.0-py3-none-any.whl (15 kB)\n",
      "Downloading alembic-1.16.1-py3-none-any.whl (242 kB)\n",
      "Using cached apprise-1.9.3-py3-none-any.whl (1.4 MB)\n",
      "Using cached asgi_lifespan-2.1.0-py3-none-any.whl (10 kB)\n",
      "Downloading asyncpg-0.30.0-cp310-cp310-macosx_11_0_arm64.whl (645 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m645.0/645.0 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached coolname-2.2.0-py2.py3-none-any.whl (37 kB)\n",
      "Downloading cryptography-45.0.3-cp37-abi3-macosx_10_9_universal2.whl (7.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached dateparser-1.2.1-py3-none-any.whl (295 kB)\n",
      "Using cached docker-7.1.0-py3-none-any.whl (147 kB)\n",
      "Using cached fastapi-0.115.12-py3-none-any.whl (95 kB)\n",
      "Downloading fsspec-2025.5.1-py3-none-any.whl (199 kB)\n",
      "Using cached graphviz-0.20.3-py3-none-any.whl (47 kB)\n",
      "Using cached griffe-1.7.3-py3-none-any.whl (129 kB)\n",
      "Using cached humanize-4.12.3-py3-none-any.whl (128 kB)\n",
      "Using cached jinja2-3.1.6-py3-none-any.whl (134 kB)\n",
      "Using cached jinja2_humanize_extension-0.4.0-py3-none-any.whl (4.8 kB)\n",
      "Using cached jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
      "Downloading opentelemetry_api-1.33.1-py3-none-any.whl (65 kB)\n",
      "Downloading pendulum-3.1.0-cp310-cp310-macosx_11_0_arm64.whl (326 kB)\n",
      "Using cached pydantic_extra_types-2.10.4-py3-none-any.whl (37 kB)\n",
      "Using cached pydantic_settings-2.9.1-py3-none-any.whl (44 kB)\n",
      "Using cached python_slugify-8.0.4-py2.py3-none-any.whl (10 kB)\n",
      "Using cached python_socks-2.7.1-py3-none-any.whl (54 kB)\n",
      "Using cached readchar-4.2.1-py3-none-any.whl (9.3 kB)\n",
      "Using cached rich-14.0.0-py3-none-any.whl (243 kB)\n",
      "Downloading ruamel.yaml-0.18.11-py3-none-any.whl (118 kB)\n",
      "Using cached toml-0.10.2-py2.py3-none-any.whl (16 kB)\n",
      "Downloading typer-0.16.0-py3-none-any.whl (46 kB)\n",
      "Downloading ujson-5.10.0-cp310-cp310-macosx_11_0_arm64.whl (51 kB)\n",
      "Downloading uv-0.7.8-py3-none-macosx_11_0_arm64.whl (15.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.6/15.6 MB\u001b[0m \u001b[31m19.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached uvicorn-0.34.2-py3-none-any.whl (62 kB)\n",
      "Downloading websockets-15.0.1-cp310-cp310-macosx_11_0_arm64.whl (173 kB)\n",
      "Downloading async_timeout-5.0.1-py3-none-any.whl (6.2 kB)\n",
      "Using cached colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
      "Using cached Deprecated-1.2.18-py2.py3-none-any.whl (10.0 kB)\n",
      "Downloading greenlet-3.2.2-cp310-cp310-macosx_11_0_universal2.whl (267 kB)\n",
      "Using cached h2-4.2.0-py3-none-any.whl (60 kB)\n",
      "Using cached markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "Using cached python_dotenv-1.1.0-py3-none-any.whl (20 kB)\n",
      "Using cached regex-2024.11.6-cp310-cp310-macosx_11_0_arm64.whl (284 kB)\n",
      "Downloading ruamel.yaml.clib-0.2.12-cp310-cp310-macosx_13_0_arm64.whl (131 kB)\n",
      "Using cached shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
      "Using cached starlette-0.46.2-py3-none-any.whl (72 kB)\n",
      "Using cached text_unidecode-1.3-py2.py3-none-any.whl (78 kB)\n",
      "Downloading typing_inspection-0.4.1-py3-none-any.whl (14 kB)\n",
      "Using cached tzlocal-5.3.1-py3-none-any.whl (18 kB)\n",
      "Using cached mako-1.3.10-py3-none-any.whl (78 kB)\n",
      "Using cached markdown-3.8-py3-none-any.whl (106 kB)\n",
      "Using cached requests_oauthlib-2.0.0-py2.py3-none-any.whl (24 kB)\n",
      "Using cached hpack-4.1.0-py3-none-any.whl (34 kB)\n",
      "Using cached hyperframe-6.1.0-py3-none-any.whl (13 kB)\n",
      "Using cached mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Using cached oauthlib-3.2.2-py3-none-any.whl (151 kB)\n",
      "Downloading wrapt-1.17.2-cp310-cp310-macosx_11_0_arm64.whl (38 kB)\n",
      "Installing collected packages: text-unidecode, coolname, wrapt, websockets, uvicorn, uv, ujson, tzlocal, typing-inspection, toml, shellingham, ruamel.yaml.clib, regex, readchar, python-socks, python-slugify, python-dotenv, oauthlib, mdurl, markdown, Mako, jsonpatch, jinja2, hyperframe, humanize, hpack, greenlet, graphviz, fsspec, colorama, async-timeout, asgi-lifespan, aiosqlite, starlette, ruamel-yaml, requests-oauthlib, pendulum, markdown-it-py, jinja2-humanize-extension, h2, griffe, docker, deprecated, dateparser, cryptography, asyncpg, alembic, rich, pydantic-settings, pydantic-extra-types, opentelemetry-api, fastapi, apprise, typer, prefect\n",
      "  Attempting uninstall: jinja2\n",
      "    Found existing installation: Jinja2 3.1.4\n",
      "    Uninstalling Jinja2-3.1.4:\n",
      "      Successfully uninstalled Jinja2-3.1.4\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "pycaret 3.3.2 requires pandas<2.2.0, but you have pandas 2.2.3 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed Mako-1.3.10 aiosqlite-0.21.0 alembic-1.16.1 apprise-1.9.3 asgi-lifespan-2.1.0 async-timeout-5.0.1 asyncpg-0.30.0 colorama-0.4.6 coolname-2.2.0 cryptography-45.0.3 dateparser-1.2.1 deprecated-1.2.18 docker-7.1.0 fastapi-0.115.12 fsspec-2025.5.1 graphviz-0.20.3 greenlet-3.2.2 griffe-1.7.3 h2-4.2.0 hpack-4.1.0 humanize-4.12.3 hyperframe-6.1.0 jinja2-3.1.6 jinja2-humanize-extension-0.4.0 jsonpatch-1.33 markdown-3.8 markdown-it-py-3.0.0 mdurl-0.1.2 oauthlib-3.2.2 opentelemetry-api-1.33.1 pendulum-3.1.0 prefect-3.4.4 pydantic-extra-types-2.10.4 pydantic-settings-2.9.1 python-dotenv-1.1.0 python-slugify-8.0.4 python-socks-2.7.1 readchar-4.2.1 regex-2024.11.6 requests-oauthlib-2.0.0 rich-14.0.0 ruamel-yaml-0.18.11 ruamel.yaml.clib-0.2.12 shellingham-1.5.4 starlette-0.46.2 text-unidecode-1.3 toml-0.10.2 typer-0.16.0 typing-inspection-0.4.1 tzlocal-5.3.1 ujson-5.10.0 uv-0.7.8 uvicorn-0.34.2 websockets-15.0.1 wrapt-1.17.2\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install prefect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8903bfaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Testing pipeline...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">07:53:00.553 | <span style=\"color: #008080; text-decoration-color: #008080\">INFO</span>    | prefect - Starting temporary server on <span style=\"color: #0000ff; text-decoration-color: #0000ff\">http://127.0.0.1:8993</span>\n",
       "See <span style=\"color: #0000ff; text-decoration-color: #0000ff\">https://docs.prefect.io/3.0/manage/self-host#self-host-a-prefect-server</span> for more information on running a dedicated Prefect server.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "07:53:00.553 | \u001b[36mINFO\u001b[0m    | prefect - Starting temporary server on \u001b[94mhttp://127.0.0.1:8993\u001b[0m\n",
       "See \u001b[94mhttps://docs.prefect.io/3.0/manage/self-host#self-host-a-prefect-server\u001b[0m for more information on running a dedicated Prefect server.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">07:53:20.864 | <span style=\"color: #008080; text-decoration-color: #008080\">INFO</span>    | Flow run<span style=\"color: #800080; text-decoration-color: #800080\"> 'tentacled-hog'</span> - Beginning flow run<span style=\"color: #800080; text-decoration-color: #800080\"> 'tentacled-hog'</span> for flow<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> 'full-pipeline-flow'</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "07:53:20.864 | \u001b[36mINFO\u001b[0m    | Flow run\u001b[35m 'tentacled-hog'\u001b[0m - Beginning flow run\u001b[35m 'tentacled-hog'\u001b[0m for flow\u001b[1;35m 'full-pipeline-flow'\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Run weather notebooks manually:\n",
      "1. API_open_meteo_historical.ipynb\n",
      "2. API_open_meteo_preds.ipynb\n",
      "3. transform_weather_obs.ipynb\n",
      "4. transform_weather_preds.ipynb\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">07:53:20.949 | <span style=\"color: #008080; text-decoration-color: #008080\">INFO</span>    | Task run 'ingest_and_process-2a8' - Finished in state <span style=\"color: #008000; text-decoration-color: #008000\">Completed</span>()\n",
       "</pre>\n"
      ],
      "text/plain": [
       "07:53:20.949 | \u001b[36mINFO\u001b[0m    | Task run 'ingest_and_process-2a8' - Finished in state \u001b[32mCompleted\u001b[0m()\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">07:53:21.032 | <span style=\"color: #d70000; text-decoration-color: #d70000\">ERROR</span>   | build_master_predictions - ❌ Fout tijdens build: 'target_datetime'\n",
       "Traceback (most recent call last):\n",
       "  File \"/Users/redouan/.pyenv/versions/3.10.13/lib/python3.10/site-packages/pandas/core/indexes/base.py\", line 3805, in get_loc\n",
       "    return self._engine.get_loc(casted_key)\n",
       "  File \"index.pyx\", line 167, in pandas._libs.index.IndexEngine.get_loc\n",
       "  File \"index.pyx\", line 196, in pandas._libs.index.IndexEngine.get_loc\n",
       "  File \"pandas/_libs/hashtable_class_helper.pxi\", line 7081, in pandas._libs.hashtable.PyObjectHashTable.get_item\n",
       "  File \"pandas/_libs/hashtable_class_helper.pxi\", line 7089, in pandas._libs.hashtable.PyObjectHashTable.get_item\n",
       "KeyError: 'target_datetime'\n",
       "\n",
       "The above exception was the direct cause of the following exception:\n",
       "\n",
       "Traceback (most recent call last):\n",
       "  File \"/Users/redouan/ENEXIS/src/data_master/build_master_predictions.py\", line 44, in build_master\n",
       "    df_weather[\"target_datetime\"] = pd.to_datetime(df_weather[\"target_datetime\"], utc=True)\n",
       "  File \"/Users/redouan/.pyenv/versions/3.10.13/lib/python3.10/site-packages/pandas/core/frame.py\", line 4102, in __getitem__\n",
       "    indexer = self.columns.get_loc(key)\n",
       "  File \"/Users/redouan/.pyenv/versions/3.10.13/lib/python3.10/site-packages/pandas/core/indexes/base.py\", line 3812, in get_loc\n",
       "    raise KeyError(key) from err\n",
       "KeyError: 'target_datetime'\n",
       "</pre>\n"
      ],
      "text/plain": [
       "07:53:21.032 | \u001b[38;5;160mERROR\u001b[0m   | build_master_predictions - ❌ Fout tijdens build: 'target_datetime'\n",
       "Traceback (most recent call last):\n",
       "  File \"/Users/redouan/.pyenv/versions/3.10.13/lib/python3.10/site-packages/pandas/core/indexes/base.py\", line 3805, in get_loc\n",
       "    return self._engine.get_loc(casted_key)\n",
       "  File \"index.pyx\", line 167, in pandas._libs.index.IndexEngine.get_loc\n",
       "  File \"index.pyx\", line 196, in pandas._libs.index.IndexEngine.get_loc\n",
       "  File \"pandas/_libs/hashtable_class_helper.pxi\", line 7081, in pandas._libs.hashtable.PyObjectHashTable.get_item\n",
       "  File \"pandas/_libs/hashtable_class_helper.pxi\", line 7089, in pandas._libs.hashtable.PyObjectHashTable.get_item\n",
       "KeyError: 'target_datetime'\n",
       "\n",
       "The above exception was the direct cause of the following exception:\n",
       "\n",
       "Traceback (most recent call last):\n",
       "  File \"/Users/redouan/ENEXIS/src/data_master/build_master_predictions.py\", line 44, in build_master\n",
       "    df_weather[\"target_datetime\"] = pd.to_datetime(df_weather[\"target_datetime\"], utc=True)\n",
       "  File \"/Users/redouan/.pyenv/versions/3.10.13/lib/python3.10/site-packages/pandas/core/frame.py\", line 4102, in __getitem__\n",
       "    indexer = self.columns.get_loc(key)\n",
       "  File \"/Users/redouan/.pyenv/versions/3.10.13/lib/python3.10/site-packages/pandas/core/indexes/base.py\", line 3812, in get_loc\n",
       "    raise KeyError(key) from err\n",
       "KeyError: 'target_datetime'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">07:53:21.039 | <span style=\"color: #008080; text-decoration-color: #008080\">INFO</span>    | Task run 'build_masters-bbd' - Finished in state <span style=\"color: #008000; text-decoration-color: #008000\">Completed</span>()\n",
       "</pre>\n"
      ],
      "text/plain": [
       "07:53:21.039 | \u001b[36mINFO\u001b[0m    | Task run 'build_masters-bbd' - Finished in state \u001b[32mCompleted\u001b[0m()\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">07:53:21.075 | <span style=\"color: #008080; text-decoration-color: #008080\">INFO</span>    | Flow run<span style=\"color: #800080; text-decoration-color: #800080\"> 'tentacled-hog'</span> - Finished in state <span style=\"color: #008000; text-decoration-color: #008000\">Completed</span>()\n",
       "</pre>\n"
      ],
      "text/plain": [
       "07:53:21.075 | \u001b[36mINFO\u001b[0m    | Flow run\u001b[35m 'tentacled-hog'\u001b[0m - Finished in state \u001b[32mCompleted\u001b[0m()\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Pipeline completed successfully!\n",
      "Result: (None, None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "08:45:00.953 | \u001b[31mERROR\u001b[0m   | prefect.server.services.telemetry - \u001b[31mFailed\u001b[0m to send telemetry: [Errno 8] nodename nor servname provided, or not known\n",
      "Shutting down telemetry service...\n"
     ]
    }
   ],
   "source": [
    "# Simple test to see if pipeline triggers all modules\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the project root to Python path\n",
    "sys.path.append(os.path.abspath('..'))\n",
    "\n",
    "# Import the pipeline\n",
    "from flows.full_pipeline_flow import full_pipeline_flow\n",
    "\n",
    "# Test the pipeline\n",
    "print(\"🚀 Testing pipeline...\")\n",
    "try:\n",
    "    result = full_pipeline_flow()\n",
    "    print(\"✅ Pipeline completed successfully!\")\n",
    "    print(f\"Result: {result}\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Pipeline error: {str(e)}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21172b01",
   "metadata": {},
   "source": [
    "# DB-scan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "57a22014",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📦 Gevonden tabellen in WARP.db:\n",
      "\n",
      "🔹 raw_ned_obs: 40807 records\n",
      "🔹 transform_ned_obs: 29255 records\n",
      "🔹 raw_ned_df: 26304 records\n",
      "🔹 raw_meteo_preds_history: 3192 records\n",
      "🔹 raw_meteo_obs: 3187 records\n",
      "🔹 transform_meteo_forecast_now: 173 records\n",
      "🔹 transform_weather_preds_history: 3192 records\n",
      "🔹 raw_ned_obs_2: 12764 records\n",
      "🔹 transform_weather_obs: 3202 records\n",
      "🔹 transform_ned_obs_2: 3191 records\n",
      "🔹 process_weather_preds: 8568 records\n",
      "🔹 master_warp: 3355 records\n",
      "🔹 raw_NED_preds: 153602 records\n",
      "🔹 raw_meteo_forecast_now: 192 records\n",
      "🔹 processed_NED_preds: 22078 records\n",
      "🔹 dim_datetime: 3672 records\n",
      "🔹 raw_weather_obs: 3320 records\n",
      "🔹 raw_weather_preds: 3504 records\n",
      "🔹 raw_weather_preds_test: 3504 records\n",
      "🔹 raw_meteo_obs_test: 3321 records\n",
      "🔹 raw_entsoe_obs: 13342 records\n",
      "🔹 transform_entsoe_obs: 3359 records\n",
      "🔹 master_predictions: 10008 records\n",
      "🔹 training_set: 1752 records\n"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "import os\n",
    "\n",
    "# Pad naar jouw DB (pas aan als nodig)\n",
    "db_path = os.path.abspath(\"../src/data/WARP.db\")\n",
    "\n",
    "if not os.path.exists(db_path):\n",
    "    raise FileNotFoundError(f\"Database niet gevonden op: {db_path}\")\n",
    "\n",
    "# Verbinding maken\n",
    "conn = sqlite3.connect(db_path)\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Tabellen ophalen\n",
    "cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n",
    "tables = cursor.fetchall()\n",
    "\n",
    "print(f\"📦 Gevonden tabellen in {os.path.basename(db_path)}:\\n\")\n",
    "\n",
    "# Voor elke tabel: print aantal rijen\n",
    "for table in tables:\n",
    "    name = table[0]\n",
    "    try:\n",
    "        cursor.execute(f\"SELECT COUNT(*) FROM {name}\")\n",
    "        count = cursor.fetchone()[0]\n",
    "        print(f\"🔹 {name}: {count} records\")\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Fout bij {name}: {e}\")\n",
    "\n",
    "conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "90ef107a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting to database at: ..\\src\\data\\WARP.db\n",
      "\n",
      "=== Sample from raw_weather_preds ===\n",
      "Shape: (5, 81)\n",
      "Columns:\n",
      "  - date\n",
      "  - temperature_2m\n",
      "  - temperature_2m_previous_day1\n",
      "  - temperature_2m_previous_day2\n",
      "  - temperature_2m_previous_day3\n",
      "  - temperature_2m_previous_day4\n",
      "  - temperature_2m_previous_day5\n",
      "  - temperature_2m_previous_day6\n",
      "  - temperature_2m_previous_day7\n",
      "  - wind_speed_10m\n",
      "  - wind_speed_10m_previous_day1\n",
      "  - wind_speed_10m_previous_day2\n",
      "  - wind_speed_10m_previous_day3\n",
      "  - wind_speed_10m_previous_day4\n",
      "  - wind_speed_10m_previous_day5\n",
      "  - wind_speed_10m_previous_day6\n",
      "  - wind_speed_10m_previous_day7\n",
      "  - wind_direction_10m_previous_day7\n",
      "  - wind_direction_10m_previous_day6\n",
      "  - wind_direction_10m_previous_day5\n",
      "  - cloud_cover\n",
      "  - cloud_cover_previous_day1\n",
      "  - cloud_cover_previous_day2\n",
      "  - cloud_cover_previous_day3\n",
      "  - cloud_cover_previous_day4\n",
      "  - cloud_cover_previous_day5\n",
      "  - cloud_cover_previous_day6\n",
      "  - cloud_cover_previous_day7\n",
      "  - snowfall\n",
      "  - snowfall_previous_day1\n",
      "  - snowfall_previous_day2\n",
      "  - snowfall_previous_day3\n",
      "  - snowfall_previous_day4\n",
      "  - snowfall_previous_day5\n",
      "  - snowfall_previous_day6\n",
      "  - snowfall_previous_day7\n",
      "  - apparent_temperature\n",
      "  - apparent_temperature_previous_day1\n",
      "  - apparent_temperature_previous_day2\n",
      "  - apparent_temperature_previous_day3\n",
      "  - apparent_temperature_previous_day4\n",
      "  - apparent_temperature_previous_day5\n",
      "  - apparent_temperature_previous_day6\n",
      "  - apparent_temperature_previous_day7\n",
      "  - wind_direction_10m_previous_day4\n",
      "  - wind_direction_10m_previous_day3\n",
      "  - wind_direction_10m_previous_day2\n",
      "  - wind_direction_10m_previous_day1\n",
      "  - wind_direction_10m\n",
      "  - diffuse_radiation\n",
      "  - diffuse_radiation_previous_day1\n",
      "  - diffuse_radiation_previous_day2\n",
      "  - diffuse_radiation_previous_day3\n",
      "  - diffuse_radiation_previous_day4\n",
      "  - diffuse_radiation_previous_day5\n",
      "  - diffuse_radiation_previous_day6\n",
      "  - diffuse_radiation_previous_day7\n",
      "  - direct_normal_irradiance\n",
      "  - direct_normal_irradiance_previous_day1\n",
      "  - direct_normal_irradiance_previous_day2\n",
      "  - direct_normal_irradiance_previous_day3\n",
      "  - direct_normal_irradiance_previous_day4\n",
      "  - direct_normal_irradiance_previous_day5\n",
      "  - direct_normal_irradiance_previous_day6\n",
      "  - direct_normal_irradiance_previous_day7\n",
      "  - shortwave_radiation\n",
      "  - shortwave_radiation_previous_day1\n",
      "  - shortwave_radiation_previous_day2\n",
      "  - shortwave_radiation_previous_day3\n",
      "  - shortwave_radiation_previous_day4\n",
      "  - shortwave_radiation_previous_day5\n",
      "  - shortwave_radiation_previous_day6\n",
      "  - shortwave_radiation_previous_day7\n",
      "  - direct_radiation\n",
      "  - direct_radiation_previous_day1\n",
      "  - direct_radiation_previous_day2\n",
      "  - direct_radiation_previous_day3\n",
      "  - direct_radiation_previous_day4\n",
      "  - direct_radiation_previous_day5\n",
      "  - direct_radiation_previous_day6\n",
      "  - direct_radiation_previous_day7\n",
      "\n",
      "Data Sample:\n",
      "                        date  temperature_2m  temperature_2m_previous_day1  \\\n",
      "0  2024-12-25 00:00:00+00:00        7.513000                      7.863000   \n",
      "1  2024-12-25 01:00:00+00:00        7.463000                      8.662999   \n",
      "2  2024-12-25 02:00:00+00:00        7.863000                      8.363000   \n",
      "3  2024-12-25 03:00:00+00:00        8.113000                      8.763000   \n",
      "4  2024-12-25 04:00:00+00:00        8.162999                      9.313000   \n",
      "\n",
      "   temperature_2m_previous_day2  temperature_2m_previous_day3  \\\n",
      "0                          8.95                           7.9   \n",
      "1                          8.95                           8.2   \n",
      "2                          8.85                           8.3   \n",
      "3                          8.75                           8.4   \n",
      "4                          8.70                           8.6   \n",
      "\n",
      "   temperature_2m_previous_day4  temperature_2m_previous_day5  \\\n",
      "0                          5.15                          8.70   \n",
      "1                          5.05                          8.80   \n",
      "2                          5.20                          8.95   \n",
      "3                          5.45                          9.00   \n",
      "4                          5.95                          9.00   \n",
      "\n",
      "   temperature_2m_previous_day6  temperature_2m_previous_day7  wind_speed_10m  \\\n",
      "0                          2.80                          9.65            9.00   \n",
      "1                          2.40                          9.60            7.20   \n",
      "2                          2.85                          9.50            6.84   \n",
      "3                          3.30                          9.35            7.92   \n",
      "4                          3.60                          9.25            5.04   \n",
      "\n",
      "   ...  shortwave_radiation_previous_day6  shortwave_radiation_previous_day7  \\\n",
      "0  ...                                0.0                                0.0   \n",
      "1  ...                                0.0                                0.0   \n",
      "2  ...                                0.0                                0.0   \n",
      "3  ...                                0.0                                0.0   \n",
      "4  ...                                0.0                                0.0   \n",
      "\n",
      "   direct_radiation  direct_radiation_previous_day1  \\\n",
      "0               0.0                             0.0   \n",
      "1               0.0                             0.0   \n",
      "2               0.0                             0.0   \n",
      "3               0.0                             0.0   \n",
      "4               0.0                             0.0   \n",
      "\n",
      "   direct_radiation_previous_day2  direct_radiation_previous_day3  \\\n",
      "0                             0.0                             0.0   \n",
      "1                             0.0                             0.0   \n",
      "2                             0.0                             0.0   \n",
      "3                             0.0                             0.0   \n",
      "4                             0.0                             0.0   \n",
      "\n",
      "   direct_radiation_previous_day4  direct_radiation_previous_day5  \\\n",
      "0                             0.0                             0.0   \n",
      "1                             0.0                             0.0   \n",
      "2                             0.0                             0.0   \n",
      "3                             0.0                             0.0   \n",
      "4                             0.0                             0.0   \n",
      "\n",
      "   direct_radiation_previous_day6  direct_radiation_previous_day7  \n",
      "0                             0.0                             0.0  \n",
      "1                             0.0                             0.0  \n",
      "2                             0.0                             0.0  \n",
      "3                             0.0                             0.0  \n",
      "4                             0.0                             0.0  \n",
      "\n",
      "[5 rows x 81 columns]\n",
      "\n",
      "=== Sample from process_weather_preds ===\n",
      "Shape: (5, 12)\n",
      "Columns:\n",
      "  - target_datetime\n",
      "  - temperature_2m\n",
      "  - run_date\n",
      "  - wind_speed_10m\n",
      "  - wind_direction_10m\n",
      "  - cloud_cover\n",
      "  - snowfall\n",
      "  - apparent_temperature\n",
      "  - diffuse_radiation\n",
      "  - direct_normal_irradiance\n",
      "  - shortwave_radiation\n",
      "  - direct_radiation\n",
      "\n",
      "Data Sample:\n",
      "             target_datetime  temperature_2m                   run_date  \\\n",
      "0  2024-12-25 00:00:00+00:00            9.65  2024-12-18 00:00:00+00:00   \n",
      "1  2024-12-25 00:00:00+00:00            2.80  2024-12-19 00:00:00+00:00   \n",
      "2  2024-12-25 00:00:00+00:00            8.70  2024-12-20 00:00:00+00:00   \n",
      "3  2024-12-25 00:00:00+00:00            5.15  2024-12-21 00:00:00+00:00   \n",
      "4  2024-12-25 00:00:00+00:00            7.90  2024-12-22 00:00:00+00:00   \n",
      "\n",
      "   wind_speed_10m  wind_direction_10m  cloud_cover  snowfall  \\\n",
      "0        8.121970           257.19574         93.0       0.0   \n",
      "1        8.089994           212.27562        100.0       0.0   \n",
      "2       10.787993           244.29010        100.0       0.0   \n",
      "3        9.885262           190.49142        100.0       0.0   \n",
      "4        9.422101           223.45189        100.0       0.0   \n",
      "\n",
      "   apparent_temperature  diffuse_radiation  direct_normal_irradiance  \\\n",
      "0              8.258032                0.0                       0.0   \n",
      "1             -0.161992                0.0                       0.0   \n",
      "2              6.707765                0.0                       0.0   \n",
      "3              2.277478                0.0                       0.0   \n",
      "4              5.904596                0.0                       0.0   \n",
      "\n",
      "   shortwave_radiation  direct_radiation  \n",
      "0                  0.0               0.0  \n",
      "1                  0.0               0.0  \n",
      "2                  0.0               0.0  \n",
      "3                  0.0               0.0  \n",
      "4                  0.0               0.0  \n",
      "\n",
      "=== Weather Variables in Transformed Table ===\n",
      "Weather variables: ['temperature_2m', 'wind_speed_10m', 'wind_direction_10m', 'cloud_cover', 'snowfall', 'apparent_temperature', 'diffuse_radiation', 'direct_normal_irradiance', 'shortwave_radiation', 'direct_radiation']\n",
      "\n",
      "=== Sample of Row Counts per Target Date ===\n",
      "             target_datetime  count\n",
      "0  2024-12-25 00:00:00+00:00      7\n",
      "1  2024-12-25 01:00:00+00:00      7\n",
      "2  2024-12-25 02:00:00+00:00      7\n",
      "3  2024-12-25 03:00:00+00:00      7\n",
      "4  2024-12-25 04:00:00+00:00      7\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\n",
    "import pandas as pd\n",
    "import sqlite3\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "# Try to locate the database - first using the path from your original script\n",
    "# and then trying relative paths if that doesn't work\n",
    "db_paths_to_try = [\n",
    "    Path.cwd().parent.parent / \"src\" / \"data\" / \"WARP.db\",\n",
    "    Path(\"../src/data/WARP.db\"),\n",
    "    Path(\"./src/data/WARP.db\"),\n",
    "    Path(\"./WARP.db\")\n",
    "]\n",
    "\n",
    "db_path = None\n",
    "for path in db_paths_to_try:\n",
    "    if path.exists():\n",
    "        db_path = path\n",
    "        break\n",
    "\n",
    "if db_path is None:\n",
    "    # Allow user to input path if not found\n",
    "    user_path = input(\"Database not found at expected locations. Please enter the path to WARP.db: \")\n",
    "    db_path = Path(user_path)\n",
    "    if not db_path.exists():\n",
    "        raise FileNotFoundError(f\"Database not found at: {db_path}\")\n",
    "\n",
    "# Constants\n",
    "RAW_TABLE = \"raw_weather_preds\"\n",
    "TRANSFORM_TABLE = \"process_weather_preds\"\n",
    "\n",
    "# Connect to the database\n",
    "print(f\"Connecting to database at: {db_path}\")\n",
    "conn = sqlite3.connect(db_path)\n",
    "\n",
    "# Get sample of raw data\n",
    "print(f\"\\n=== Sample from {RAW_TABLE} ===\")\n",
    "raw_df = pd.read_sql_query(f\"SELECT * FROM {RAW_TABLE} LIMIT 5\", conn)\n",
    "print(f\"Shape: {raw_df.shape}\")\n",
    "print(\"Columns:\")\n",
    "for col in raw_df.columns:\n",
    "    print(f\"  - {col}\")\n",
    "print(\"\\nData Sample:\")\n",
    "print(raw_df.head())\n",
    "\n",
    "# Get sample of transformed data\n",
    "print(f\"\\n=== Sample from {TRANSFORM_TABLE} ===\")\n",
    "transform_df = pd.read_sql_query(f\"SELECT * FROM {TRANSFORM_TABLE} LIMIT 5\", conn)\n",
    "print(f\"Shape: {transform_df.shape}\")\n",
    "print(\"Columns:\")\n",
    "for col in transform_df.columns:\n",
    "    print(f\"  - {col}\")\n",
    "print(\"\\nData Sample:\")\n",
    "print(transform_df.head())\n",
    "\n",
    "# Get unique weather variables in the transformed table\n",
    "print(\"\\n=== Weather Variables in Transformed Table ===\")\n",
    "weather_vars = [col for col in transform_df.columns \n",
    "               if col not in [\"run_date\", \"target_datetime\"]]\n",
    "print(f\"Weather variables: {weather_vars}\")\n",
    "\n",
    "# Count rows per target date in the transformed table (to understand the expansion)\n",
    "print(\"\\n=== Sample of Row Counts per Target Date ===\")\n",
    "count_query = \"\"\"\n",
    "SELECT target_datetime, COUNT(*) as count \n",
    "FROM process_weather_preds \n",
    "GROUP BY target_datetime \n",
    "ORDER BY target_datetime \n",
    "LIMIT 5\n",
    "\"\"\"\n",
    "counts_df = pd.read_sql_query(count_query, conn)\n",
    "print(counts_df)\n",
    "\n",
    "# Close the connection\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d0186dd7",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'auto_arima_order' from 'models.sarimax_model' (c:\\Users\\dai\\ENEXIS\\src\\models\\sarimax_model.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 27\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mdata_processing\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msplit\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m time_based_split\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnaive_model\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m run_naive_model\n\u001b[1;32m---> 27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msarimax_model\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m run_sarimax, auto_arima_order\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlogger\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m init_logger, log_info\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;66;03m# ✅ 3. Logging starten\u001b[39;00m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'auto_arima_order' from 'models.sarimax_model' (c:\\Users\\dai\\ENEXIS\\src\\models\\sarimax_model.py)"
     ]
    }
   ],
   "source": [
    "# ----------------------------\n",
    "# ✅ 1. FIX voor imports in Jupyter\n",
    "# ----------------------------\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Stel pad naar src/ in (pas aan indien je notebook elders staat)\n",
    "src_path = os.path.abspath(\"../src\")  # Als je notebook in /tests staat\n",
    "if src_path not in sys.path:\n",
    "    sys.path.append(src_path)\n",
    "\n",
    "# ----------------------------\n",
    "# ✅ 2. Imports modules uit src/\n",
    "# ----------------------------\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "from data_processing.feature_eng import (\n",
    "    add_lag_features,\n",
    "    add_rolling_features,\n",
    "    add_time_features,\n",
    "    scale_features\n",
    ")\n",
    "from data_processing.split import time_based_split\n",
    "from models.naive_model import run_naive_model\n",
    "from models.sarimax_model import run_sarimax, auto_arima_order\n",
    "from utils.logger import init_logger, log_info\n",
    "\n",
    "# ----------------------------\n",
    "# ✅ 3. Logging starten\n",
    "# ----------------------------\n",
    "init_logger()\n",
    "log_info(\"Starting test pipeline run\", module=\"test_pipeline\")\n",
    "\n",
    "# ----------------------------\n",
    "# ✅ 4. Dummy DataFrame aanmaken\n",
    "# ----------------------------\n",
    "np.random.seed(42)\n",
    "dates = pd.date_range(start=\"2025-01-01\", periods=100, freq=\"H\")\n",
    "df = pd.DataFrame({\n",
    "    \"datetime\": dates,\n",
    "    \"price\": np.random.normal(loc=50, scale=10, size=100),\n",
    "    \"load\": np.random.normal(loc=300, scale=30, size=100)\n",
    "})\n",
    "df.set_index(\"datetime\", inplace=True)\n",
    "df.reset_index(inplace=True)\n",
    "\n",
    "# ----------------------------\n",
    "# ✅ 5. Feature Engineering\n",
    "# ----------------------------\n",
    "df = add_lag_features(df, columns=[\"price\", \"load\"], lags=[1, 24])\n",
    "df = add_rolling_features(df, columns=[\"price\", \"load\"], windows=[3, 24])\n",
    "df = add_time_features(df)\n",
    "df_scaled, _ = scale_features(df, columns=[\"price\", \"load\"])\n",
    "print(\"✅ Feature engineering done:\", df_scaled.columns.tolist())\n",
    "\n",
    "# ----------------------------\n",
    "# ✅ 6. Train/Test splitsing\n",
    "# ----------------------------\n",
    "train_df, test_df = time_based_split(df_scaled, train_ratio=0.8)\n",
    "print(f\"✅ Train size: {len(train_df)}, Test size: {len(test_df)}\")\n",
    "\n",
    "# ----------------------------\n",
    "# ✅ 7. Naive model testen\n",
    "# ----------------------------\n",
    "y_pred_naive, metrics_naive = run_naive_model(test_df, target_column=\"price\", lag=24)\n",
    "print(\"✅ Naive RMSE:\", metrics_naive[\"rmse\"])\n",
    "\n",
    "# ----------------------------\n",
    "# ✅ 8. SARIMAX model testen\n",
    "# ----------------------------\n",
    "try:\n",
    "    order, seasonal_order = auto_arima_order(train_df, target_column=\"price\", m=24)\n",
    "    y_pred_sarimax, metrics_sarimax = run_sarimax(\n",
    "        train_df,\n",
    "        test_df,\n",
    "        target_column=\"price\",\n",
    "        order=order,\n",
    "        seasonal_order=seasonal_order\n",
    "    )\n",
    "    if y_pred_sarimax is not None:\n",
    "        print(\"✅ SARIMAX RMSE:\", metrics_sarimax[\"rmse\"])\n",
    "    else:\n",
    "        print(\"ℹ️ SARIMAX: combination already tested\")\n",
    "except Exception as e:\n",
    "    print(\"❌ SARIMAX failed:\", e)\n",
    "\n",
    "print(\"🎯 Test pipeline run complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3d16b263",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log database initialized at src/data/logs.db\n"
     ]
    },
    {
     "ename": "DatabaseError",
     "evalue": "Execution failed on sql 'SELECT * FROM master_warp': no such table: master_warp",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOperationalError\u001b[0m                          Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\dai\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\io\\sql.py:2674\u001b[0m, in \u001b[0;36mSQLiteDatabase.execute\u001b[1;34m(self, sql, params)\u001b[0m\n\u001b[0;32m   2673\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 2674\u001b[0m     \u001b[43mcur\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43msql\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2675\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cur\n",
      "\u001b[1;31mOperationalError\u001b[0m: no such table: master_warp",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mDatabaseError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 247\u001b[0m\n\u001b[0;32m    245\u001b[0m \u001b[38;5;66;03m# Read the master_warp table into a pandas DataFrame\u001b[39;00m\n\u001b[0;32m    246\u001b[0m query \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSELECT * FROM master_warp\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 247\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_sql_query\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;66;03m# Convert datetime column from string to proper datetime format with UTC timezone\u001b[39;00m\n\u001b[0;32m    250\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdatetime\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mto_datetime(df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdatetime\u001b[39m\u001b[38;5;124m'\u001b[39m], utc\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\dai\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\io\\sql.py:526\u001b[0m, in \u001b[0;36mread_sql_query\u001b[1;34m(sql, con, index_col, coerce_float, params, parse_dates, chunksize, dtype, dtype_backend)\u001b[0m\n\u001b[0;32m    523\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m dtype_backend \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mno_default\n\u001b[0;32m    525\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m pandasSQL_builder(con) \u001b[38;5;28;01mas\u001b[39;00m pandas_sql:\n\u001b[1;32m--> 526\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpandas_sql\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_query\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    527\u001b[0m \u001b[43m        \u001b[49m\u001b[43msql\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    528\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindex_col\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex_col\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    529\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    530\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcoerce_float\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcoerce_float\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    531\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparse_dates\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparse_dates\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    532\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunksize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    533\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    534\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype_backend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype_backend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    535\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\dai\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\io\\sql.py:2738\u001b[0m, in \u001b[0;36mSQLiteDatabase.read_query\u001b[1;34m(self, sql, index_col, coerce_float, parse_dates, params, chunksize, dtype, dtype_backend)\u001b[0m\n\u001b[0;32m   2727\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mread_query\u001b[39m(\n\u001b[0;32m   2728\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   2729\u001b[0m     sql,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2736\u001b[0m     dtype_backend: DtypeBackend \u001b[38;5;241m|\u001b[39m Literal[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnumpy\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnumpy\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   2737\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Iterator[DataFrame]:\n\u001b[1;32m-> 2738\u001b[0m     cursor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43msql\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2739\u001b[0m     columns \u001b[38;5;241m=\u001b[39m [col_desc[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m col_desc \u001b[38;5;129;01min\u001b[39;00m cursor\u001b[38;5;241m.\u001b[39mdescription]\n\u001b[0;32m   2741\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\dai\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\io\\sql.py:2686\u001b[0m, in \u001b[0;36mSQLiteDatabase.execute\u001b[1;34m(self, sql, params)\u001b[0m\n\u001b[0;32m   2683\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ex \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01minner_exc\u001b[39;00m\n\u001b[0;32m   2685\u001b[0m ex \u001b[38;5;241m=\u001b[39m DatabaseError(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExecution failed on sql \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msql\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 2686\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m ex \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mexc\u001b[39;00m\n",
      "\u001b[1;31mDatabaseError\u001b[0m: Execution failed on sql 'SELECT * FROM master_warp': no such table: master_warp"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "import warnings\n",
    "import time\n",
    "import json\n",
    "import datetime\n",
    "import os\n",
    "from pathlib import Path\n",
    "warnings.filterwarnings('ignore')  # Suppress warnings for cleaner output\n",
    "\n",
    "# Function to initialize the log database\n",
    "def initialize_log_database(db_path='src/data/logs.db'):\n",
    "    \"\"\"Create the model_performance_log table if it doesn't exist\"\"\"\n",
    "    \n",
    "    # Ensure the directory exists\n",
    "    Path(os.path.dirname(db_path)).mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Connect to the database\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    # Create the model performance log table if it doesn't exist\n",
    "    cursor.execute('''\n",
    "    CREATE TABLE IF NOT EXISTS model_performance_log (\n",
    "        id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "        timestamp TEXT,\n",
    "        model_name TEXT,\n",
    "        model_version TEXT,\n",
    "        features_used TEXT,\n",
    "        parameters TEXT,\n",
    "        train_start_date TEXT,\n",
    "        train_end_date TEXT,\n",
    "        validation_start_date TEXT,\n",
    "        validation_end_date TEXT,\n",
    "        overall_rmse REAL,\n",
    "        min_daily_rmse REAL,\n",
    "        max_daily_rmse REAL,\n",
    "        avg_daily_rmse REAL,\n",
    "        hourly_rmse_distribution TEXT,\n",
    "        run_time_seconds REAL,\n",
    "        notes TEXT\n",
    "    )\n",
    "    ''')\n",
    "    \n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "    \n",
    "    print(f\"Log database initialized at {db_path}\")\n",
    "    return db_path\n",
    "\n",
    "# Function to log model performance\n",
    "def log_model_performance(\n",
    "    model_name,\n",
    "    model_version,\n",
    "    features_used,\n",
    "    parameters,\n",
    "    train_start_date,\n",
    "    train_end_date,\n",
    "    validation_start_date,\n",
    "    validation_end_date,\n",
    "    forecast_vs_actual,\n",
    "    daily_rmse,\n",
    "    run_time_seconds,\n",
    "    notes=\"\",\n",
    "    db_path='src/data/logs.db'\n",
    "):\n",
    "    \"\"\"\n",
    "    Log the performance of a model run to the database\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    model_name : str\n",
    "        Name of the model (e.g., 'SARIMAX', 'Naive', 'XGBoost', 'Prophet')\n",
    "    model_version : str\n",
    "        Version or configuration of the model (e.g., 'Basic', 'Enhanced', 'Full-Feature')\n",
    "    features_used : list\n",
    "        List of feature names used in the model\n",
    "    parameters : dict\n",
    "        Dictionary of model parameters\n",
    "    train_start_date, train_end_date : datetime\n",
    "        Start and end dates of the training period\n",
    "    validation_start_date, validation_end_date : datetime\n",
    "        Start and end dates of the validation period\n",
    "    forecast_vs_actual : DataFrame\n",
    "        DataFrame containing forecast and actual values with 'rmse' column\n",
    "    daily_rmse : Series\n",
    "        Series of daily RMSE values\n",
    "    run_time_seconds : float\n",
    "        Model training and prediction time in seconds\n",
    "    notes : str, optional\n",
    "        Additional notes about the model run\n",
    "    db_path : str, optional\n",
    "        Path to the database file\n",
    "    \"\"\"\n",
    "    \n",
    "    # Calculate performance metrics\n",
    "    overall_rmse = np.sqrt(mean_squared_error(\n",
    "        forecast_vs_actual['Price'], \n",
    "        forecast_vs_actual['Price_forecast']\n",
    "    ))\n",
    "    \n",
    "    min_daily_rmse = daily_rmse.min()\n",
    "    max_daily_rmse = daily_rmse.max()\n",
    "    avg_daily_rmse = daily_rmse.mean()\n",
    "    \n",
    "    # Get hourly RMSE distribution statistics\n",
    "    hourly_rmse_stats = {\n",
    "        'min': float(forecast_vs_actual['rmse'].min()),\n",
    "        'max': float(forecast_vs_actual['rmse'].max()),\n",
    "        'mean': float(forecast_vs_actual['rmse'].mean()),\n",
    "        'median': float(forecast_vs_actual['rmse'].median()),\n",
    "        'q25': float(forecast_vs_actual['rmse'].quantile(0.25)),\n",
    "        'q75': float(forecast_vs_actual['rmse'].quantile(0.75)),\n",
    "        'std': float(forecast_vs_actual['rmse'].std())\n",
    "    }\n",
    "    \n",
    "    # Connect to the database\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    # Insert the log entry\n",
    "    cursor.execute('''\n",
    "    INSERT INTO model_performance_log (\n",
    "        timestamp,\n",
    "        model_name,\n",
    "        model_version,\n",
    "        features_used,\n",
    "        parameters,\n",
    "        train_start_date,\n",
    "        train_end_date,\n",
    "        validation_start_date,\n",
    "        validation_end_date,\n",
    "        overall_rmse,\n",
    "        min_daily_rmse,\n",
    "        max_daily_rmse,\n",
    "        avg_daily_rmse,\n",
    "        hourly_rmse_distribution,\n",
    "        run_time_seconds,\n",
    "        notes\n",
    "    ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\n",
    "    ''', (\n",
    "        datetime.datetime.now().isoformat(),\n",
    "        model_name,\n",
    "        model_version,\n",
    "        json.dumps(features_used),\n",
    "        json.dumps(parameters),\n",
    "        train_start_date.isoformat(),\n",
    "        train_end_date.isoformat(),\n",
    "        validation_start_date.isoformat(),\n",
    "        validation_end_date.isoformat(),\n",
    "        overall_rmse,\n",
    "        min_daily_rmse,\n",
    "        max_daily_rmse,\n",
    "        avg_daily_rmse,\n",
    "        json.dumps(hourly_rmse_stats),\n",
    "        run_time_seconds,\n",
    "        notes\n",
    "    ))\n",
    "    \n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "    \n",
    "    print(f\"Model performance logged to {db_path}\")\n",
    "    return True\n",
    "\n",
    "# Function to get the logged model performances\n",
    "def get_model_performances(db_path='src/data/logs.db'):\n",
    "    \"\"\"Retrieve all model performance logs as a DataFrame\"\"\"\n",
    "    \n",
    "    conn = sqlite3.connect(db_path)\n",
    "    \n",
    "    # Query the database\n",
    "    query = \"SELECT * FROM model_performance_log ORDER BY timestamp DESC\"\n",
    "    logs_df = pd.read_sql_query(query, conn)\n",
    "    \n",
    "    conn.close()\n",
    "    \n",
    "    return logs_df\n",
    "\n",
    "# Function to create a comparison table of model performances\n",
    "def compare_models(db_path='src/data/logs.db', latest_only=True):\n",
    "    \"\"\"\n",
    "    Create a comparison table of model performances\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    db_path : str, optional\n",
    "        Path to the database file\n",
    "    latest_only : bool, optional\n",
    "        If True, only show the latest run of each model type\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    DataFrame\n",
    "        Comparison table of model performances\n",
    "    \"\"\"\n",
    "    \n",
    "    logs_df = get_model_performances(db_path)\n",
    "    \n",
    "    if logs_df.empty:\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # If latest_only is True, get only the latest run for each model type\n",
    "    if latest_only:\n",
    "        # Get latest run for each combination of model_name and model_version\n",
    "        logs_df['timestamp'] = pd.to_datetime(logs_df['timestamp'])\n",
    "        idx = logs_df.groupby(['model_name', 'model_version'])['timestamp'].idxmax()\n",
    "        logs_df = logs_df.loc[idx]\n",
    "    \n",
    "    # Select relevant columns for comparison\n",
    "    comparison_df = logs_df[[\n",
    "        'model_name', \n",
    "        'model_version', \n",
    "        'overall_rmse', \n",
    "        'avg_daily_rmse',\n",
    "        'min_daily_rmse',\n",
    "        'max_daily_rmse',\n",
    "        'run_time_seconds',\n",
    "        'timestamp'\n",
    "    ]].copy()\n",
    "    \n",
    "    # Sort by overall RMSE (best performing models first)\n",
    "    comparison_df = comparison_df.sort_values('overall_rmse')\n",
    "    \n",
    "    return comparison_df\n",
    "\n",
    "# Initialize the log database\n",
    "db_path = initialize_log_database()\n",
    "\n",
    "# Start timing the model run\n",
    "start_time = time.time()\n",
    "\n",
    "# Connect to the SQLite database\n",
    "conn = sqlite3.connect('src/data/WARP.db')\n",
    "\n",
    "# Read the master_warp table into a pandas DataFrame\n",
    "query = \"SELECT * FROM master_warp\"\n",
    "df = pd.read_sql_query(query, conn)\n",
    "\n",
    "# Convert datetime column from string to proper datetime format with UTC timezone\n",
    "df['datetime'] = pd.to_datetime(df['datetime'], utc=True)\n",
    "\n",
    "# Define data ranges for SARIMAX model\n",
    "train_start = pd.to_datetime('2025-01-01').tz_localize('UTC')\n",
    "observation_start = pd.to_datetime('2025-04-17').tz_localize('UTC')\n",
    "validation_start = pd.to_datetime('2025-04-24').tz_localize('UTC')\n",
    "validation_end = pd.to_datetime('2025-04-30 23:00:00').tz_localize('UTC')\n",
    "\n",
    "# Filter the data for each period\n",
    "train_data = df[(df['datetime'] >= train_start) & (df['datetime'] < observation_start)]\n",
    "observation_data = df[(df['datetime'] >= observation_start) & (df['datetime'] < validation_start)]\n",
    "validation_data = df[(df['datetime'] >= validation_start) & (df['datetime'] <= validation_end)]\n",
    "\n",
    "print(f\"Training data: {len(train_data)} hours\")\n",
    "print(f\"Observation data: {len(observation_data)} hours\")\n",
    "print(f\"Validation data: {len(validation_data)} hours\")\n",
    "\n",
    "# Sort the data by datetime\n",
    "train_data = train_data.sort_values('datetime')\n",
    "observation_data = observation_data.sort_values('datetime')\n",
    "validation_data = validation_data.sort_values('datetime')\n",
    "\n",
    "# ---- SARIMAX Model Implementation with All Features ----\n",
    "# Combine train and observation data for model fitting\n",
    "model_data = pd.concat([train_data, observation_data])\n",
    "model_data = model_data.sort_values('datetime')\n",
    "\n",
    "# Define all features we will use (excluding datetime, Price, and derived columns)\n",
    "feature_columns = [\n",
    "    # Flow features\n",
    "    'Flow_BE_to_NL', 'Flow_NL_to_BE', 'Flow_DE_to_NL', 'Flow_NL_to_DE',\n",
    "    'Flow_GB_to_NL', 'Flow_NL_to_GB', 'Flow_DK_to_NL', 'Flow_NL_to_DK',\n",
    "    'Flow_NO_to_NL', 'Flow_NL_to_NO', 'Flow_BE', 'Flow_DE', 'Flow_GB',\n",
    "    'Flow_DK', 'Flow_NO', 'Total_Flow',\n",
    "    \n",
    "    # Weather features\n",
    "    'temperature_2m', 'wind_speed_10m', 'apparent_temperature', 'cloud_cover',\n",
    "    'snowfall', 'diffuse_radiation', 'direct_normal_irradiance', 'shortwave_radiation',\n",
    "    \n",
    "    # Capacity features\n",
    "    'ned.capacity', 'ned.volume', 'ned.percentage',\n",
    "    \n",
    "    # Demand feature\n",
    "    'Load'\n",
    "]\n",
    "\n",
    "# Pre-existing time features from the dataset\n",
    "time_features = [\n",
    "    'hour_sin', 'hour_cos', 'weekday_sin', 'weekday_cos',\n",
    "    'yearday_sin', 'yearday_cos', 'is_holiday', 'is_weekend', 'is_non_working_day'\n",
    "]\n",
    "\n",
    "# Check which features are available in the data\n",
    "available_features = []\n",
    "for feature in feature_columns:\n",
    "    if feature in model_data.columns:\n",
    "        available_features.append(feature)\n",
    "\n",
    "for feature in time_features:\n",
    "    if feature in model_data.columns:\n",
    "        available_features.append(feature)\n",
    "\n",
    "print(f\"Using {len(available_features)} features in the model:\")\n",
    "print(available_features)\n",
    "\n",
    "# Set the datetime as index for the time series analysis\n",
    "model_data_ts = model_data[['datetime', 'Price'] + available_features].set_index('datetime')\n",
    "\n",
    "# Initialize the scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Scale all features except binary indicators\n",
    "non_binary_features = [f for f in available_features if f not in ['is_holiday', 'is_weekend', 'is_non_working_day']]\n",
    "model_data_ts[non_binary_features] = scaler.fit_transform(model_data_ts[non_binary_features])\n",
    "\n",
    "# Prepare exogenous variables for model\n",
    "exog = model_data_ts[available_features]\n",
    "\n",
    "# Fit SARIMAX model\n",
    "print(\"Fitting SARIMAX model with all available features...\")\n",
    "model = SARIMAX(\n",
    "    model_data_ts['Price'],\n",
    "    exog=exog,\n",
    "    order=(1, 1, 1),          # (p,d,q)\n",
    "    seasonal_order=(1, 1, 1, 24),  # (P,D,Q,s) - 24 for hourly data with daily seasonality\n",
    "    enforce_stationarity=False,\n",
    "    enforce_invertibility=False\n",
    ")\n",
    "\n",
    "# Fit the model with limited iterations to speed up the process\n",
    "results = model.fit(disp=False, maxiter=50)\n",
    "print(\"Model fitted successfully.\")\n",
    "\n",
    "# Prepare exogenous variables for forecasting the validation period\n",
    "validation_exog = validation_data[['datetime'] + available_features].copy()\n",
    "\n",
    "# Scale the validation features using the same scaler\n",
    "validation_exog[non_binary_features] = scaler.transform(validation_exog[non_binary_features])\n",
    "\n",
    "# Set the datetime as index\n",
    "validation_exog = validation_exog.set_index('datetime')[available_features]\n",
    "\n",
    "# Forecast the validation period\n",
    "print(\"Forecasting validation period...\")\n",
    "forecast = results.get_forecast(steps=len(validation_exog), exog=validation_exog)\n",
    "forecast_mean = forecast.predicted_mean\n",
    "forecast_ci = forecast.conf_int()\n",
    "\n",
    "# Create a dataframe with the forecast\n",
    "sarimax_forecast = pd.DataFrame({\n",
    "    'datetime': validation_data['datetime'],\n",
    "    'Price_forecast': forecast_mean.values\n",
    "})\n",
    "\n",
    "# Merge with actual validation data to compare\n",
    "forecast_vs_actual = sarimax_forecast.merge(\n",
    "    validation_data[['datetime', 'Price']], \n",
    "    on='datetime', \n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# ----- Calculate RMSE -----\n",
    "# Overall RMSE\n",
    "overall_rmse = np.sqrt(mean_squared_error(forecast_vs_actual['Price'], forecast_vs_actual['Price_forecast']))\n",
    "print(f\"\\nOverall RMSE: {overall_rmse:.2f}\")\n",
    "\n",
    "# RMSE per hour (for each of the 168 hours in the validation period)\n",
    "forecast_vs_actual['hour_num'] = range(1, len(forecast_vs_actual) + 1)\n",
    "forecast_vs_actual['squared_error'] = (forecast_vs_actual['Price'] - forecast_vs_actual['Price_forecast']) ** 2\n",
    "forecast_vs_actual['rmse'] = np.sqrt(forecast_vs_actual['squared_error'])\n",
    "\n",
    "# RMSE per day\n",
    "forecast_vs_actual['date'] = forecast_vs_actual['datetime'].dt.date\n",
    "daily_rmse = forecast_vs_actual.groupby('date').apply(\n",
    "    lambda x: np.sqrt(mean_squared_error(x['Price'], x['Price_forecast']))\n",
    ")\n",
    "print(\"\\nRMSE per day:\")\n",
    "print(daily_rmse)\n",
    "\n",
    "# Calculate run time\n",
    "run_time_seconds = time.time() - start_time\n",
    "\n",
    "# Log the model performance\n",
    "log_model_performance(\n",
    "    model_name=\"SARIMAX\",\n",
    "    model_version=\"Full-Feature\",\n",
    "    features_used=available_features,\n",
    "    parameters={\n",
    "        \"order\": \"(1,1,1)\",\n",
    "        \"seasonal_order\": \"(1,1,1,24)\",\n",
    "        \"enforce_stationarity\": False,\n",
    "        \"enforce_invertibility\": False,\n",
    "        \"maxiter\": 50\n",
    "    },\n",
    "    train_start_date=train_start,\n",
    "    train_end_date=observation_start - pd.Timedelta(seconds=1),\n",
    "    validation_start_date=validation_start,\n",
    "    validation_end_date=validation_end,\n",
    "    forecast_vs_actual=forecast_vs_actual,\n",
    "    daily_rmse=daily_rmse,\n",
    "    run_time_seconds=run_time_seconds,\n",
    "    notes=\"SARIMAX model with all available features\"\n",
    ")\n",
    "\n",
    "# ----- Create interactive Plotly figure without legend -----\n",
    "fig = make_subplots(\n",
    "    rows=1, cols=1,\n",
    "    specs=[[{\"secondary_y\": True}]]\n",
    ")\n",
    "\n",
    "# Add observation data (last week of training)\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=observation_data['datetime'],\n",
    "        y=observation_data['Price'],\n",
    "        mode='lines',\n",
    "        name='Observed Prices (Apr 17-23)',\n",
    "        line=dict(color='royalblue'),\n",
    "        showlegend=False\n",
    "    )\n",
    ")\n",
    "\n",
    "# Add actual validation data\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=validation_data['datetime'],\n",
    "        y=validation_data['Price'],\n",
    "        mode='lines',\n",
    "        name='Actual Prices (Apr 24-30)',\n",
    "        line=dict(color='green'),\n",
    "        showlegend=False\n",
    "    )\n",
    ")\n",
    "\n",
    "# Add SARIMAX forecast\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=sarimax_forecast['datetime'],\n",
    "        y=sarimax_forecast['Price_forecast'],\n",
    "        mode='lines',\n",
    "        name='SARIMAX Forecast (Apr 24-30)',\n",
    "        line=dict(color='red', dash='dash'),\n",
    "        showlegend=False\n",
    "    )\n",
    ")\n",
    "\n",
    "# Add a vertical line using shape\n",
    "fig.add_shape(\n",
    "    type=\"line\",\n",
    "    x0=validation_start.strftime('%Y-%m-%d %H:%M:%S+00:00'),\n",
    "    x1=validation_start.strftime('%Y-%m-%d %H:%M:%S+00:00'),\n",
    "    y0=0,\n",
    "    y1=1,\n",
    "    yref=\"paper\",\n",
    "    line=dict(color=\"black\", width=1, dash=\"dash\"),\n",
    ")\n",
    "\n",
    "# Add annotation for forecast start\n",
    "fig.add_annotation(\n",
    "    x=validation_start.strftime('%Y-%m-%d %H:%M:%S+00:00'),\n",
    "    y=1,\n",
    "    yref=\"paper\",\n",
    "    text=\"Forecast Start\",\n",
    "    showarrow=False,\n",
    "    xanchor=\"left\",\n",
    "    yanchor=\"bottom\"\n",
    ")\n",
    "\n",
    "# Update layout for white background and other styles\n",
    "fig.update_layout(\n",
    "    title='Price Time Series: Observation, Actual, and Full-Feature SARIMAX Forecast',\n",
    "    xaxis_title='Date',\n",
    "    yaxis_title='Price (€/MWh)',\n",
    "    hovermode='x unified',\n",
    "    plot_bgcolor='white',\n",
    "    paper_bgcolor='white',\n",
    "    font=dict(size=12),\n",
    "    height=600,\n",
    "    width=1000,\n",
    "    margin=dict(l=50, r=50, t=50, b=50),\n",
    "    showlegend=False\n",
    ")\n",
    "\n",
    "# Update x-axis to show all hours\n",
    "fig.update_xaxes(\n",
    "    tickformat='%b %d %H:%M',\n",
    "    tickangle=-45,\n",
    "    tickmode='auto',\n",
    "    nticks=24,\n",
    "    gridcolor='lightgrey'\n",
    ")\n",
    "\n",
    "fig.update_yaxes(gridcolor='lightgrey')\n",
    "\n",
    "# Show the interactive plot\n",
    "fig.show()\n",
    "\n",
    "# ----- Create a table with hourly RMSE -----\n",
    "# Prepare the data for the table\n",
    "hourly_rmse_table = forecast_vs_actual[['datetime', 'Price', 'Price_forecast', 'rmse']].copy()\n",
    "hourly_rmse_table['hour'] = hourly_rmse_table['datetime'].dt.hour\n",
    "hourly_rmse_table['date'] = hourly_rmse_table['datetime'].dt.date\n",
    "hourly_rmse_table['datetime_str'] = hourly_rmse_table['datetime'].dt.strftime('%Y-%m-%d %H:%M')\n",
    "\n",
    "# Create a nicer formatted table for display\n",
    "hourly_rmse_display = hourly_rmse_table[['datetime_str', 'Price', 'Price_forecast', 'rmse']].copy()\n",
    "hourly_rmse_display.columns = ['DateTime', 'Actual Price', 'Forecast Price', 'RMSE']\n",
    "\n",
    "# Print first few rows of the table\n",
    "print(\"\\nHourly RMSE (first 10 rows):\")\n",
    "print(hourly_rmse_display.head(10))\n",
    "\n",
    "# Create a full HTML table for all hours\n",
    "hourly_table = go.Figure(data=[go.Table(\n",
    "    header=dict(\n",
    "        values=list(hourly_rmse_display.columns),\n",
    "        fill_color='royalblue',\n",
    "        align='left',\n",
    "        font=dict(color='white', size=12)\n",
    "    ),\n",
    "    cells=dict(\n",
    "        values=[hourly_rmse_display[col] for col in hourly_rmse_display.columns],\n",
    "        fill_color='lavender',\n",
    "        align='left'\n",
    "    )\n",
    ")])\n",
    "\n",
    "hourly_table.update_layout(\n",
    "    title='Hourly RMSE for All 168 Hours - Full-Feature SARIMAX Model',\n",
    "    height=800,\n",
    "    width=1000\n",
    ")\n",
    "\n",
    "# Show the table\n",
    "hourly_table.show()\n",
    "\n",
    "# Get model comparison\n",
    "comparison_table = compare_models()\n",
    "print(\"\\nModel Comparison:\")\n",
    "print(comparison_table)\n",
    "\n",
    "print(\"\\nFull-Feature SARIMAX model analysis complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b130fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Connect to the SQLite database\n",
    "conn = sqlite3.connect('src/data/WARP.db')\n",
    "\n",
    "# Read the master_warp table into a pandas DataFrame\n",
    "query = \"SELECT * FROM master_warp\"\n",
    "df = pd.read_sql_query(query, conn)\n",
    "\n",
    "# Convert datetime column from string to proper datetime format with UTC timezone\n",
    "df['datetime'] = pd.to_datetime(df['datetime'], utc=True)\n",
    "\n",
    "# Define our time periods\n",
    "# Week for observations (training): Apr 17-23, 2025\n",
    "# Week for validation: Apr 24-30, 2025\n",
    "observation_start = pd.to_datetime('2025-04-17').tz_localize('UTC')\n",
    "validation_start = pd.to_datetime('2025-04-24').tz_localize('UTC')\n",
    "validation_end = pd.to_datetime('2025-04-30 23:00:00').tz_localize('UTC')\n",
    "\n",
    "# Filter the data for each period\n",
    "observation_data = df[(df['datetime'] >= observation_start) & (df['datetime'] < validation_start)]\n",
    "validation_data = df[(df['datetime'] >= validation_start) & (df['datetime'] <= validation_end)]\n",
    "\n",
    "print(f\"Observation data: {len(observation_data)} hours\")\n",
    "print(f\"Validation data: {len(validation_data)} hours\")\n",
    "\n",
    "# Sort the data by datetime\n",
    "observation_data = observation_data.sort_values('datetime')\n",
    "validation_data = validation_data.sort_values('datetime')\n",
    "\n",
    "# ---- Naive Model Implementation ----\n",
    "# Create a dataframe with the same structure as validation_data\n",
    "naive_forecast = pd.DataFrame({'datetime': validation_data['datetime']})\n",
    "naive_forecast['hour'] = naive_forecast['datetime'].dt.hour\n",
    "naive_forecast['day_of_week'] = naive_forecast['datetime'].dt.dayofweek\n",
    "\n",
    "# For each hour in the forecast, take the price from the same hour and day of week in the observation period\n",
    "forecast_prices = []\n",
    "\n",
    "for i, row in naive_forecast.iterrows():\n",
    "    # Find the matching hour and day of week from the observation data\n",
    "    matching_obs = observation_data[(observation_data['hour'] == row['hour']) & \n",
    "                                   (observation_data['day_of_week'] == row['day_of_week'])]\n",
    "    \n",
    "    # If we have a match, use that price; otherwise use the mean price\n",
    "    if not matching_obs.empty:\n",
    "        forecast_prices.append(matching_obs['Price'].values[0])\n",
    "    else:\n",
    "        forecast_prices.append(observation_data['Price'].mean())\n",
    "\n",
    "naive_forecast['Price_forecast'] = forecast_prices\n",
    "\n",
    "# Merge with actual validation data to compare\n",
    "forecast_vs_actual = naive_forecast.merge(\n",
    "    validation_data[['datetime', 'Price']], \n",
    "    on='datetime', \n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# ----- Calculate RMSE -----\n",
    "# Overall RMSE\n",
    "overall_rmse = np.sqrt(mean_squared_error(forecast_vs_actual['Price'], forecast_vs_actual['Price_forecast']))\n",
    "print(f\"\\nOverall RMSE: {overall_rmse:.2f}\")\n",
    "\n",
    "# RMSE per hour (for each of the 168 hours in the validation period)\n",
    "forecast_vs_actual['hour_num'] = range(1, len(forecast_vs_actual) + 1)\n",
    "forecast_vs_actual['squared_error'] = (forecast_vs_actual['Price'] - forecast_vs_actual['Price_forecast']) ** 2\n",
    "forecast_vs_actual['rmse'] = np.sqrt(forecast_vs_actual['squared_error'])\n",
    "\n",
    "# RMSE per day\n",
    "forecast_vs_actual['date'] = forecast_vs_actual['datetime'].dt.date\n",
    "daily_rmse = forecast_vs_actual.groupby('date').apply(\n",
    "    lambda x: np.sqrt(mean_squared_error(x['Price'], x['Price_forecast']))\n",
    ")\n",
    "print(\"\\nRMSE per day:\")\n",
    "print(daily_rmse)\n",
    "\n",
    "# ----- Create interactive Plotly figure without legend -----\n",
    "fig = make_subplots(\n",
    "    rows=1, cols=1,\n",
    "    specs=[[{\"secondary_y\": True}]]\n",
    ")\n",
    "\n",
    "# Add observation data\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=observation_data['datetime'],\n",
    "        y=observation_data['Price'],\n",
    "        mode='lines',\n",
    "        name='Observed Prices (Apr 17-23)',\n",
    "        line=dict(color='royalblue'),\n",
    "        showlegend=False  # Hide legend\n",
    "    )\n",
    ")\n",
    "\n",
    "# Add actual validation data\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=validation_data['datetime'],\n",
    "        y=validation_data['Price'],\n",
    "        mode='lines',\n",
    "        name='Actual Prices (Apr 24-30)',\n",
    "        line=dict(color='green'),\n",
    "        showlegend=False  # Hide legend\n",
    "    )\n",
    ")\n",
    "\n",
    "# Add naive forecast\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=naive_forecast['datetime'],\n",
    "        y=naive_forecast['Price_forecast'],\n",
    "        mode='lines',\n",
    "        name='Naive Forecast (Apr 24-30)',\n",
    "        line=dict(color='red', dash='dash'),\n",
    "        showlegend=False  # Hide legend\n",
    "    )\n",
    ")\n",
    "\n",
    "# Add a vertical line using shape\n",
    "fig.add_shape(\n",
    "    type=\"line\",\n",
    "    x0=validation_start.strftime('%Y-%m-%d %H:%M:%S+00:00'),\n",
    "    x1=validation_start.strftime('%Y-%m-%d %H:%M:%S+00:00'),\n",
    "    y0=0,\n",
    "    y1=1,\n",
    "    yref=\"paper\",\n",
    "    line=dict(color=\"black\", width=1, dash=\"dash\"),\n",
    ")\n",
    "\n",
    "# Add annotation for forecast start\n",
    "fig.add_annotation(\n",
    "    x=validation_start.strftime('%Y-%m-%d %H:%M:%S+00:00'),\n",
    "    y=1,\n",
    "    yref=\"paper\",\n",
    "    text=\"Forecast Start\",\n",
    "    showarrow=False,\n",
    "    xanchor=\"left\",\n",
    "    yanchor=\"bottom\"\n",
    ")\n",
    "\n",
    "# Update layout for white background and other styles\n",
    "fig.update_layout(\n",
    "    title='Price Time Series: Observation, Actual, and Naive Forecast',\n",
    "    xaxis_title='Date',\n",
    "    yaxis_title='Price (€/MWh)',\n",
    "    hovermode='x unified',\n",
    "    plot_bgcolor='white',\n",
    "    paper_bgcolor='white',\n",
    "    font=dict(size=12),\n",
    "    height=600,\n",
    "    width=1000,\n",
    "    margin=dict(l=50, r=50, t=50, b=50),\n",
    "    showlegend=False  # Ensure legend is hidden\n",
    ")\n",
    "\n",
    "# Update x-axis to show all hours\n",
    "fig.update_xaxes(\n",
    "    tickformat='%b %d %H:%M',\n",
    "    tickangle=-45,\n",
    "    tickmode='auto',\n",
    "    nticks=24,  # Show approximately 24 tick marks on the x-axis\n",
    "    gridcolor='lightgrey'\n",
    ")\n",
    "\n",
    "fig.update_yaxes(gridcolor='lightgrey')\n",
    "\n",
    "# Show the interactive plot\n",
    "fig.show()\n",
    "\n",
    "# Save the plotly figure as HTML for interactive viewing\n",
    "fig.write_html(\"naive_forecast_interactive.html\")\n",
    "\n",
    "# ----- Create a table with hourly RMSE -----\n",
    "# Prepare the data for the table\n",
    "hourly_rmse_table = forecast_vs_actual[['datetime', 'Price', 'Price_forecast', 'rmse']].copy()\n",
    "hourly_rmse_table['hour'] = hourly_rmse_table['datetime'].dt.hour\n",
    "hourly_rmse_table['date'] = hourly_rmse_table['datetime'].dt.date\n",
    "hourly_rmse_table['datetime_str'] = hourly_rmse_table['datetime'].dt.strftime('%Y-%m-%d %H:%M')\n",
    "\n",
    "# Create a nicer formatted table for display\n",
    "hourly_rmse_display = hourly_rmse_table[['datetime_str', 'Price', 'Price_forecast', 'rmse']].copy()\n",
    "hourly_rmse_display.columns = ['DateTime', 'Actual Price', 'Forecast Price', 'RMSE']\n",
    "\n",
    "# Print first few rows of the table\n",
    "print(\"\\nHourly RMSE (first 10 rows):\")\n",
    "print(hourly_rmse_display.head(10))\n",
    "\n",
    "# Create a full HTML table for all hours\n",
    "hourly_table = go.Figure(data=[go.Table(\n",
    "    header=dict(\n",
    "        values=list(hourly_rmse_display.columns),\n",
    "        fill_color='royalblue',\n",
    "        align='left',\n",
    "        font=dict(color='white', size=12)\n",
    "    ),\n",
    "    cells=dict(\n",
    "        values=[hourly_rmse_display[col] for col in hourly_rmse_display.columns],\n",
    "        fill_color='lavender',\n",
    "        align='left'\n",
    "    )\n",
    ")])\n",
    "\n",
    "hourly_table.update_layout(\n",
    "    title='Hourly RMSE for All 168 Hours',\n",
    "    height=800,  # Taller to show more rows\n",
    "    width=1000\n",
    ")\n",
    "\n",
    "# Show the table\n",
    "hourly_table.show()\n",
    "\n",
    "# Save the hourly RMSE table to HTML\n",
    "hourly_table.write_html(\"hourly_rmse_table.html\")\n",
    "\n",
    "# Save the forecast and validation results to CSV\n",
    "forecast_vs_actual[['datetime', 'Price', 'Price_forecast', 'rmse']].to_csv('naive_forecast_validation.csv', index=False)\n",
    "print(\"\\nValidation results saved to 'naive_forecast_validation.csv'\")\n",
    "print(\"Interactive plot saved to 'naive_forecast_interactive.html'\")\n",
    "print(\"Hourly RMSE table saved to 'hourly_rmse_table.html'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea6cd729",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "import warnings\n",
    "import time\n",
    "import json\n",
    "import datetime\n",
    "import os\n",
    "from pathlib import Path\n",
    "warnings.filterwarnings('ignore')  # Suppress warnings for cleaner output\n",
    "\n",
    "# Function to initialize the log database\n",
    "def initialize_log_database(db_path='src/data/logs.db'):\n",
    "    \"\"\"Create the model_performance_log table if it doesn't exist\"\"\"\n",
    "    \n",
    "    # Ensure the directory exists\n",
    "    Path(os.path.dirname(db_path)).mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Connect to the database\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    # Create the model performance log table if it doesn't exist\n",
    "    cursor.execute('''\n",
    "    CREATE TABLE IF NOT EXISTS model_performance_log (\n",
    "        id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "        timestamp TEXT,\n",
    "        model_name TEXT,\n",
    "        model_version TEXT,\n",
    "        features_used TEXT,\n",
    "        parameters TEXT,\n",
    "        train_start_date TEXT,\n",
    "        train_end_date TEXT,\n",
    "        validation_start_date TEXT,\n",
    "        validation_end_date TEXT,\n",
    "        overall_rmse REAL,\n",
    "        min_daily_rmse REAL,\n",
    "        max_daily_rmse REAL,\n",
    "        avg_daily_rmse REAL,\n",
    "        hourly_rmse_distribution TEXT,\n",
    "        run_time_seconds REAL,\n",
    "        notes TEXT\n",
    "    )\n",
    "    ''')\n",
    "    \n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "    \n",
    "    print(f\"Log database initialized at {db_path}\")\n",
    "    return db_path\n",
    "\n",
    "# Function to log model performance\n",
    "def log_model_performance(\n",
    "    model_name,\n",
    "    model_version,\n",
    "    features_used,\n",
    "    parameters,\n",
    "    train_start_date,\n",
    "    train_end_date,\n",
    "    validation_start_date,\n",
    "    validation_end_date,\n",
    "    forecast_vs_actual,\n",
    "    daily_rmse,\n",
    "    run_time_seconds,\n",
    "    notes=\"\",\n",
    "    db_path='src/data/logs.db'\n",
    "):\n",
    "    \"\"\"Log the performance of a model run to the database\"\"\"\n",
    "    \n",
    "    # Calculate performance metrics\n",
    "    overall_rmse = np.sqrt(mean_squared_error(\n",
    "        forecast_vs_actual['Price'], \n",
    "        forecast_vs_actual['Price_forecast']\n",
    "    ))\n",
    "    \n",
    "    min_daily_rmse = daily_rmse.min()\n",
    "    max_daily_rmse = daily_rmse.max()\n",
    "    avg_daily_rmse = daily_rmse.mean()\n",
    "    \n",
    "    # Get hourly RMSE distribution statistics\n",
    "    hourly_rmse_stats = {\n",
    "        'min': float(forecast_vs_actual['rmse'].min()),\n",
    "        'max': float(forecast_vs_actual['rmse'].max()),\n",
    "        'mean': float(forecast_vs_actual['rmse'].mean()),\n",
    "        'median': float(forecast_vs_actual['rmse'].median()),\n",
    "        'q25': float(forecast_vs_actual['rmse'].quantile(0.25)),\n",
    "        'q75': float(forecast_vs_actual['rmse'].quantile(0.75)),\n",
    "        'std': float(forecast_vs_actual['rmse'].std())\n",
    "    }\n",
    "    \n",
    "    # Connect to the database\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    # Insert the log entry\n",
    "    cursor.execute('''\n",
    "    INSERT INTO model_performance_log (\n",
    "        timestamp,\n",
    "        model_name,\n",
    "        model_version,\n",
    "        features_used,\n",
    "        parameters,\n",
    "        train_start_date,\n",
    "        train_end_date,\n",
    "        validation_start_date,\n",
    "        validation_end_date,\n",
    "        overall_rmse,\n",
    "        min_daily_rmse,\n",
    "        max_daily_rmse,\n",
    "        avg_daily_rmse,\n",
    "        hourly_rmse_distribution,\n",
    "        run_time_seconds,\n",
    "        notes\n",
    "    ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\n",
    "    ''', (\n",
    "        datetime.datetime.now().isoformat(),\n",
    "        model_name,\n",
    "        model_version,\n",
    "        json.dumps(features_used),\n",
    "        json.dumps(parameters),\n",
    "        train_start_date.isoformat(),\n",
    "        train_end_date.isoformat(),\n",
    "        validation_start_date.isoformat(),\n",
    "        validation_end_date.isoformat(),\n",
    "        overall_rmse,\n",
    "        min_daily_rmse,\n",
    "        max_daily_rmse,\n",
    "        avg_daily_rmse,\n",
    "        json.dumps(hourly_rmse_stats),\n",
    "        run_time_seconds,\n",
    "        notes\n",
    "    ))\n",
    "    \n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "    \n",
    "    print(f\"Model performance logged to {db_path}\")\n",
    "    return True\n",
    "\n",
    "# Function to get the logged model performances\n",
    "def get_model_performances(db_path='src/data/logs.db'):\n",
    "    \"\"\"Retrieve all model performance logs as a DataFrame\"\"\"\n",
    "    \n",
    "    conn = sqlite3.connect(db_path)\n",
    "    \n",
    "    # Query the database\n",
    "    query = \"SELECT * FROM model_performance_log ORDER BY timestamp DESC\"\n",
    "    logs_df = pd.read_sql_query(query, conn)\n",
    "    \n",
    "    conn.close()\n",
    "    \n",
    "    return logs_df\n",
    "\n",
    "# Function to create a comparison table of model performances\n",
    "def compare_models(db_path='src/data/logs.db', latest_only=True):\n",
    "    \"\"\"Create a comparison table of model performances\"\"\"\n",
    "    \n",
    "    logs_df = get_model_performances(db_path)\n",
    "    \n",
    "    if logs_df.empty:\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # If latest_only is True, get only the latest run for each model type\n",
    "    if latest_only:\n",
    "        # Get latest run for each combination of model_name and model_version\n",
    "        logs_df['timestamp'] = pd.to_datetime(logs_df['timestamp'])\n",
    "        idx = logs_df.groupby(['model_name', 'model_version'])['timestamp'].idxmax()\n",
    "        logs_df = logs_df.loc[idx]\n",
    "    \n",
    "    # Select relevant columns for comparison\n",
    "    comparison_df = logs_df[[\n",
    "        'model_name', \n",
    "        'model_version', \n",
    "        'overall_rmse', \n",
    "        'avg_daily_rmse',\n",
    "        'min_daily_rmse',\n",
    "        'max_daily_rmse',\n",
    "        'run_time_seconds',\n",
    "        'timestamp'\n",
    "    ]].copy()\n",
    "    \n",
    "    # Sort by overall RMSE (best performing models first)\n",
    "    comparison_df = comparison_df.sort_values('overall_rmse')\n",
    "    \n",
    "    return comparison_df\n",
    "\n",
    "# Initialize the log database\n",
    "db_path = initialize_log_database()\n",
    "\n",
    "# Start timing the model run\n",
    "start_time = time.time()\n",
    "\n",
    "# Connect to the SQLite database\n",
    "conn = sqlite3.connect('src/data/WARP.db')\n",
    "\n",
    "# Read the master_warp table into a pandas DataFrame\n",
    "query = \"SELECT * FROM master_warp\"\n",
    "df = pd.read_sql_query(query, conn)\n",
    "\n",
    "# Print column names to check availability\n",
    "print(\"Available columns in the dataset:\")\n",
    "print(df.columns.tolist())\n",
    "\n",
    "# Convert datetime column from string to proper datetime format with UTC timezone\n",
    "df['datetime'] = pd.to_datetime(df['datetime'], utc=True)\n",
    "\n",
    "# Define data ranges for SARIMAX model\n",
    "train_start = pd.to_datetime('2025-01-01').tz_localize('UTC')\n",
    "observation_start = pd.to_datetime('2025-04-17').tz_localize('UTC')\n",
    "validation_start = pd.to_datetime('2025-04-24').tz_localize('UTC')\n",
    "validation_end = pd.to_datetime('2025-04-30 23:00:00').tz_localize('UTC')\n",
    "\n",
    "# Filter the data for each period\n",
    "train_data = df[(df['datetime'] >= train_start) & (df['datetime'] < observation_start)]\n",
    "observation_data = df[(df['datetime'] >= observation_start) & (df['datetime'] < validation_start)]\n",
    "validation_data = df[(df['datetime'] >= validation_start) & (df['datetime'] <= validation_end)]\n",
    "\n",
    "print(f\"Training data: {len(train_data)} hours\")\n",
    "print(f\"Observation data: {len(observation_data)} hours\")\n",
    "print(f\"Validation data: {len(validation_data)} hours\")\n",
    "\n",
    "# Sort the data by datetime\n",
    "train_data = train_data.sort_values('datetime')\n",
    "observation_data = observation_data.sort_values('datetime')\n",
    "validation_data = validation_data.sort_values('datetime')\n",
    "\n",
    "# ---- SARIMAX Model Implementation with Available Features ----\n",
    "# Combine train and observation data for model fitting\n",
    "model_data = pd.concat([train_data, observation_data])\n",
    "model_data = model_data.sort_values('datetime')\n",
    "\n",
    "# Define desired features\n",
    "desired_features = [\n",
    "    # Weather and related\n",
    "    'temperature_2m', 'Total_Flow', 'ned.volume',\n",
    "    \n",
    "    # Check if we have these features\n",
    "    'wind_speed_10m', 'apparent_temperature', 'cloud_cover',\n",
    "    'snowfall', 'diffuse_radiation', 'direct_normal_irradiance', \n",
    "    'shortwave_radiation', 'ned.capacity', 'ned.percentage', 'Load'\n",
    "]\n",
    "\n",
    "# Check which features are available\n",
    "available_features = ['datetime', 'Price']\n",
    "for feature in desired_features:\n",
    "    if feature in model_data.columns:\n",
    "        available_features.append(feature)\n",
    "\n",
    "print(f\"Using {len(available_features) - 2} features in the model:\")  # -2 for datetime and Price\n",
    "print(available_features[2:])  # Skip datetime and Price\n",
    "\n",
    "# Set the datetime as index for the time series analysis\n",
    "model_data_ts = model_data[available_features].set_index('datetime')\n",
    "\n",
    "# Add basic time features\n",
    "model_data_ts['hour'] = model_data_ts.index.hour\n",
    "model_data_ts['day_of_week'] = model_data_ts.index.dayofweek\n",
    "model_data_ts['is_weekend'] = (model_data_ts.index.dayofweek >= 5).astype(int)\n",
    "\n",
    "# Add these time features to our available features list\n",
    "time_features = ['hour', 'day_of_week', 'is_weekend']\n",
    "all_exog_features = available_features[2:] + time_features  # Skip datetime and Price\n",
    "\n",
    "# Define features to scale (all except binary indicators)\n",
    "scale_features = [f for f in all_exog_features if f not in ['is_weekend']]\n",
    "\n",
    "# Initialize the scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Scale the features\n",
    "model_data_ts[scale_features] = scaler.fit_transform(model_data_ts[scale_features])\n",
    "\n",
    "# For this enhanced model, we'll use time features + scaled available variables\n",
    "exog = model_data_ts[all_exog_features]\n",
    "\n",
    "# Fit SARIMAX model\n",
    "print(\"Fitting SARIMAX model with available features...\")\n",
    "model = SARIMAX(\n",
    "    model_data_ts['Price'],\n",
    "    exog=exog,\n",
    "    order=(1, 1, 1),          # (p,d,q) - simple parameters\n",
    "    seasonal_order=(1, 1, 1, 24),  # (P,D,Q,s) - 24 for hourly data with daily seasonality\n",
    "    enforce_stationarity=False,\n",
    "    enforce_invertibility=False\n",
    ")\n",
    "\n",
    "# Fit the model\n",
    "results = model.fit(disp=False)\n",
    "print(\"Model fitted successfully.\")\n",
    "\n",
    "# Prepare exogenous variables for forecasting the validation period\n",
    "validation_exog = validation_data[['datetime'] + available_features[2:]].copy()\n",
    "validation_exog['hour'] = validation_exog['datetime'].dt.hour\n",
    "validation_exog['day_of_week'] = validation_exog['datetime'].dt.dayofweek\n",
    "validation_exog['is_weekend'] = (validation_exog['datetime'].dt.dayofweek >= 5).astype(int)\n",
    "\n",
    "# Scale the validation exogenous variables using the same scaler\n",
    "validation_exog[scale_features] = scaler.transform(validation_exog[scale_features])\n",
    "\n",
    "# Set the datetime as index\n",
    "validation_exog = validation_exog.set_index('datetime')[all_exog_features]\n",
    "\n",
    "# Forecast the validation period\n",
    "print(\"Forecasting validation period...\")\n",
    "forecast = results.get_forecast(steps=len(validation_exog), exog=validation_exog)\n",
    "forecast_mean = forecast.predicted_mean\n",
    "forecast_ci = forecast.conf_int()\n",
    "\n",
    "# Create a dataframe with the forecast\n",
    "sarimax_forecast = pd.DataFrame({\n",
    "    'datetime': validation_data['datetime'],\n",
    "    'Price_forecast': forecast_mean.values\n",
    "})\n",
    "\n",
    "# Merge with actual validation data to compare\n",
    "forecast_vs_actual = sarimax_forecast.merge(\n",
    "    validation_data[['datetime', 'Price']], \n",
    "    on='datetime', \n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# ----- Calculate RMSE -----\n",
    "# Overall RMSE\n",
    "overall_rmse = np.sqrt(mean_squared_error(forecast_vs_actual['Price'], forecast_vs_actual['Price_forecast']))\n",
    "print(f\"\\nOverall RMSE: {overall_rmse:.2f}\")\n",
    "\n",
    "# RMSE per hour (for each of the 168 hours in the validation period)\n",
    "forecast_vs_actual['hour_num'] = range(1, len(forecast_vs_actual) + 1)\n",
    "forecast_vs_actual['squared_error'] = (forecast_vs_actual['Price'] - forecast_vs_actual['Price_forecast']) ** 2\n",
    "forecast_vs_actual['rmse'] = np.sqrt(forecast_vs_actual['squared_error'])\n",
    "\n",
    "# RMSE per day\n",
    "forecast_vs_actual['date'] = forecast_vs_actual['datetime'].dt.date\n",
    "daily_rmse = forecast_vs_actual.groupby('date').apply(\n",
    "    lambda x: np.sqrt(mean_squared_error(x['Price'], x['Price_forecast']))\n",
    ")\n",
    "print(\"\\nRMSE per day:\")\n",
    "print(daily_rmse)\n",
    "\n",
    "# Calculate run time\n",
    "run_time_seconds = time.time() - start_time\n",
    "\n",
    "# Log the model performance\n",
    "log_model_performance(\n",
    "    model_name=\"SARIMAX\",\n",
    "    model_version=\"Available-Features\",\n",
    "    features_used=all_exog_features,\n",
    "    parameters={\n",
    "        \"order\": \"(1,1,1)\",\n",
    "        \"seasonal_order\": \"(1,1,1,24)\",\n",
    "        \"enforce_stationarity\": False,\n",
    "        \"enforce_invertibility\": False\n",
    "    },\n",
    "    train_start_date=train_start,\n",
    "    train_end_date=observation_start - pd.Timedelta(seconds=1),\n",
    "    validation_start_date=validation_start,\n",
    "    validation_end_date=validation_end,\n",
    "    forecast_vs_actual=forecast_vs_actual,\n",
    "    daily_rmse=daily_rmse,\n",
    "    run_time_seconds=run_time_seconds,\n",
    "    notes=f\"SARIMAX model with available features: {', '.join(all_exog_features)}\"\n",
    ")\n",
    "\n",
    "# ----- Create interactive Plotly figure without legend -----\n",
    "fig = make_subplots(\n",
    "    rows=1, cols=1,\n",
    "    specs=[[{\"secondary_y\": True}]]\n",
    ")\n",
    "\n",
    "# Add observation data (last week of training)\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=observation_data['datetime'],\n",
    "        y=observation_data['Price'],\n",
    "        mode='lines',\n",
    "        name='Observed Prices (Apr 17-23)',\n",
    "        line=dict(color='royalblue'),\n",
    "        showlegend=False\n",
    "    )\n",
    ")\n",
    "\n",
    "# Add actual validation data\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=validation_data['datetime'],\n",
    "        y=validation_data['Price'],\n",
    "        mode='lines',\n",
    "        name='Actual Prices (Apr 24-30)',\n",
    "        line=dict(color='green'),\n",
    "        showlegend=False\n",
    "    )\n",
    ")\n",
    "\n",
    "# Add SARIMAX forecast\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=sarimax_forecast['datetime'],\n",
    "        y=sarimax_forecast['Price_forecast'],\n",
    "        mode='lines',\n",
    "        name='SARIMAX Forecast (Apr 24-30)',\n",
    "        line=dict(color='red', dash='dash'),\n",
    "        showlegend=False\n",
    "    )\n",
    ")\n",
    "\n",
    "# Add a vertical line using shape\n",
    "fig.add_shape(\n",
    "    type=\"line\",\n",
    "    x0=validation_start.strftime('%Y-%m-%d %H:%M:%S+00:00'),\n",
    "    x1=validation_start.strftime('%Y-%m-%d %H:%M:%S+00:00'),\n",
    "    y0=0,\n",
    "    y1=1,\n",
    "    yref=\"paper\",\n",
    "    line=dict(color=\"black\", width=1, dash=\"dash\"),\n",
    ")\n",
    "\n",
    "# Add annotation for forecast start\n",
    "fig.add_annotation(\n",
    "    x=validation_start.strftime('%Y-%m-%d %H:%M:%S+00:00'),\n",
    "    y=1,\n",
    "    yref=\"paper\",\n",
    "    text=\"Forecast Start\",\n",
    "    showarrow=False,\n",
    "    xanchor=\"left\",\n",
    "    yanchor=\"bottom\"\n",
    ")\n",
    "\n",
    "# Update layout for white background and other styles\n",
    "fig.update_layout(\n",
    "    title='Price Time Series: Observation, Actual, and SARIMAX Forecast',\n",
    "    xaxis_title='Date',\n",
    "    yaxis_title='Price (€/MWh)',\n",
    "    hovermode='x unified',\n",
    "    plot_bgcolor='white',\n",
    "    paper_bgcolor='white',\n",
    "    font=dict(size=12),\n",
    "    height=600,\n",
    "    width=1000,\n",
    "    margin=dict(l=50, r=50, t=50, b=50),\n",
    "    showlegend=False\n",
    ")\n",
    "\n",
    "# Update x-axis to show all hours\n",
    "fig.update_xaxes(\n",
    "    tickformat='%b %d %H:%M',\n",
    "    tickangle=-45,\n",
    "    tickmode='auto',\n",
    "    nticks=24,\n",
    "    gridcolor='lightgrey'\n",
    ")\n",
    "\n",
    "fig.update_yaxes(gridcolor='lightgrey')\n",
    "\n",
    "# Show the interactive plot\n",
    "fig.show()\n",
    "\n",
    "# ----- Create a table with hourly RMSE -----\n",
    "# Prepare the data for the table\n",
    "hourly_rmse_table = forecast_vs_actual[['datetime', 'Price', 'Price_forecast', 'rmse']].copy()\n",
    "hourly_rmse_table['hour'] = hourly_rmse_table['datetime'].dt.hour\n",
    "hourly_rmse_table['date'] = hourly_rmse_table['datetime'].dt.date\n",
    "hourly_rmse_table['datetime_str'] = hourly_rmse_table['datetime'].dt.strftime('%Y-%m-%d %H:%M')\n",
    "\n",
    "# Create a nicer formatted table for display\n",
    "hourly_rmse_display = hourly_rmse_table[['datetime_str', 'Price', 'Price_forecast', 'rmse']].copy()\n",
    "hourly_rmse_display.columns = ['DateTime', 'Actual Price', 'Forecast Price', 'RMSE']\n",
    "\n",
    "# Print first few rows of the table\n",
    "print(\"\\nHourly RMSE (first 10 rows):\")\n",
    "print(hourly_rmse_display.head(10))\n",
    "\n",
    "# Create a full HTML table for all hours\n",
    "hourly_table = go.Figure(data=[go.Table(\n",
    "    header=dict(\n",
    "        values=list(hourly_rmse_display.columns),\n",
    "        fill_color='royalblue',\n",
    "        align='left',\n",
    "        font=dict(color='white', size=12)\n",
    "    ),\n",
    "    cells=dict(\n",
    "        values=[hourly_rmse_display[col] for col in hourly_rmse_display.columns],\n",
    "        fill_color='lavender',\n",
    "        align='left'\n",
    "    )\n",
    ")])\n",
    "\n",
    "hourly_table.update_layout(\n",
    "    title='Hourly RMSE for All 168 Hours - SARIMAX Model',\n",
    "    height=800,\n",
    "    width=1000\n",
    ")\n",
    "\n",
    "# Show the table\n",
    "hourly_table.show()\n",
    "\n",
    "# Try to get feature importance information\n",
    "try:\n",
    "    # Get the feature importance from the SARIMAX model coefficients\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'Feature': all_exog_features,\n",
    "        'Coefficient': results.params[-len(all_exog_features):]\n",
    "    })\n",
    "    \n",
    "    # Sort by absolute value of coefficients\n",
    "    feature_importance['Abs_Coefficient'] = feature_importance['Coefficient'].abs()\n",
    "    feature_importance = feature_importance.sort_values('Abs_Coefficient', ascending=False)\n",
    "    \n",
    "    print(\"\\nFeature Importance (top features):\")\n",
    "    print(feature_importance.head(10))\n",
    "except Exception as e:\n",
    "    print(f\"\\nCould not display feature importance: {e}\")\n",
    "\n",
    "# Get model comparison\n",
    "comparison_table = compare_models()\n",
    "print(\"\\nModel Comparison:\")\n",
    "print(comparison_table)\n",
    "\n",
    "print(\"\\nSARIMAX model analysis complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0f37979e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔍 Laatste 35 rijen van transform_entsoe_obs in ../src/data/WARP.db:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Timestamp</th>\n",
       "      <th>Load</th>\n",
       "      <th>Price</th>\n",
       "      <th>Forecast_Load</th>\n",
       "      <th>Flow_GB</th>\n",
       "      <th>Flow_NO</th>\n",
       "      <th>Total_Flow</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>2025-05-29 12:00:00+00:00</td>\n",
       "      <td>7270.250000</td>\n",
       "      <td>-0.00200</td>\n",
       "      <td>6918.50</td>\n",
       "      <td>116.75</td>\n",
       "      <td>0.0</td>\n",
       "      <td>116.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>2025-05-29 13:00:00+00:00</td>\n",
       "      <td>7063.000000</td>\n",
       "      <td>-0.00200</td>\n",
       "      <td>6680.75</td>\n",
       "      <td>300.25</td>\n",
       "      <td>0.0</td>\n",
       "      <td>300.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>2025-05-29 14:00:00+00:00</td>\n",
       "      <td>6716.000000</td>\n",
       "      <td>-0.00276</td>\n",
       "      <td>6699.75</td>\n",
       "      <td>740.75</td>\n",
       "      <td>0.0</td>\n",
       "      <td>740.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>2025-05-29 15:00:00+00:00</td>\n",
       "      <td>6588.666667</td>\n",
       "      <td>-0.00159</td>\n",
       "      <td>6948.50</td>\n",
       "      <td>730.75</td>\n",
       "      <td>0.0</td>\n",
       "      <td>730.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>2025-05-29 16:00:00+00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.02000</td>\n",
       "      <td>7925.75</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>2025-05-29 17:00:00+00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.05962</td>\n",
       "      <td>9077.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>2025-05-29 18:00:00+00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.07351</td>\n",
       "      <td>10194.75</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>2025-05-29 19:00:00+00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.07520</td>\n",
       "      <td>11271.75</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>2025-05-29 20:00:00+00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.07340</td>\n",
       "      <td>11671.25</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2025-05-29 21:00:00+00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.07630</td>\n",
       "      <td>11542.25</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2025-05-29 22:00:00+00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.05700</td>\n",
       "      <td>11224.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2025-05-29 23:00:00+00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.04490</td>\n",
       "      <td>10831.75</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2025-05-30 00:00:00+00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.02746</td>\n",
       "      <td>10529.50</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2025-05-30 01:00:00+00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.03348</td>\n",
       "      <td>10226.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2025-05-30 02:00:00+00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.03395</td>\n",
       "      <td>10095.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2025-05-30 03:00:00+00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.03739</td>\n",
       "      <td>10026.75</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2025-05-30 04:00:00+00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.05508</td>\n",
       "      <td>10062.75</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2025-05-30 05:00:00+00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.06773</td>\n",
       "      <td>10139.50</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2025-05-30 06:00:00+00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.06838</td>\n",
       "      <td>10127.50</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2025-05-30 07:00:00+00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.05301</td>\n",
       "      <td>9569.25</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2025-05-30 08:00:00+00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.02913</td>\n",
       "      <td>8508.75</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2025-05-30 09:00:00+00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00183</td>\n",
       "      <td>7964.50</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2025-05-30 10:00:00+00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.00005</td>\n",
       "      <td>7567.75</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2025-05-30 11:00:00+00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.00071</td>\n",
       "      <td>7260.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2025-05-30 12:00:00+00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.00282</td>\n",
       "      <td>7145.25</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2025-05-30 13:00:00+00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.00319</td>\n",
       "      <td>7217.25</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2025-05-30 14:00:00+00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.00172</td>\n",
       "      <td>7089.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2025-05-30 15:00:00+00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.00003</td>\n",
       "      <td>7798.25</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2025-05-30 16:00:00+00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00700</td>\n",
       "      <td>8593.50</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2025-05-30 17:00:00+00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.07004</td>\n",
       "      <td>9680.50</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2025-05-30 18:00:00+00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.11000</td>\n",
       "      <td>10525.75</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2025-05-30 19:00:00+00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.13527</td>\n",
       "      <td>11285.50</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2025-05-30 20:00:00+00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.15018</td>\n",
       "      <td>11704.25</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2025-05-30 21:00:00+00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.13019</td>\n",
       "      <td>11579.75</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2025-05-30 22:00:00+00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.10815</td>\n",
       "      <td>11043.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Timestamp         Load    Price  Forecast_Load  Flow_GB  \\\n",
       "34 2025-05-29 12:00:00+00:00  7270.250000 -0.00200        6918.50   116.75   \n",
       "33 2025-05-29 13:00:00+00:00  7063.000000 -0.00200        6680.75   300.25   \n",
       "32 2025-05-29 14:00:00+00:00  6716.000000 -0.00276        6699.75   740.75   \n",
       "31 2025-05-29 15:00:00+00:00  6588.666667 -0.00159        6948.50   730.75   \n",
       "30 2025-05-29 16:00:00+00:00          NaN  0.02000        7925.75      NaN   \n",
       "29 2025-05-29 17:00:00+00:00          NaN  0.05962        9077.00      NaN   \n",
       "28 2025-05-29 18:00:00+00:00          NaN  0.07351       10194.75      NaN   \n",
       "27 2025-05-29 19:00:00+00:00          NaN  0.07520       11271.75      NaN   \n",
       "26 2025-05-29 20:00:00+00:00          NaN  0.07340       11671.25      NaN   \n",
       "25 2025-05-29 21:00:00+00:00          NaN  0.07630       11542.25      NaN   \n",
       "24 2025-05-29 22:00:00+00:00          NaN  0.05700       11224.00      NaN   \n",
       "23 2025-05-29 23:00:00+00:00          NaN  0.04490       10831.75      NaN   \n",
       "22 2025-05-30 00:00:00+00:00          NaN  0.02746       10529.50      NaN   \n",
       "21 2025-05-30 01:00:00+00:00          NaN  0.03348       10226.00      NaN   \n",
       "20 2025-05-30 02:00:00+00:00          NaN  0.03395       10095.00      NaN   \n",
       "19 2025-05-30 03:00:00+00:00          NaN  0.03739       10026.75      NaN   \n",
       "18 2025-05-30 04:00:00+00:00          NaN  0.05508       10062.75      NaN   \n",
       "17 2025-05-30 05:00:00+00:00          NaN  0.06773       10139.50      NaN   \n",
       "16 2025-05-30 06:00:00+00:00          NaN  0.06838       10127.50      NaN   \n",
       "15 2025-05-30 07:00:00+00:00          NaN  0.05301        9569.25      NaN   \n",
       "14 2025-05-30 08:00:00+00:00          NaN  0.02913        8508.75      NaN   \n",
       "13 2025-05-30 09:00:00+00:00          NaN  0.00183        7964.50      NaN   \n",
       "12 2025-05-30 10:00:00+00:00          NaN -0.00005        7567.75      NaN   \n",
       "11 2025-05-30 11:00:00+00:00          NaN -0.00071        7260.00      NaN   \n",
       "10 2025-05-30 12:00:00+00:00          NaN -0.00282        7145.25      NaN   \n",
       "9  2025-05-30 13:00:00+00:00          NaN -0.00319        7217.25      NaN   \n",
       "8  2025-05-30 14:00:00+00:00          NaN -0.00172        7089.00      NaN   \n",
       "7  2025-05-30 15:00:00+00:00          NaN -0.00003        7798.25      NaN   \n",
       "6  2025-05-30 16:00:00+00:00          NaN  0.00700        8593.50      NaN   \n",
       "5  2025-05-30 17:00:00+00:00          NaN  0.07004        9680.50      NaN   \n",
       "4  2025-05-30 18:00:00+00:00          NaN  0.11000       10525.75      NaN   \n",
       "3  2025-05-30 19:00:00+00:00          NaN  0.13527       11285.50      NaN   \n",
       "2  2025-05-30 20:00:00+00:00          NaN  0.15018       11704.25      NaN   \n",
       "1  2025-05-30 21:00:00+00:00          NaN  0.13019       11579.75      NaN   \n",
       "0  2025-05-30 22:00:00+00:00          NaN  0.10815       11043.00      NaN   \n",
       "\n",
       "    Flow_NO  Total_Flow  \n",
       "34      0.0      116.75  \n",
       "33      0.0      300.25  \n",
       "32      0.0      740.75  \n",
       "31      0.0      730.75  \n",
       "30      NaN         NaN  \n",
       "29      NaN         NaN  \n",
       "28      NaN         NaN  \n",
       "27      NaN         NaN  \n",
       "26      NaN         NaN  \n",
       "25      NaN         NaN  \n",
       "24      NaN         NaN  \n",
       "23      NaN         NaN  \n",
       "22      NaN         NaN  \n",
       "21      NaN         NaN  \n",
       "20      NaN         NaN  \n",
       "19      NaN         NaN  \n",
       "18      NaN         NaN  \n",
       "17      NaN         NaN  \n",
       "16      NaN         NaN  \n",
       "15      NaN         NaN  \n",
       "14      NaN         NaN  \n",
       "13      NaN         NaN  \n",
       "12      NaN         NaN  \n",
       "11      NaN         NaN  \n",
       "10      NaN         NaN  \n",
       "9       NaN         NaN  \n",
       "8       NaN         NaN  \n",
       "7       NaN         NaN  \n",
       "6       NaN         NaN  \n",
       "5       NaN         NaN  \n",
       "4       NaN         NaN  \n",
       "3       NaN         NaN  \n",
       "2       NaN         NaN  \n",
       "1       NaN         NaN  \n",
       "0       NaN         NaN  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import sqlite3\n",
    "\n",
    "def view_last_rows(db_path, table_name, n=35):\n",
    "    print(f\"\\n🔍 Laatste {n} rijen van {table_name} in {db_path}:\")\n",
    "    try:\n",
    "        conn = sqlite3.connect(db_path)\n",
    "        query = f\"SELECT * FROM {table_name} ORDER BY Timestamp DESC LIMIT {n}\"\n",
    "        df = pd.read_sql_query(query, conn)\n",
    "        conn.close()\n",
    "\n",
    "        # Converteer Timestamp naar datetime voor leesbaarheid\n",
    "        if 'Timestamp' in df.columns:\n",
    "            df['Timestamp'] = pd.to_datetime(df['Timestamp'], errors='coerce')\n",
    "\n",
    "        # Sorteer weer oplopend zodat je de tijdlijn van oud → nieuw ziet\n",
    "        df = df.sort_values(\"Timestamp\")\n",
    "        display(df)  # Werkt in Jupyter Notebook\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Fout bij ophalen van {table_name}: {e}\")\n",
    "\n",
    "# Run deze met je juiste pad\n",
    "view_last_rows(\"../src/data/WARP.db\", \"transform_entsoe_obs\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.10.13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
