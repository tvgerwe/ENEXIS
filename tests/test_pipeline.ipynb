{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "384b4d96",
   "metadata": {},
   "source": [
    "# Folder structuur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c2e44b78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“ ENEXIS/\n",
      "    ğŸ“„ .cache.sqlite\n",
      "    ğŸ“„ .gitattributes\n",
      "    ğŸ“„ .gitignore\n",
      "    ğŸ“„ actual_total_load_nl_2022_2024.csv\n",
      "    ğŸ“„ cleaned_output_2025-05-05.txt\n",
      "    ğŸ“„ electricity_data_nl_2022_2024 kolommen.csv\n",
      "    ğŸ“„ electricity_data_nl_2022_2024.csv\n",
      "    ğŸ“„ electricity_data_nl_2022_2024_hourly.csv\n",
      "    ğŸ“„ electricity_data_nl_2022_2025_hourly.csv\n",
      "    ğŸ“„ electricity_data_nl_2022_2025_hourly_flow.csv\n",
      "    ğŸ“„ electricity_data_nl_2022_2025_utc.csv\n",
      "    ğŸ“„ fetch_entsoe_data.py\n",
      "    ğŸ“„ fetch_entsoe_data_v.py\n",
      "    ğŸ“„ hourly_rmse_table.html\n",
      "    ğŸ“„ import pandas as pd v2.py\n",
      "    ğŸ“„ import pandas as pd v3.py\n",
      "    ğŸ“„ import pandas as pd v4.py\n",
      "    ğŸ“„ import pandas as pd.py\n",
      "    ğŸ“„ naive_forecast_interactive.html\n",
      "    ğŸ“„ naive_forecast_validation.csv\n",
      "    ğŸ“„ output.csv\n",
      "    ğŸ“„ pip install entsoe-py.py\n",
      "    ğŸ“„ pipeline_flow.png\n",
      "    ğŸ“„ python code ENTSO-E v2.ipynb\n",
      "    ğŸ“„ python code ENTSO-E.ipynb\n",
      "    ğŸ“„ raw_entsoe.csv\n",
      "    ğŸ“„ README.md\n",
      "    ğŸ“„ requirements.txt\n",
      "    ğŸ“„ step_by_step.md\n",
      "    ğŸ“„ test.ipynb\n",
      "    ğŸ“„ test.py\n",
      "    ğŸ“„ transform_entsoe.csv\n",
      "    ğŸ“„ validation_results.csv\n",
      "    ğŸ“ #/\n",
      "        ğŸ“„ pyvenv.cfg\n",
      "        ğŸ“ bin/\n",
      "            ğŸ“„ activate\n",
      "            ğŸ“„ activate.csh\n",
      "            ğŸ“„ activate.fish\n",
      "            ğŸ“„ Activate.ps1\n",
      "            ğŸ“„ pip\n",
      "            ğŸ“„ pip3\n",
      "            ğŸ“„ pip3.9\n",
      "            ğŸ“„ python\n",
      "            ğŸ“„ python3\n",
      "            ğŸ“„ python3.9\n",
      "        ğŸ“ lib/\n",
      "    ğŸ“ .git/\n",
      "        ğŸ“„ COMMIT_EDITMSG\n",
      "        ğŸ“„ config\n",
      "        ğŸ“„ description\n",
      "        ğŸ“„ FETCH_HEAD\n",
      "        ğŸ“„ HEAD\n",
      "        ğŸ“„ index\n",
      "        ğŸ“„ ORIG_HEAD\n",
      "        ğŸ“„ packed-refs\n",
      "        ğŸ“ hooks/\n",
      "            ğŸ“„ applypatch-msg.sample\n",
      "            ğŸ“„ commit-msg.sample\n",
      "            ğŸ“„ fsmonitor-watchman.sample\n",
      "            ğŸ“„ post-update.sample\n",
      "            ğŸ“„ pre-applypatch.sample\n",
      "            ğŸ“„ pre-commit.sample\n",
      "            ğŸ“„ pre-merge-commit.sample\n",
      "            ğŸ“„ pre-push.sample\n",
      "            ğŸ“„ pre-rebase.sample\n",
      "            ğŸ“„ pre-receive.sample\n",
      "            ğŸ“„ prepare-commit-msg.sample\n",
      "            ğŸ“„ push-to-checkout.sample\n",
      "            ğŸ“„ sendemail-validate.sample\n",
      "            ğŸ“„ update.sample\n",
      "        ğŸ“ info/\n",
      "            ğŸ“„ exclude\n",
      "        ğŸ“ logs/\n",
      "            ğŸ“„ HEAD\n",
      "        ğŸ“ objects/\n",
      "        ğŸ“ refs/\n",
      "    ğŸ“ .github/\n",
      "        ğŸ“ workflows/\n",
      "            ğŸ“„ ci.yml\n",
      "            ğŸ“„ index.md\n",
      "    ğŸ“ .history/\n",
      "        ğŸ“„ .gitignore_20250326100045\n",
      "        ğŸ“„ .gitignore_20250326101227\n",
      "        ğŸ“„ .gitignore_20250326101525\n",
      "        ğŸ“„ .gitignore_20250326101526\n",
      "        ğŸ“„ .gitignore_20250407110310\n",
      "        ğŸ“„ .gitignore_20250407110339\n",
      "        ğŸ“„ .gitignore_20250509201151\n",
      "        ğŸ“„ .gitignore_20250509201836\n",
      "        ğŸ“„ README_20250414152007.md\n",
      "        ğŸ“„ README_20250416110720.md\n",
      "        ğŸ“„ README_20250416110728.md\n",
      "        ğŸ“„ README_20250416110729.md\n",
      "        ğŸ“„ README_20250416110733.md\n",
      "        ğŸ“„ README_20250416110746.md\n",
      "        ğŸ“„ README_20250416110805.md\n",
      "        ğŸ“„ README_20250416110938.md\n",
      "        ğŸ“„ README_20250416111233.md\n",
      "        ğŸ“„ README_20250416111257.md\n",
      "        ğŸ“„ README_20250416111303.md\n",
      "        ğŸ“„ README_20250416111320.md\n",
      "        ğŸ“„ README_20250416111343.md\n",
      "        ğŸ“„ README_20250416111408.md\n",
      "        ğŸ“„ README_20250416111430.md\n",
      "        ğŸ“„ README_20250416111432.md\n",
      "        ğŸ“„ README_20250416111435.md\n",
      "        ğŸ“„ README_20250416111441.md\n",
      "        ğŸ“„ README_20250416111444.md\n",
      "        ğŸ“„ README_20250416111447.md\n",
      "        ğŸ“„ test_20250228090944.py\n",
      "        ğŸ“„ test_20250318073937.py\n",
      "        ğŸ“„ test_20250318073940.py\n",
      "        ğŸ“„ test_20250318073945.py\n",
      "        ğŸ“ docs/\n",
      "            ğŸ“„ readme_MVP_20250414100703.md\n",
      "            ğŸ“„ readme_MVP_20250414100707.md\n",
      "            ğŸ“„ readme_MVP_20250414100737.md\n",
      "            ğŸ“„ readme_MVP_20250414100745.md\n",
      "            ğŸ“„ readme_MVP_20250414100748.md\n",
      "            ğŸ“„ readme_MVP_20250414100751.md\n",
      "            ğŸ“„ readme_MVP_20250414100821.md\n",
      "            ğŸ“„ readme_MVP_20250414100922.md\n",
      "            ğŸ“„ readme_MVP_20250414100931.md\n",
      "            ğŸ“„ readme_MVP_20250414100933.md\n",
      "            ğŸ“„ readme_MVP_20250414165242.md\n",
      "            ğŸ“„ readme_MVP_20250414165244.md\n",
      "            ğŸ“„ readme_MVP_20250414165247.md\n",
      "            ğŸ“„ readme_MVP_20250414165411.md\n",
      "            ğŸ“„ readme_MVP_20250414165415.md\n",
      "            ğŸ“„ readme_MVP_20250414165418.md\n",
      "            ğŸ“„ readme_MVP_20250416111602.md\n",
      "            ğŸ“„ readme_MVP_20250416111609.md\n",
      "            ğŸ“„ readme_MVP_20250416111613.md\n",
      "            ğŸ“„ readme_MVP_20250416111650.md\n",
      "            ğŸ“„ readme_MVP_20250416111654.md\n",
      "            ğŸ“„ readme_MVP_20250416111702.md\n",
      "            ğŸ“„ readme_MVP_20250416111750.md\n",
      "            ğŸ“„ readme_MVP_20250416111751.md\n",
      "            ğŸ“„ readme_MVP_20250416111804.md\n",
      "            ğŸ“„ readme_MVP_20250416111806.md\n",
      "            ğŸ“„ readme_MVP_20250416111808.md\n",
      "            ğŸ“„ readme_MVP_20250416111812.md\n",
      "            ğŸ“„ readme_MVP_20250416111813.md\n",
      "            ğŸ“„ readme_MVP_20250416111822.md\n",
      "            ğŸ“„ readme_MVP_20250417081105.md\n",
      "            ğŸ“„ readme_MVP_20250417092835.md\n",
      "        ğŸ“ src/\n",
      "        ğŸ“ workspaces/\n",
      "    ğŸ“ .venv/\n",
      "        ğŸ“„ .gitignore\n",
      "        ğŸ“„ pyvenv.cfg\n",
      "        ğŸ“ Include/\n",
      "        ğŸ“ Lib/\n",
      "        ğŸ“ Scripts/\n",
      "            ğŸ“„ activate\n",
      "            ğŸ“„ activate.bat\n",
      "            ğŸ“„ activate.fish\n",
      "            ğŸ“„ Activate.ps1\n",
      "            ğŸ“„ deactivate.bat\n",
      "            ğŸ“„ debugpy-adapter.exe\n",
      "            ğŸ“„ debugpy.exe\n",
      "            ğŸ“„ ipython.exe\n",
      "            ğŸ“„ ipython3.exe\n",
      "            ğŸ“„ jupyter-kernel.exe\n",
      "            ğŸ“„ jupyter-kernelspec.exe\n",
      "            ğŸ“„ jupyter-migrate.exe\n",
      "            ğŸ“„ jupyter-run.exe\n",
      "            ğŸ“„ jupyter-troubleshoot.exe\n",
      "            ğŸ“„ jupyter.exe\n",
      "            ğŸ“„ pip.exe\n",
      "            ğŸ“„ pip3.13.exe\n",
      "            ğŸ“„ pip3.exe\n",
      "            ğŸ“„ pygmentize.exe\n",
      "            ğŸ“„ python.exe\n",
      "            ğŸ“„ pythonw.exe\n",
      "            ğŸ“„ pywin32_postinstall.exe\n",
      "            ğŸ“„ pywin32_postinstall.py\n",
      "            ğŸ“„ pywin32_testall.exe\n",
      "            ğŸ“„ pywin32_testall.py\n",
      "        ğŸ“ share/\n",
      "    ğŸ“ create/\n",
      "        ğŸ“„ pyvenv.cfg\n",
      "        ğŸ“ bin/\n",
      "            ğŸ“„ activate\n",
      "            ğŸ“„ activate.csh\n",
      "            ğŸ“„ activate.fish\n",
      "            ğŸ“„ Activate.ps1\n",
      "            ğŸ“„ pip\n",
      "            ğŸ“„ pip3\n",
      "            ğŸ“„ pip3.9\n",
      "            ğŸ“„ python\n",
      "            ğŸ“„ python3\n",
      "            ğŸ“„ python3.9\n",
      "        ğŸ“ lib/\n",
      "    ğŸ“ docs/\n",
      "        ğŸ“„ data_dictionary.md\n",
      "        ğŸ“„ index.md\n",
      "        ğŸ“„ modeling_notes.md\n",
      "        ğŸ“„ price pred oxygent.png\n",
      "        ğŸ“„ readme_MVP.md\n",
      "        ğŸ“„ WARP_wireframe.png\n",
      "    ğŸ“ environment/\n",
      "        ğŸ“„ pyvenv.cfg\n",
      "        ğŸ“ bin/\n",
      "            ğŸ“„ activate\n",
      "            ğŸ“„ activate.csh\n",
      "            ğŸ“„ activate.fish\n",
      "            ğŸ“„ Activate.ps1\n",
      "            ğŸ“„ pip\n",
      "            ğŸ“„ pip3\n",
      "            ğŸ“„ pip3.9\n",
      "            ğŸ“„ python\n",
      "            ğŸ“„ python3\n",
      "            ğŸ“„ python3.9\n",
      "        ğŸ“ lib/\n",
      "    ğŸ“ flows/\n",
      "        ğŸ“„ full_pipeline_flow.py\n",
      "        ğŸ“„ README.md\n",
      "    ğŸ“ src/\n",
      "        ğŸ“„ __init__.py\n",
      "        ğŸ“ config/\n",
      "            ğŸ“„ config.json\n",
      "            ğŸ“„ config.py\n",
      "            ğŸ“„ house_style.py\n",
      "        ğŸ“ data/\n",
      "            ğŸ“„ logs.db\n",
      "            ğŸ“„ warp-csv-dataset.csv\n",
      "            ğŸ“„ WARP.db\n",
      "        ğŸ“ data_ingestion/\n",
      "            ğŸ“„ .cache.sqlite\n",
      "            ğŸ“„ API_open_meteo_historical.ipynb\n",
      "            ğŸ“„ API_open_meteo_preds.ipynb\n",
      "            ğŸ“„ API_open_meteo_preds_readCSV.ipynb\n",
      "            ğŸ“„ ingest_date.py\n",
      "            ğŸ“„ ingest_date_DST.py\n",
      "            ğŸ“„ ingest_entsoe.py\n",
      "            ğŸ“„ ingest_meteo_forecast_now.py\n",
      "            ğŸ“„ ingest_meteo_historical_pred.py\n",
      "            ğŸ“„ ingest_meteo_obs.py\n",
      "            ğŸ“„ ingest_ned.py\n",
      "            ğŸ“„ ingest_open-meteo_obs.py\n",
      "            ğŸ“„ NED.py\n",
      "            ğŸ“„ NED_preds_API_to_db.ipynb\n",
      "            ğŸ“„ warp-gen-feature-selection.ipynb\n",
      "        ğŸ“ data_master/\n",
      "            ğŸ“„ build_master_observed.py\n",
      "            ğŸ“„ build_master_predictions.py\n",
      "            ğŸ“„ check_master_WARP.ipynb\n",
      "            ğŸ“„ master_merging\n",
      "            ğŸ“„ merge_obs_preds.py\n",
      "        ğŸ“ data_processing/\n",
      "            ğŸ“„ entsoe_dataprocessing.py\n",
      "            ğŸ“„ feature_eng.py\n",
      "            ğŸ“„ split.py\n",
      "            ğŸ“„ transform_meteo_forecast_now.py\n",
      "            ğŸ“„ transform_meteo_obs.py\n",
      "            ğŸ“„ transform_meteo_preds.py\n",
      "            ğŸ“„ transform_meteo_preds_history.py\n",
      "            ğŸ“„ transform_ned.py\n",
      "            ğŸ“„ transform_NED_obs_2.ipynb\n",
      "            ğŸ“„ transform_NED_preds.ipynb\n",
      "            ğŸ“„ transform_open_meteo_preds.ipynb\n",
      "            ğŸ“„ transform_weather_obs.ipynb\n",
      "            ğŸ“„ transform_weather_preds.ipynb\n",
      "            ğŸ“„ __init__.py\n",
      "        ğŸ“ load-data/\n",
      "            ğŸ“„ .cache.sqlite\n",
      "            ğŸ“„ load_data 2.py\n",
      "            ğŸ“„ load_data.py\n",
      "            ğŸ“„ ned-api-descriptive-analysis.ipynb\n",
      "            ğŸ“„ ned-api-get-all-types_NEW.ipynb\n",
      "            ğŸ“„ ned-api-get-all-types_OLD.ipynb\n",
      "            ğŸ“„ ned-api-get-yrs-data-0-type 2.ipynb\n",
      "            ğŸ“„ ned-api-get-yrs-data-0-type.ipynb\n",
      "            ğŸ“„ NED-api-PREDs-7Types-thinclient.ipynb\n",
      "        ğŸ“ models/\n",
      "            ğŸ“„ Benchmark_model.py\n",
      "            ğŸ“„ benchmark_model_diagnostics.ipynb\n",
      "            ğŸ“„ naive_benchmark_timeseries.ipynb\n",
      "            ğŸ“„ naive_model.py\n",
      "            ğŸ“„ naive_model_diagnostics.ipynb\n",
      "            ğŸ“„ ned-gen-merge-data-prediction-models.ipynb\n",
      "            ğŸ“„ ned-gen-prediction.ipynb\n",
      "            ğŸ“„ sarimax_model.py\n",
      "            ğŸ“„ Timeseries.ipynb\n",
      "            ğŸ“„ time_df-code_extended.ipynb\n",
      "            ğŸ“„ train-models.py\n",
      "            ğŸ“„ WARP.db\n",
      "            ğŸ“„ XGBoost_evaluation.ipynb\n",
      "        ğŸ“ notebooks/\n",
      "            ğŸ“„ index.md\n",
      "        ğŸ“ src/\n",
      "        ğŸ“ utils/\n",
      "            ğŸ“„ auto_arima_optimizer.py\n",
      "            ğŸ“„ logger.py\n",
      "            ğŸ“„ log_rmse_to_sqlite.py\n",
      "        ğŸ“ __pycache__/\n",
      "            ğŸ“„ __init__.cpython-310.pyc\n",
      "            ğŸ“„ __init__.cpython-311.pyc\n",
      "    ğŸ“ tests/\n",
      "        ğŸ“„ test_data.py\n",
      "        ğŸ“„ test_models.ipynb\n",
      "        ğŸ“„ test_pipeline.ipynb\n",
      "        ğŸ“„ test_raw_ned_obs.ipynb\n",
      "        ğŸ“„ test_transform_ned_obs.ipynb\n",
      "        ğŸ“„ test_TvG.py\n",
      "    ğŸ“ the/\n",
      "        ğŸ“„ pyvenv.cfg\n",
      "        ğŸ“ bin/\n",
      "            ğŸ“„ activate\n",
      "            ğŸ“„ activate.csh\n",
      "            ğŸ“„ activate.fish\n",
      "            ğŸ“„ Activate.ps1\n",
      "            ğŸ“„ pip\n",
      "            ğŸ“„ pip3\n",
      "            ğŸ“„ pip3.9\n",
      "            ğŸ“„ python\n",
      "            ğŸ“„ python3\n",
      "            ğŸ“„ python3.9\n",
      "        ğŸ“ lib/\n",
      "    ğŸ“ virtual/\n",
      "        ğŸ“„ pyvenv.cfg\n",
      "        ğŸ“ bin/\n",
      "            ğŸ“„ activate\n",
      "            ğŸ“„ activate.csh\n",
      "            ğŸ“„ activate.fish\n",
      "            ğŸ“„ Activate.ps1\n",
      "            ğŸ“„ pip\n",
      "            ğŸ“„ pip3\n",
      "            ğŸ“„ pip3.9\n",
      "            ğŸ“„ python\n",
      "            ğŸ“„ python3\n",
      "            ğŸ“„ python3.9\n",
      "        ğŸ“ lib/\n",
      "    ğŸ“ workspaces/\n",
      "        ğŸ“ redouan/\n",
      "            ğŸ“„ GUI_ENERGY_PRICES_202501010000-202601010000.csv\n",
      "            ğŸ“„ index.md\n",
      "        ğŸ“ sandeep/\n",
      "            ğŸ“„ 2021 01 05 - Enexis Energy - v1.ipynb\n",
      "            ğŸ“„ environment.yml\n",
      "            ğŸ“„ index.md\n",
      "            ğŸ“„ ned-plots 2.ipynb\n",
      "            ğŸ“„ ned-plots.ipynb\n",
      "        ğŸ“ sharell/\n",
      "            ğŸ“„ # Pad naar de SQLite-database.py\n",
      "            ğŸ“„ Code merge datasets, plots, regression.ipynb\n",
      "            ğŸ“„ correlation_matrix.html\n",
      "            ğŸ“„ ENTSO-E code plots.ipynb\n",
      "            ğŸ“„ entsoe_2022-2025_load\n",
      "            ğŸ“„ entsoe_exploration.py\n",
      "            ğŸ“„ entsoe_load.py\n",
      "            ğŸ“„ entsoe_test.py\n",
      "            ğŸ“„ index.md\n",
      "            ğŸ“„ Weather_data.ipynb\n",
      "        ğŸ“ twan/\n",
      "            ğŸ“„ .cache.sqlite\n",
      "            ğŸ“„ get_prices.ipynb\n",
      "            ğŸ“„ get_prices2.ipynb\n",
      "            ğŸ“„ get_prices_NEW.ipynb\n",
      "            ğŸ“„ index.md\n",
      "            ğŸ“„ merged_data_price_preds.csv\n",
      "            ğŸ“„ pred_accuracy.ipynb\n",
      "            ğŸ“„ proces_prices-NEW.ipynb\n",
      "            ğŸ“„ proces_prices.ipynb\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def print_tree(startpath, max_depth=3):\n",
    "    startpath = os.path.abspath(startpath)\n",
    "    for root, dirs, files in os.walk(startpath):\n",
    "        depth = root.replace(startpath, '').count(os.sep)\n",
    "        if depth >= max_depth:\n",
    "            dirs[:] = []  # stop met verder afdalen\n",
    "            continue\n",
    "        indent = ' ' * 4 * depth\n",
    "        print(f\"{indent}ğŸ“ {os.path.basename(root)}/\")\n",
    "        for f in files:\n",
    "            print(f\"{indent}    ğŸ“„ {f}\")\n",
    "\n",
    "# Pas dit pad aan indien nodig â€” bijvoorbeeld \".\" of \"../\"\n",
    "print_tree(\"..\", max_depth=3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "995c527d",
   "metadata": {},
   "source": [
    "# Code base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "15606154",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“ src/data_ingestion (14 bestanden):\n",
      "  - src\\data_ingestion\\API_open_meteo_historical.ipynb\n",
      "  - src\\data_ingestion\\API_open_meteo_preds.ipynb\n",
      "  - src\\data_ingestion\\API_open_meteo_preds_readCSV.ipynb\n",
      "  - src\\data_ingestion\\ingest_date.py\n",
      "  - src\\data_ingestion\\ingest_date_DST.py\n",
      "  - src\\data_ingestion\\ingest_entsoe.py\n",
      "  - src\\data_ingestion\\ingest_meteo_forecast_now.py\n",
      "  - src\\data_ingestion\\ingest_meteo_historical_pred.py\n",
      "  - src\\data_ingestion\\ingest_meteo_obs.py\n",
      "  - src\\data_ingestion\\ingest_ned.py\n",
      "  - src\\data_ingestion\\ingest_open-meteo_obs.py\n",
      "  - src\\data_ingestion\\NED.py\n",
      "  - src\\data_ingestion\\NED_preds_API_to_db.ipynb\n",
      "  - src\\data_ingestion\\warp-gen-feature-selection.ipynb\n",
      "\n",
      "ğŸ“ src/data_processing (14 bestanden):\n",
      "  - src\\data_processing\\entsoe_dataprocessing.py\n",
      "  - src\\data_processing\\feature_eng.py\n",
      "  - src\\data_processing\\split.py\n",
      "  - src\\data_processing\\transform_meteo_forecast_now.py\n",
      "  - src\\data_processing\\transform_meteo_obs.py\n",
      "  - src\\data_processing\\transform_meteo_preds.py\n",
      "  - src\\data_processing\\transform_meteo_preds_history.py\n",
      "  - src\\data_processing\\transform_ned.py\n",
      "  - src\\data_processing\\transform_NED_obs_2.ipynb\n",
      "  - src\\data_processing\\transform_NED_preds.ipynb\n",
      "  - src\\data_processing\\transform_open_meteo_preds.ipynb\n",
      "  - src\\data_processing\\transform_weather_obs.ipynb\n",
      "  - src\\data_processing\\transform_weather_preds.ipynb\n",
      "  - src\\data_processing\\__init__.py\n",
      "\n",
      "ğŸ“ src/data_master (4 bestanden):\n",
      "  - src\\data_master\\build_master_observed.py\n",
      "  - src\\data_master\\build_master_predictions.py\n",
      "  - src\\data_master\\check_master_WARP.ipynb\n",
      "  - src\\data_master\\merge_obs_preds.py\n",
      "\n",
      "ğŸ“ src/models (12 bestanden):\n",
      "  - src\\models\\Benchmark_model.py\n",
      "  - src\\models\\benchmark_model_diagnostics.ipynb\n",
      "  - src\\models\\naive_benchmark_timeseries.ipynb\n",
      "  - src\\models\\naive_model.py\n",
      "  - src\\models\\naive_model_diagnostics.ipynb\n",
      "  - src\\models\\ned-gen-merge-data-prediction-models.ipynb\n",
      "  - src\\models\\ned-gen-prediction.ipynb\n",
      "  - src\\models\\sarimax_model.py\n",
      "  - src\\models\\Timeseries.ipynb\n",
      "  - src\\models\\time_df-code_extended.ipynb\n",
      "  - src\\models\\train-models.py\n",
      "  - src\\models\\XGBoost_evaluation.ipynb\n",
      "\n",
      "ğŸ“ src/utils (3 bestanden):\n",
      "  - src\\utils\\auto_arima_optimizer.py\n",
      "  - src\\utils\\logger.py\n",
      "  - src\\utils\\log_rmse_to_sqlite.py\n",
      "\n",
      "ğŸ“ flows (1 bestanden):\n",
      "  - flows\\full_pipeline_flow.py\n",
      "\n",
      "ğŸ“ tests (6 bestanden):\n",
      "  - tests\\test_data.py\n",
      "  - tests\\test_models.ipynb\n",
      "  - tests\\test_pipeline.ipynb\n",
      "  - tests\\test_raw_ned_obs.ipynb\n",
      "  - tests\\test_transform_ned_obs.ipynb\n",
      "  - tests\\test_TvG.py\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Pas dit pad aan als notebook niet 1 niveau onder project root zit\n",
    "project_root = os.path.abspath(\"..\")\n",
    "relevante_mappen = [\n",
    "    \"src/data_ingestion\",\n",
    "    \"src/data_processing\",\n",
    "    \"src/data_master\",\n",
    "    \"src/models\",\n",
    "    \"src/utils\",\n",
    "    \"flows\",\n",
    "    \"tests\",\n",
    "    # optioneel:\n",
    "    # \"workspaces/redouan\",\n",
    "    # \"workspaces/sandeep\",\n",
    "    # \"workspaces/sharell\",\n",
    "    # \"workspaces/twan\"\n",
    "]\n",
    "\n",
    "bestand_inventaris = {}\n",
    "\n",
    "for mapnaam in relevante_mappen:\n",
    "    map_pad = os.path.join(project_root, mapnaam)\n",
    "    if os.path.exists(map_pad):\n",
    "        bestanden = []\n",
    "        for root, _, files in os.walk(map_pad):\n",
    "            for file in files:\n",
    "                if file.endswith((\".py\", \".ipynb\")):\n",
    "                    rel_path = os.path.relpath(os.path.join(root, file), project_root)\n",
    "                    bestanden.append(rel_path)\n",
    "        bestand_inventaris[mapnaam] = bestanden\n",
    "    else:\n",
    "        bestand_inventaris[mapnaam] = [\"âŒ Map niet gevonden\"]\n",
    "\n",
    "# Print overzicht\n",
    "for mapnaam, bestanden in bestand_inventaris.items():\n",
    "    print(f\"\\nğŸ“ {mapnaam} ({len(bestanden)} bestanden):\")\n",
    "    for bestand in bestanden:\n",
    "        print(f\"  - {bestand}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21172b01",
   "metadata": {},
   "source": [
    "# DB-scan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "57a22014",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“¦ Gevonden tabellen in WARP.db:\n",
      "\n",
      "ğŸ”¹ raw_ned_obs: 40807 records\n",
      "ğŸ”¹ transform_ned_obs: 29255 records\n",
      "ğŸ”¹ raw_entsoe_obs: 12627 records\n",
      "ğŸ”¹ raw_ned_df: 26304 records\n",
      "ğŸ”¹ dim_datetime: 3576 records\n",
      "ğŸ”¹ raw_meteo_preds_history: 3192 records\n",
      "ğŸ”¹ raw_meteo_obs: 3187 records\n",
      "ğŸ”¹ transform_meteo_forecast_now: 173 records\n",
      "ğŸ”¹ transform_weather_preds_history: 3192 records\n",
      "ğŸ”¹ raw_ned_obs_2: 12764 records\n",
      "ğŸ”¹ raw_weather_obs: 3202 records\n",
      "ğŸ”¹ transform_weather_obs: 3202 records\n",
      "ğŸ”¹ transform_entsoe_obs: 3167 records\n",
      "ğŸ”¹ transform_ned_obs_2: 3191 records\n",
      "ğŸ”¹ process_weather_preds: 8568 records\n",
      "ğŸ”¹ master_warp: 3355 records\n",
      "ğŸ”¹ raw_NED_preds: 153602 records\n",
      "ğŸ”¹ raw_weather_preds: 3408 records\n",
      "ğŸ”¹ raw_meteo_forecast_now: 192 records\n",
      "ğŸ”¹ processed_NED_preds: 22078 records\n",
      "ğŸ”¹ master_predictions: 9912 records\n"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "import os\n",
    "\n",
    "# Pad naar jouw DB (pas aan als nodig)\n",
    "db_path = os.path.abspath(\"../src/data/WARP.db\")\n",
    "\n",
    "if not os.path.exists(db_path):\n",
    "    raise FileNotFoundError(f\"Database niet gevonden op: {db_path}\")\n",
    "\n",
    "# Verbinding maken\n",
    "conn = sqlite3.connect(db_path)\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Tabellen ophalen\n",
    "cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n",
    "tables = cursor.fetchall()\n",
    "\n",
    "print(f\"ğŸ“¦ Gevonden tabellen in {os.path.basename(db_path)}:\\n\")\n",
    "\n",
    "# Voor elke tabel: print aantal rijen\n",
    "for table in tables:\n",
    "    name = table[0]\n",
    "    try:\n",
    "        cursor.execute(f\"SELECT COUNT(*) FROM {name}\")\n",
    "        count = cursor.fetchone()[0]\n",
    "        print(f\"ğŸ”¹ {name}: {count} records\")\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ Fout bij {name}: {e}\")\n",
    "\n",
    "conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d0186dd7",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'auto_arima_order' from 'models.sarimax_model' (c:\\Users\\dai\\ENEXIS\\src\\models\\sarimax_model.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 27\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mdata_processing\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msplit\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m time_based_split\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnaive_model\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m run_naive_model\n\u001b[1;32m---> 27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msarimax_model\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m run_sarimax, auto_arima_order\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlogger\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m init_logger, log_info\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;66;03m# âœ… 3. Logging starten\u001b[39;00m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'auto_arima_order' from 'models.sarimax_model' (c:\\Users\\dai\\ENEXIS\\src\\models\\sarimax_model.py)"
     ]
    }
   ],
   "source": [
    "# ----------------------------\n",
    "# âœ… 1. FIX voor imports in Jupyter\n",
    "# ----------------------------\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Stel pad naar src/ in (pas aan indien je notebook elders staat)\n",
    "src_path = os.path.abspath(\"../src\")  # Als je notebook in /tests staat\n",
    "if src_path not in sys.path:\n",
    "    sys.path.append(src_path)\n",
    "\n",
    "# ----------------------------\n",
    "# âœ… 2. Imports modules uit src/\n",
    "# ----------------------------\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "from data_processing.feature_eng import (\n",
    "    add_lag_features,\n",
    "    add_rolling_features,\n",
    "    add_time_features,\n",
    "    scale_features\n",
    ")\n",
    "from data_processing.split import time_based_split\n",
    "from models.naive_model import run_naive_model\n",
    "from models.sarimax_model import run_sarimax, auto_arima_order\n",
    "from utils.logger import init_logger, log_info\n",
    "\n",
    "# ----------------------------\n",
    "# âœ… 3. Logging starten\n",
    "# ----------------------------\n",
    "init_logger()\n",
    "log_info(\"Starting test pipeline run\", module=\"test_pipeline\")\n",
    "\n",
    "# ----------------------------\n",
    "# âœ… 4. Dummy DataFrame aanmaken\n",
    "# ----------------------------\n",
    "np.random.seed(42)\n",
    "dates = pd.date_range(start=\"2025-01-01\", periods=100, freq=\"H\")\n",
    "df = pd.DataFrame({\n",
    "    \"datetime\": dates,\n",
    "    \"price\": np.random.normal(loc=50, scale=10, size=100),\n",
    "    \"load\": np.random.normal(loc=300, scale=30, size=100)\n",
    "})\n",
    "df.set_index(\"datetime\", inplace=True)\n",
    "df.reset_index(inplace=True)\n",
    "\n",
    "# ----------------------------\n",
    "# âœ… 5. Feature Engineering\n",
    "# ----------------------------\n",
    "df = add_lag_features(df, columns=[\"price\", \"load\"], lags=[1, 24])\n",
    "df = add_rolling_features(df, columns=[\"price\", \"load\"], windows=[3, 24])\n",
    "df = add_time_features(df)\n",
    "df_scaled, _ = scale_features(df, columns=[\"price\", \"load\"])\n",
    "print(\"âœ… Feature engineering done:\", df_scaled.columns.tolist())\n",
    "\n",
    "# ----------------------------\n",
    "# âœ… 6. Train/Test splitsing\n",
    "# ----------------------------\n",
    "train_df, test_df = time_based_split(df_scaled, train_ratio=0.8)\n",
    "print(f\"âœ… Train size: {len(train_df)}, Test size: {len(test_df)}\")\n",
    "\n",
    "# ----------------------------\n",
    "# âœ… 7. Naive model testen\n",
    "# ----------------------------\n",
    "y_pred_naive, metrics_naive = run_naive_model(test_df, target_column=\"price\", lag=24)\n",
    "print(\"âœ… Naive RMSE:\", metrics_naive[\"rmse\"])\n",
    "\n",
    "# ----------------------------\n",
    "# âœ… 8. SARIMAX model testen\n",
    "# ----------------------------\n",
    "try:\n",
    "    order, seasonal_order = auto_arima_order(train_df, target_column=\"price\", m=24)\n",
    "    y_pred_sarimax, metrics_sarimax = run_sarimax(\n",
    "        train_df,\n",
    "        test_df,\n",
    "        target_column=\"price\",\n",
    "        order=order,\n",
    "        seasonal_order=seasonal_order\n",
    "    )\n",
    "    if y_pred_sarimax is not None:\n",
    "        print(\"âœ… SARIMAX RMSE:\", metrics_sarimax[\"rmse\"])\n",
    "    else:\n",
    "        print(\"â„¹ï¸ SARIMAX: combination already tested\")\n",
    "except Exception as e:\n",
    "    print(\"âŒ SARIMAX failed:\", e)\n",
    "\n",
    "print(\"ğŸ¯ Test pipeline run complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d16b263",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "import warnings\n",
    "import time\n",
    "import json\n",
    "import datetime\n",
    "import os\n",
    "from pathlib import Path\n",
    "warnings.filterwarnings('ignore')  # Suppress warnings for cleaner output\n",
    "\n",
    "# Function to initialize the log database\n",
    "def initialize_log_database(db_path='src/data/logs.db'):\n",
    "    \"\"\"Create the model_performance_log table if it doesn't exist\"\"\"\n",
    "    \n",
    "    # Ensure the directory exists\n",
    "    Path(os.path.dirname(db_path)).mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Connect to the database\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    # Create the model performance log table if it doesn't exist\n",
    "    cursor.execute('''\n",
    "    CREATE TABLE IF NOT EXISTS model_performance_log (\n",
    "        id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "        timestamp TEXT,\n",
    "        model_name TEXT,\n",
    "        model_version TEXT,\n",
    "        features_used TEXT,\n",
    "        parameters TEXT,\n",
    "        train_start_date TEXT,\n",
    "        train_end_date TEXT,\n",
    "        validation_start_date TEXT,\n",
    "        validation_end_date TEXT,\n",
    "        overall_rmse REAL,\n",
    "        min_daily_rmse REAL,\n",
    "        max_daily_rmse REAL,\n",
    "        avg_daily_rmse REAL,\n",
    "        hourly_rmse_distribution TEXT,\n",
    "        run_time_seconds REAL,\n",
    "        notes TEXT\n",
    "    )\n",
    "    ''')\n",
    "    \n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "    \n",
    "    print(f\"Log database initialized at {db_path}\")\n",
    "    return db_path\n",
    "\n",
    "# Function to log model performance\n",
    "def log_model_performance(\n",
    "    model_name,\n",
    "    model_version,\n",
    "    features_used,\n",
    "    parameters,\n",
    "    train_start_date,\n",
    "    train_end_date,\n",
    "    validation_start_date,\n",
    "    validation_end_date,\n",
    "    forecast_vs_actual,\n",
    "    daily_rmse,\n",
    "    run_time_seconds,\n",
    "    notes=\"\",\n",
    "    db_path='src/data/logs.db'\n",
    "):\n",
    "    \"\"\"\n",
    "    Log the performance of a model run to the database\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    model_name : str\n",
    "        Name of the model (e.g., 'SARIMAX', 'Naive', 'XGBoost', 'Prophet')\n",
    "    model_version : str\n",
    "        Version or configuration of the model (e.g., 'Basic', 'Enhanced', 'Full-Feature')\n",
    "    features_used : list\n",
    "        List of feature names used in the model\n",
    "    parameters : dict\n",
    "        Dictionary of model parameters\n",
    "    train_start_date, train_end_date : datetime\n",
    "        Start and end dates of the training period\n",
    "    validation_start_date, validation_end_date : datetime\n",
    "        Start and end dates of the validation period\n",
    "    forecast_vs_actual : DataFrame\n",
    "        DataFrame containing forecast and actual values with 'rmse' column\n",
    "    daily_rmse : Series\n",
    "        Series of daily RMSE values\n",
    "    run_time_seconds : float\n",
    "        Model training and prediction time in seconds\n",
    "    notes : str, optional\n",
    "        Additional notes about the model run\n",
    "    db_path : str, optional\n",
    "        Path to the database file\n",
    "    \"\"\"\n",
    "    \n",
    "    # Calculate performance metrics\n",
    "    overall_rmse = np.sqrt(mean_squared_error(\n",
    "        forecast_vs_actual['Price'], \n",
    "        forecast_vs_actual['Price_forecast']\n",
    "    ))\n",
    "    \n",
    "    min_daily_rmse = daily_rmse.min()\n",
    "    max_daily_rmse = daily_rmse.max()\n",
    "    avg_daily_rmse = daily_rmse.mean()\n",
    "    \n",
    "    # Get hourly RMSE distribution statistics\n",
    "    hourly_rmse_stats = {\n",
    "        'min': float(forecast_vs_actual['rmse'].min()),\n",
    "        'max': float(forecast_vs_actual['rmse'].max()),\n",
    "        'mean': float(forecast_vs_actual['rmse'].mean()),\n",
    "        'median': float(forecast_vs_actual['rmse'].median()),\n",
    "        'q25': float(forecast_vs_actual['rmse'].quantile(0.25)),\n",
    "        'q75': float(forecast_vs_actual['rmse'].quantile(0.75)),\n",
    "        'std': float(forecast_vs_actual['rmse'].std())\n",
    "    }\n",
    "    \n",
    "    # Connect to the database\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    # Insert the log entry\n",
    "    cursor.execute('''\n",
    "    INSERT INTO model_performance_log (\n",
    "        timestamp,\n",
    "        model_name,\n",
    "        model_version,\n",
    "        features_used,\n",
    "        parameters,\n",
    "        train_start_date,\n",
    "        train_end_date,\n",
    "        validation_start_date,\n",
    "        validation_end_date,\n",
    "        overall_rmse,\n",
    "        min_daily_rmse,\n",
    "        max_daily_rmse,\n",
    "        avg_daily_rmse,\n",
    "        hourly_rmse_distribution,\n",
    "        run_time_seconds,\n",
    "        notes\n",
    "    ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\n",
    "    ''', (\n",
    "        datetime.datetime.now().isoformat(),\n",
    "        model_name,\n",
    "        model_version,\n",
    "        json.dumps(features_used),\n",
    "        json.dumps(parameters),\n",
    "        train_start_date.isoformat(),\n",
    "        train_end_date.isoformat(),\n",
    "        validation_start_date.isoformat(),\n",
    "        validation_end_date.isoformat(),\n",
    "        overall_rmse,\n",
    "        min_daily_rmse,\n",
    "        max_daily_rmse,\n",
    "        avg_daily_rmse,\n",
    "        json.dumps(hourly_rmse_stats),\n",
    "        run_time_seconds,\n",
    "        notes\n",
    "    ))\n",
    "    \n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "    \n",
    "    print(f\"Model performance logged to {db_path}\")\n",
    "    return True\n",
    "\n",
    "# Function to get the logged model performances\n",
    "def get_model_performances(db_path='src/data/logs.db'):\n",
    "    \"\"\"Retrieve all model performance logs as a DataFrame\"\"\"\n",
    "    \n",
    "    conn = sqlite3.connect(db_path)\n",
    "    \n",
    "    # Query the database\n",
    "    query = \"SELECT * FROM model_performance_log ORDER BY timestamp DESC\"\n",
    "    logs_df = pd.read_sql_query(query, conn)\n",
    "    \n",
    "    conn.close()\n",
    "    \n",
    "    return logs_df\n",
    "\n",
    "# Function to create a comparison table of model performances\n",
    "def compare_models(db_path='src/data/logs.db', latest_only=True):\n",
    "    \"\"\"\n",
    "    Create a comparison table of model performances\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    db_path : str, optional\n",
    "        Path to the database file\n",
    "    latest_only : bool, optional\n",
    "        If True, only show the latest run of each model type\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    DataFrame\n",
    "        Comparison table of model performances\n",
    "    \"\"\"\n",
    "    \n",
    "    logs_df = get_model_performances(db_path)\n",
    "    \n",
    "    if logs_df.empty:\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # If latest_only is True, get only the latest run for each model type\n",
    "    if latest_only:\n",
    "        # Get latest run for each combination of model_name and model_version\n",
    "        logs_df['timestamp'] = pd.to_datetime(logs_df['timestamp'])\n",
    "        idx = logs_df.groupby(['model_name', 'model_version'])['timestamp'].idxmax()\n",
    "        logs_df = logs_df.loc[idx]\n",
    "    \n",
    "    # Select relevant columns for comparison\n",
    "    comparison_df = logs_df[[\n",
    "        'model_name', \n",
    "        'model_version', \n",
    "        'overall_rmse', \n",
    "        'avg_daily_rmse',\n",
    "        'min_daily_rmse',\n",
    "        'max_daily_rmse',\n",
    "        'run_time_seconds',\n",
    "        'timestamp'\n",
    "    ]].copy()\n",
    "    \n",
    "    # Sort by overall RMSE (best performing models first)\n",
    "    comparison_df = comparison_df.sort_values('overall_rmse')\n",
    "    \n",
    "    return comparison_df\n",
    "\n",
    "# Initialize the log database\n",
    "db_path = initialize_log_database()\n",
    "\n",
    "# Start timing the model run\n",
    "start_time = time.time()\n",
    "\n",
    "# Connect to the SQLite database\n",
    "conn = sqlite3.connect('src/data/WARP.db')\n",
    "\n",
    "# Read the master_warp table into a pandas DataFrame\n",
    "query = \"SELECT * FROM master_warp\"\n",
    "df = pd.read_sql_query(query, conn)\n",
    "\n",
    "# Convert datetime column from string to proper datetime format with UTC timezone\n",
    "df['datetime'] = pd.to_datetime(df['datetime'], utc=True)\n",
    "\n",
    "# Define data ranges for SARIMAX model\n",
    "train_start = pd.to_datetime('2025-01-01').tz_localize('UTC')\n",
    "observation_start = pd.to_datetime('2025-04-17').tz_localize('UTC')\n",
    "validation_start = pd.to_datetime('2025-04-24').tz_localize('UTC')\n",
    "validation_end = pd.to_datetime('2025-04-30 23:00:00').tz_localize('UTC')\n",
    "\n",
    "# Filter the data for each period\n",
    "train_data = df[(df['datetime'] >= train_start) & (df['datetime'] < observation_start)]\n",
    "observation_data = df[(df['datetime'] >= observation_start) & (df['datetime'] < validation_start)]\n",
    "validation_data = df[(df['datetime'] >= validation_start) & (df['datetime'] <= validation_end)]\n",
    "\n",
    "print(f\"Training data: {len(train_data)} hours\")\n",
    "print(f\"Observation data: {len(observation_data)} hours\")\n",
    "print(f\"Validation data: {len(validation_data)} hours\")\n",
    "\n",
    "# Sort the data by datetime\n",
    "train_data = train_data.sort_values('datetime')\n",
    "observation_data = observation_data.sort_values('datetime')\n",
    "validation_data = validation_data.sort_values('datetime')\n",
    "\n",
    "# ---- SARIMAX Model Implementation with All Features ----\n",
    "# Combine train and observation data for model fitting\n",
    "model_data = pd.concat([train_data, observation_data])\n",
    "model_data = model_data.sort_values('datetime')\n",
    "\n",
    "# Define all features we will use (excluding datetime, Price, and derived columns)\n",
    "feature_columns = [\n",
    "    # Flow features\n",
    "    'Flow_BE_to_NL', 'Flow_NL_to_BE', 'Flow_DE_to_NL', 'Flow_NL_to_DE',\n",
    "    'Flow_GB_to_NL', 'Flow_NL_to_GB', 'Flow_DK_to_NL', 'Flow_NL_to_DK',\n",
    "    'Flow_NO_to_NL', 'Flow_NL_to_NO', 'Flow_BE', 'Flow_DE', 'Flow_GB',\n",
    "    'Flow_DK', 'Flow_NO', 'Total_Flow',\n",
    "    \n",
    "    # Weather features\n",
    "    'temperature_2m', 'wind_speed_10m', 'apparent_temperature', 'cloud_cover',\n",
    "    'snowfall', 'diffuse_radiation', 'direct_normal_irradiance', 'shortwave_radiation',\n",
    "    \n",
    "    # Capacity features\n",
    "    'ned.capacity', 'ned.volume', 'ned.percentage',\n",
    "    \n",
    "    # Demand feature\n",
    "    'Load'\n",
    "]\n",
    "\n",
    "# Pre-existing time features from the dataset\n",
    "time_features = [\n",
    "    'hour_sin', 'hour_cos', 'weekday_sin', 'weekday_cos',\n",
    "    'yearday_sin', 'yearday_cos', 'is_holiday', 'is_weekend', 'is_non_working_day'\n",
    "]\n",
    "\n",
    "# Check which features are available in the data\n",
    "available_features = []\n",
    "for feature in feature_columns:\n",
    "    if feature in model_data.columns:\n",
    "        available_features.append(feature)\n",
    "\n",
    "for feature in time_features:\n",
    "    if feature in model_data.columns:\n",
    "        available_features.append(feature)\n",
    "\n",
    "print(f\"Using {len(available_features)} features in the model:\")\n",
    "print(available_features)\n",
    "\n",
    "# Set the datetime as index for the time series analysis\n",
    "model_data_ts = model_data[['datetime', 'Price'] + available_features].set_index('datetime')\n",
    "\n",
    "# Initialize the scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Scale all features except binary indicators\n",
    "non_binary_features = [f for f in available_features if f not in ['is_holiday', 'is_weekend', 'is_non_working_day']]\n",
    "model_data_ts[non_binary_features] = scaler.fit_transform(model_data_ts[non_binary_features])\n",
    "\n",
    "# Prepare exogenous variables for model\n",
    "exog = model_data_ts[available_features]\n",
    "\n",
    "# Fit SARIMAX model\n",
    "print(\"Fitting SARIMAX model with all available features...\")\n",
    "model = SARIMAX(\n",
    "    model_data_ts['Price'],\n",
    "    exog=exog,\n",
    "    order=(1, 1, 1),          # (p,d,q)\n",
    "    seasonal_order=(1, 1, 1, 24),  # (P,D,Q,s) - 24 for hourly data with daily seasonality\n",
    "    enforce_stationarity=False,\n",
    "    enforce_invertibility=False\n",
    ")\n",
    "\n",
    "# Fit the model with limited iterations to speed up the process\n",
    "results = model.fit(disp=False, maxiter=50)\n",
    "print(\"Model fitted successfully.\")\n",
    "\n",
    "# Prepare exogenous variables for forecasting the validation period\n",
    "validation_exog = validation_data[['datetime'] + available_features].copy()\n",
    "\n",
    "# Scale the validation features using the same scaler\n",
    "validation_exog[non_binary_features] = scaler.transform(validation_exog[non_binary_features])\n",
    "\n",
    "# Set the datetime as index\n",
    "validation_exog = validation_exog.set_index('datetime')[available_features]\n",
    "\n",
    "# Forecast the validation period\n",
    "print(\"Forecasting validation period...\")\n",
    "forecast = results.get_forecast(steps=len(validation_exog), exog=validation_exog)\n",
    "forecast_mean = forecast.predicted_mean\n",
    "forecast_ci = forecast.conf_int()\n",
    "\n",
    "# Create a dataframe with the forecast\n",
    "sarimax_forecast = pd.DataFrame({\n",
    "    'datetime': validation_data['datetime'],\n",
    "    'Price_forecast': forecast_mean.values\n",
    "})\n",
    "\n",
    "# Merge with actual validation data to compare\n",
    "forecast_vs_actual = sarimax_forecast.merge(\n",
    "    validation_data[['datetime', 'Price']], \n",
    "    on='datetime', \n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# ----- Calculate RMSE -----\n",
    "# Overall RMSE\n",
    "overall_rmse = np.sqrt(mean_squared_error(forecast_vs_actual['Price'], forecast_vs_actual['Price_forecast']))\n",
    "print(f\"\\nOverall RMSE: {overall_rmse:.2f}\")\n",
    "\n",
    "# RMSE per hour (for each of the 168 hours in the validation period)\n",
    "forecast_vs_actual['hour_num'] = range(1, len(forecast_vs_actual) + 1)\n",
    "forecast_vs_actual['squared_error'] = (forecast_vs_actual['Price'] - forecast_vs_actual['Price_forecast']) ** 2\n",
    "forecast_vs_actual['rmse'] = np.sqrt(forecast_vs_actual['squared_error'])\n",
    "\n",
    "# RMSE per day\n",
    "forecast_vs_actual['date'] = forecast_vs_actual['datetime'].dt.date\n",
    "daily_rmse = forecast_vs_actual.groupby('date').apply(\n",
    "    lambda x: np.sqrt(mean_squared_error(x['Price'], x['Price_forecast']))\n",
    ")\n",
    "print(\"\\nRMSE per day:\")\n",
    "print(daily_rmse)\n",
    "\n",
    "# Calculate run time\n",
    "run_time_seconds = time.time() - start_time\n",
    "\n",
    "# Log the model performance\n",
    "log_model_performance(\n",
    "    model_name=\"SARIMAX\",\n",
    "    model_version=\"Full-Feature\",\n",
    "    features_used=available_features,\n",
    "    parameters={\n",
    "        \"order\": \"(1,1,1)\",\n",
    "        \"seasonal_order\": \"(1,1,1,24)\",\n",
    "        \"enforce_stationarity\": False,\n",
    "        \"enforce_invertibility\": False,\n",
    "        \"maxiter\": 50\n",
    "    },\n",
    "    train_start_date=train_start,\n",
    "    train_end_date=observation_start - pd.Timedelta(seconds=1),\n",
    "    validation_start_date=validation_start,\n",
    "    validation_end_date=validation_end,\n",
    "    forecast_vs_actual=forecast_vs_actual,\n",
    "    daily_rmse=daily_rmse,\n",
    "    run_time_seconds=run_time_seconds,\n",
    "    notes=\"SARIMAX model with all available features\"\n",
    ")\n",
    "\n",
    "# ----- Create interactive Plotly figure without legend -----\n",
    "fig = make_subplots(\n",
    "    rows=1, cols=1,\n",
    "    specs=[[{\"secondary_y\": True}]]\n",
    ")\n",
    "\n",
    "# Add observation data (last week of training)\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=observation_data['datetime'],\n",
    "        y=observation_data['Price'],\n",
    "        mode='lines',\n",
    "        name='Observed Prices (Apr 17-23)',\n",
    "        line=dict(color='royalblue'),\n",
    "        showlegend=False\n",
    "    )\n",
    ")\n",
    "\n",
    "# Add actual validation data\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=validation_data['datetime'],\n",
    "        y=validation_data['Price'],\n",
    "        mode='lines',\n",
    "        name='Actual Prices (Apr 24-30)',\n",
    "        line=dict(color='green'),\n",
    "        showlegend=False\n",
    "    )\n",
    ")\n",
    "\n",
    "# Add SARIMAX forecast\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=sarimax_forecast['datetime'],\n",
    "        y=sarimax_forecast['Price_forecast'],\n",
    "        mode='lines',\n",
    "        name='SARIMAX Forecast (Apr 24-30)',\n",
    "        line=dict(color='red', dash='dash'),\n",
    "        showlegend=False\n",
    "    )\n",
    ")\n",
    "\n",
    "# Add a vertical line using shape\n",
    "fig.add_shape(\n",
    "    type=\"line\",\n",
    "    x0=validation_start.strftime('%Y-%m-%d %H:%M:%S+00:00'),\n",
    "    x1=validation_start.strftime('%Y-%m-%d %H:%M:%S+00:00'),\n",
    "    y0=0,\n",
    "    y1=1,\n",
    "    yref=\"paper\",\n",
    "    line=dict(color=\"black\", width=1, dash=\"dash\"),\n",
    ")\n",
    "\n",
    "# Add annotation for forecast start\n",
    "fig.add_annotation(\n",
    "    x=validation_start.strftime('%Y-%m-%d %H:%M:%S+00:00'),\n",
    "    y=1,\n",
    "    yref=\"paper\",\n",
    "    text=\"Forecast Start\",\n",
    "    showarrow=False,\n",
    "    xanchor=\"left\",\n",
    "    yanchor=\"bottom\"\n",
    ")\n",
    "\n",
    "# Update layout for white background and other styles\n",
    "fig.update_layout(\n",
    "    title='Price Time Series: Observation, Actual, and Full-Feature SARIMAX Forecast',\n",
    "    xaxis_title='Date',\n",
    "    yaxis_title='Price (â‚¬/MWh)',\n",
    "    hovermode='x unified',\n",
    "    plot_bgcolor='white',\n",
    "    paper_bgcolor='white',\n",
    "    font=dict(size=12),\n",
    "    height=600,\n",
    "    width=1000,\n",
    "    margin=dict(l=50, r=50, t=50, b=50),\n",
    "    showlegend=False\n",
    ")\n",
    "\n",
    "# Update x-axis to show all hours\n",
    "fig.update_xaxes(\n",
    "    tickformat='%b %d %H:%M',\n",
    "    tickangle=-45,\n",
    "    tickmode='auto',\n",
    "    nticks=24,\n",
    "    gridcolor='lightgrey'\n",
    ")\n",
    "\n",
    "fig.update_yaxes(gridcolor='lightgrey')\n",
    "\n",
    "# Show the interactive plot\n",
    "fig.show()\n",
    "\n",
    "# ----- Create a table with hourly RMSE -----\n",
    "# Prepare the data for the table\n",
    "hourly_rmse_table = forecast_vs_actual[['datetime', 'Price', 'Price_forecast', 'rmse']].copy()\n",
    "hourly_rmse_table['hour'] = hourly_rmse_table['datetime'].dt.hour\n",
    "hourly_rmse_table['date'] = hourly_rmse_table['datetime'].dt.date\n",
    "hourly_rmse_table['datetime_str'] = hourly_rmse_table['datetime'].dt.strftime('%Y-%m-%d %H:%M')\n",
    "\n",
    "# Create a nicer formatted table for display\n",
    "hourly_rmse_display = hourly_rmse_table[['datetime_str', 'Price', 'Price_forecast', 'rmse']].copy()\n",
    "hourly_rmse_display.columns = ['DateTime', 'Actual Price', 'Forecast Price', 'RMSE']\n",
    "\n",
    "# Print first few rows of the table\n",
    "print(\"\\nHourly RMSE (first 10 rows):\")\n",
    "print(hourly_rmse_display.head(10))\n",
    "\n",
    "# Create a full HTML table for all hours\n",
    "hourly_table = go.Figure(data=[go.Table(\n",
    "    header=dict(\n",
    "        values=list(hourly_rmse_display.columns),\n",
    "        fill_color='royalblue',\n",
    "        align='left',\n",
    "        font=dict(color='white', size=12)\n",
    "    ),\n",
    "    cells=dict(\n",
    "        values=[hourly_rmse_display[col] for col in hourly_rmse_display.columns],\n",
    "        fill_color='lavender',\n",
    "        align='left'\n",
    "    )\n",
    ")])\n",
    "\n",
    "hourly_table.update_layout(\n",
    "    title='Hourly RMSE for All 168 Hours - Full-Feature SARIMAX Model',\n",
    "    height=800,\n",
    "    width=1000\n",
    ")\n",
    "\n",
    "# Show the table\n",
    "hourly_table.show()\n",
    "\n",
    "# Get model comparison\n",
    "comparison_table = compare_models()\n",
    "print(\"\\nModel Comparison:\")\n",
    "print(comparison_table)\n",
    "\n",
    "print(\"\\nFull-Feature SARIMAX model analysis complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b130fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Connect to the SQLite database\n",
    "conn = sqlite3.connect('src/data/WARP.db')\n",
    "\n",
    "# Read the master_warp table into a pandas DataFrame\n",
    "query = \"SELECT * FROM master_warp\"\n",
    "df = pd.read_sql_query(query, conn)\n",
    "\n",
    "# Convert datetime column from string to proper datetime format with UTC timezone\n",
    "df['datetime'] = pd.to_datetime(df['datetime'], utc=True)\n",
    "\n",
    "# Define our time periods\n",
    "# Week for observations (training): Apr 17-23, 2025\n",
    "# Week for validation: Apr 24-30, 2025\n",
    "observation_start = pd.to_datetime('2025-04-17').tz_localize('UTC')\n",
    "validation_start = pd.to_datetime('2025-04-24').tz_localize('UTC')\n",
    "validation_end = pd.to_datetime('2025-04-30 23:00:00').tz_localize('UTC')\n",
    "\n",
    "# Filter the data for each period\n",
    "observation_data = df[(df['datetime'] >= observation_start) & (df['datetime'] < validation_start)]\n",
    "validation_data = df[(df['datetime'] >= validation_start) & (df['datetime'] <= validation_end)]\n",
    "\n",
    "print(f\"Observation data: {len(observation_data)} hours\")\n",
    "print(f\"Validation data: {len(validation_data)} hours\")\n",
    "\n",
    "# Sort the data by datetime\n",
    "observation_data = observation_data.sort_values('datetime')\n",
    "validation_data = validation_data.sort_values('datetime')\n",
    "\n",
    "# ---- Naive Model Implementation ----\n",
    "# Create a dataframe with the same structure as validation_data\n",
    "naive_forecast = pd.DataFrame({'datetime': validation_data['datetime']})\n",
    "naive_forecast['hour'] = naive_forecast['datetime'].dt.hour\n",
    "naive_forecast['day_of_week'] = naive_forecast['datetime'].dt.dayofweek\n",
    "\n",
    "# For each hour in the forecast, take the price from the same hour and day of week in the observation period\n",
    "forecast_prices = []\n",
    "\n",
    "for i, row in naive_forecast.iterrows():\n",
    "    # Find the matching hour and day of week from the observation data\n",
    "    matching_obs = observation_data[(observation_data['hour'] == row['hour']) & \n",
    "                                   (observation_data['day_of_week'] == row['day_of_week'])]\n",
    "    \n",
    "    # If we have a match, use that price; otherwise use the mean price\n",
    "    if not matching_obs.empty:\n",
    "        forecast_prices.append(matching_obs['Price'].values[0])\n",
    "    else:\n",
    "        forecast_prices.append(observation_data['Price'].mean())\n",
    "\n",
    "naive_forecast['Price_forecast'] = forecast_prices\n",
    "\n",
    "# Merge with actual validation data to compare\n",
    "forecast_vs_actual = naive_forecast.merge(\n",
    "    validation_data[['datetime', 'Price']], \n",
    "    on='datetime', \n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# ----- Calculate RMSE -----\n",
    "# Overall RMSE\n",
    "overall_rmse = np.sqrt(mean_squared_error(forecast_vs_actual['Price'], forecast_vs_actual['Price_forecast']))\n",
    "print(f\"\\nOverall RMSE: {overall_rmse:.2f}\")\n",
    "\n",
    "# RMSE per hour (for each of the 168 hours in the validation period)\n",
    "forecast_vs_actual['hour_num'] = range(1, len(forecast_vs_actual) + 1)\n",
    "forecast_vs_actual['squared_error'] = (forecast_vs_actual['Price'] - forecast_vs_actual['Price_forecast']) ** 2\n",
    "forecast_vs_actual['rmse'] = np.sqrt(forecast_vs_actual['squared_error'])\n",
    "\n",
    "# RMSE per day\n",
    "forecast_vs_actual['date'] = forecast_vs_actual['datetime'].dt.date\n",
    "daily_rmse = forecast_vs_actual.groupby('date').apply(\n",
    "    lambda x: np.sqrt(mean_squared_error(x['Price'], x['Price_forecast']))\n",
    ")\n",
    "print(\"\\nRMSE per day:\")\n",
    "print(daily_rmse)\n",
    "\n",
    "# ----- Create interactive Plotly figure without legend -----\n",
    "fig = make_subplots(\n",
    "    rows=1, cols=1,\n",
    "    specs=[[{\"secondary_y\": True}]]\n",
    ")\n",
    "\n",
    "# Add observation data\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=observation_data['datetime'],\n",
    "        y=observation_data['Price'],\n",
    "        mode='lines',\n",
    "        name='Observed Prices (Apr 17-23)',\n",
    "        line=dict(color='royalblue'),\n",
    "        showlegend=False  # Hide legend\n",
    "    )\n",
    ")\n",
    "\n",
    "# Add actual validation data\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=validation_data['datetime'],\n",
    "        y=validation_data['Price'],\n",
    "        mode='lines',\n",
    "        name='Actual Prices (Apr 24-30)',\n",
    "        line=dict(color='green'),\n",
    "        showlegend=False  # Hide legend\n",
    "    )\n",
    ")\n",
    "\n",
    "# Add naive forecast\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=naive_forecast['datetime'],\n",
    "        y=naive_forecast['Price_forecast'],\n",
    "        mode='lines',\n",
    "        name='Naive Forecast (Apr 24-30)',\n",
    "        line=dict(color='red', dash='dash'),\n",
    "        showlegend=False  # Hide legend\n",
    "    )\n",
    ")\n",
    "\n",
    "# Add a vertical line using shape\n",
    "fig.add_shape(\n",
    "    type=\"line\",\n",
    "    x0=validation_start.strftime('%Y-%m-%d %H:%M:%S+00:00'),\n",
    "    x1=validation_start.strftime('%Y-%m-%d %H:%M:%S+00:00'),\n",
    "    y0=0,\n",
    "    y1=1,\n",
    "    yref=\"paper\",\n",
    "    line=dict(color=\"black\", width=1, dash=\"dash\"),\n",
    ")\n",
    "\n",
    "# Add annotation for forecast start\n",
    "fig.add_annotation(\n",
    "    x=validation_start.strftime('%Y-%m-%d %H:%M:%S+00:00'),\n",
    "    y=1,\n",
    "    yref=\"paper\",\n",
    "    text=\"Forecast Start\",\n",
    "    showarrow=False,\n",
    "    xanchor=\"left\",\n",
    "    yanchor=\"bottom\"\n",
    ")\n",
    "\n",
    "# Update layout for white background and other styles\n",
    "fig.update_layout(\n",
    "    title='Price Time Series: Observation, Actual, and Naive Forecast',\n",
    "    xaxis_title='Date',\n",
    "    yaxis_title='Price (â‚¬/MWh)',\n",
    "    hovermode='x unified',\n",
    "    plot_bgcolor='white',\n",
    "    paper_bgcolor='white',\n",
    "    font=dict(size=12),\n",
    "    height=600,\n",
    "    width=1000,\n",
    "    margin=dict(l=50, r=50, t=50, b=50),\n",
    "    showlegend=False  # Ensure legend is hidden\n",
    ")\n",
    "\n",
    "# Update x-axis to show all hours\n",
    "fig.update_xaxes(\n",
    "    tickformat='%b %d %H:%M',\n",
    "    tickangle=-45,\n",
    "    tickmode='auto',\n",
    "    nticks=24,  # Show approximately 24 tick marks on the x-axis\n",
    "    gridcolor='lightgrey'\n",
    ")\n",
    "\n",
    "fig.update_yaxes(gridcolor='lightgrey')\n",
    "\n",
    "# Show the interactive plot\n",
    "fig.show()\n",
    "\n",
    "# Save the plotly figure as HTML for interactive viewing\n",
    "fig.write_html(\"naive_forecast_interactive.html\")\n",
    "\n",
    "# ----- Create a table with hourly RMSE -----\n",
    "# Prepare the data for the table\n",
    "hourly_rmse_table = forecast_vs_actual[['datetime', 'Price', 'Price_forecast', 'rmse']].copy()\n",
    "hourly_rmse_table['hour'] = hourly_rmse_table['datetime'].dt.hour\n",
    "hourly_rmse_table['date'] = hourly_rmse_table['datetime'].dt.date\n",
    "hourly_rmse_table['datetime_str'] = hourly_rmse_table['datetime'].dt.strftime('%Y-%m-%d %H:%M')\n",
    "\n",
    "# Create a nicer formatted table for display\n",
    "hourly_rmse_display = hourly_rmse_table[['datetime_str', 'Price', 'Price_forecast', 'rmse']].copy()\n",
    "hourly_rmse_display.columns = ['DateTime', 'Actual Price', 'Forecast Price', 'RMSE']\n",
    "\n",
    "# Print first few rows of the table\n",
    "print(\"\\nHourly RMSE (first 10 rows):\")\n",
    "print(hourly_rmse_display.head(10))\n",
    "\n",
    "# Create a full HTML table for all hours\n",
    "hourly_table = go.Figure(data=[go.Table(\n",
    "    header=dict(\n",
    "        values=list(hourly_rmse_display.columns),\n",
    "        fill_color='royalblue',\n",
    "        align='left',\n",
    "        font=dict(color='white', size=12)\n",
    "    ),\n",
    "    cells=dict(\n",
    "        values=[hourly_rmse_display[col] for col in hourly_rmse_display.columns],\n",
    "        fill_color='lavender',\n",
    "        align='left'\n",
    "    )\n",
    ")])\n",
    "\n",
    "hourly_table.update_layout(\n",
    "    title='Hourly RMSE for All 168 Hours',\n",
    "    height=800,  # Taller to show more rows\n",
    "    width=1000\n",
    ")\n",
    "\n",
    "# Show the table\n",
    "hourly_table.show()\n",
    "\n",
    "# Save the hourly RMSE table to HTML\n",
    "hourly_table.write_html(\"hourly_rmse_table.html\")\n",
    "\n",
    "# Save the forecast and validation results to CSV\n",
    "forecast_vs_actual[['datetime', 'Price', 'Price_forecast', 'rmse']].to_csv('naive_forecast_validation.csv', index=False)\n",
    "print(\"\\nValidation results saved to 'naive_forecast_validation.csv'\")\n",
    "print(\"Interactive plot saved to 'naive_forecast_interactive.html'\")\n",
    "print(\"Hourly RMSE table saved to 'hourly_rmse_table.html'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea6cd729",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "import warnings\n",
    "import time\n",
    "import json\n",
    "import datetime\n",
    "import os\n",
    "from pathlib import Path\n",
    "warnings.filterwarnings('ignore')  # Suppress warnings for cleaner output\n",
    "\n",
    "# Function to initialize the log database\n",
    "def initialize_log_database(db_path='src/data/logs.db'):\n",
    "    \"\"\"Create the model_performance_log table if it doesn't exist\"\"\"\n",
    "    \n",
    "    # Ensure the directory exists\n",
    "    Path(os.path.dirname(db_path)).mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Connect to the database\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    # Create the model performance log table if it doesn't exist\n",
    "    cursor.execute('''\n",
    "    CREATE TABLE IF NOT EXISTS model_performance_log (\n",
    "        id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "        timestamp TEXT,\n",
    "        model_name TEXT,\n",
    "        model_version TEXT,\n",
    "        features_used TEXT,\n",
    "        parameters TEXT,\n",
    "        train_start_date TEXT,\n",
    "        train_end_date TEXT,\n",
    "        validation_start_date TEXT,\n",
    "        validation_end_date TEXT,\n",
    "        overall_rmse REAL,\n",
    "        min_daily_rmse REAL,\n",
    "        max_daily_rmse REAL,\n",
    "        avg_daily_rmse REAL,\n",
    "        hourly_rmse_distribution TEXT,\n",
    "        run_time_seconds REAL,\n",
    "        notes TEXT\n",
    "    )\n",
    "    ''')\n",
    "    \n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "    \n",
    "    print(f\"Log database initialized at {db_path}\")\n",
    "    return db_path\n",
    "\n",
    "# Function to log model performance\n",
    "def log_model_performance(\n",
    "    model_name,\n",
    "    model_version,\n",
    "    features_used,\n",
    "    parameters,\n",
    "    train_start_date,\n",
    "    train_end_date,\n",
    "    validation_start_date,\n",
    "    validation_end_date,\n",
    "    forecast_vs_actual,\n",
    "    daily_rmse,\n",
    "    run_time_seconds,\n",
    "    notes=\"\",\n",
    "    db_path='src/data/logs.db'\n",
    "):\n",
    "    \"\"\"Log the performance of a model run to the database\"\"\"\n",
    "    \n",
    "    # Calculate performance metrics\n",
    "    overall_rmse = np.sqrt(mean_squared_error(\n",
    "        forecast_vs_actual['Price'], \n",
    "        forecast_vs_actual['Price_forecast']\n",
    "    ))\n",
    "    \n",
    "    min_daily_rmse = daily_rmse.min()\n",
    "    max_daily_rmse = daily_rmse.max()\n",
    "    avg_daily_rmse = daily_rmse.mean()\n",
    "    \n",
    "    # Get hourly RMSE distribution statistics\n",
    "    hourly_rmse_stats = {\n",
    "        'min': float(forecast_vs_actual['rmse'].min()),\n",
    "        'max': float(forecast_vs_actual['rmse'].max()),\n",
    "        'mean': float(forecast_vs_actual['rmse'].mean()),\n",
    "        'median': float(forecast_vs_actual['rmse'].median()),\n",
    "        'q25': float(forecast_vs_actual['rmse'].quantile(0.25)),\n",
    "        'q75': float(forecast_vs_actual['rmse'].quantile(0.75)),\n",
    "        'std': float(forecast_vs_actual['rmse'].std())\n",
    "    }\n",
    "    \n",
    "    # Connect to the database\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    # Insert the log entry\n",
    "    cursor.execute('''\n",
    "    INSERT INTO model_performance_log (\n",
    "        timestamp,\n",
    "        model_name,\n",
    "        model_version,\n",
    "        features_used,\n",
    "        parameters,\n",
    "        train_start_date,\n",
    "        train_end_date,\n",
    "        validation_start_date,\n",
    "        validation_end_date,\n",
    "        overall_rmse,\n",
    "        min_daily_rmse,\n",
    "        max_daily_rmse,\n",
    "        avg_daily_rmse,\n",
    "        hourly_rmse_distribution,\n",
    "        run_time_seconds,\n",
    "        notes\n",
    "    ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\n",
    "    ''', (\n",
    "        datetime.datetime.now().isoformat(),\n",
    "        model_name,\n",
    "        model_version,\n",
    "        json.dumps(features_used),\n",
    "        json.dumps(parameters),\n",
    "        train_start_date.isoformat(),\n",
    "        train_end_date.isoformat(),\n",
    "        validation_start_date.isoformat(),\n",
    "        validation_end_date.isoformat(),\n",
    "        overall_rmse,\n",
    "        min_daily_rmse,\n",
    "        max_daily_rmse,\n",
    "        avg_daily_rmse,\n",
    "        json.dumps(hourly_rmse_stats),\n",
    "        run_time_seconds,\n",
    "        notes\n",
    "    ))\n",
    "    \n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "    \n",
    "    print(f\"Model performance logged to {db_path}\")\n",
    "    return True\n",
    "\n",
    "# Function to get the logged model performances\n",
    "def get_model_performances(db_path='src/data/logs.db'):\n",
    "    \"\"\"Retrieve all model performance logs as a DataFrame\"\"\"\n",
    "    \n",
    "    conn = sqlite3.connect(db_path)\n",
    "    \n",
    "    # Query the database\n",
    "    query = \"SELECT * FROM model_performance_log ORDER BY timestamp DESC\"\n",
    "    logs_df = pd.read_sql_query(query, conn)\n",
    "    \n",
    "    conn.close()\n",
    "    \n",
    "    return logs_df\n",
    "\n",
    "# Function to create a comparison table of model performances\n",
    "def compare_models(db_path='src/data/logs.db', latest_only=True):\n",
    "    \"\"\"Create a comparison table of model performances\"\"\"\n",
    "    \n",
    "    logs_df = get_model_performances(db_path)\n",
    "    \n",
    "    if logs_df.empty:\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # If latest_only is True, get only the latest run for each model type\n",
    "    if latest_only:\n",
    "        # Get latest run for each combination of model_name and model_version\n",
    "        logs_df['timestamp'] = pd.to_datetime(logs_df['timestamp'])\n",
    "        idx = logs_df.groupby(['model_name', 'model_version'])['timestamp'].idxmax()\n",
    "        logs_df = logs_df.loc[idx]\n",
    "    \n",
    "    # Select relevant columns for comparison\n",
    "    comparison_df = logs_df[[\n",
    "        'model_name', \n",
    "        'model_version', \n",
    "        'overall_rmse', \n",
    "        'avg_daily_rmse',\n",
    "        'min_daily_rmse',\n",
    "        'max_daily_rmse',\n",
    "        'run_time_seconds',\n",
    "        'timestamp'\n",
    "    ]].copy()\n",
    "    \n",
    "    # Sort by overall RMSE (best performing models first)\n",
    "    comparison_df = comparison_df.sort_values('overall_rmse')\n",
    "    \n",
    "    return comparison_df\n",
    "\n",
    "# Initialize the log database\n",
    "db_path = initialize_log_database()\n",
    "\n",
    "# Start timing the model run\n",
    "start_time = time.time()\n",
    "\n",
    "# Connect to the SQLite database\n",
    "conn = sqlite3.connect('src/data/WARP.db')\n",
    "\n",
    "# Read the master_warp table into a pandas DataFrame\n",
    "query = \"SELECT * FROM master_warp\"\n",
    "df = pd.read_sql_query(query, conn)\n",
    "\n",
    "# Print column names to check availability\n",
    "print(\"Available columns in the dataset:\")\n",
    "print(df.columns.tolist())\n",
    "\n",
    "# Convert datetime column from string to proper datetime format with UTC timezone\n",
    "df['datetime'] = pd.to_datetime(df['datetime'], utc=True)\n",
    "\n",
    "# Define data ranges for SARIMAX model\n",
    "train_start = pd.to_datetime('2025-01-01').tz_localize('UTC')\n",
    "observation_start = pd.to_datetime('2025-04-17').tz_localize('UTC')\n",
    "validation_start = pd.to_datetime('2025-04-24').tz_localize('UTC')\n",
    "validation_end = pd.to_datetime('2025-04-30 23:00:00').tz_localize('UTC')\n",
    "\n",
    "# Filter the data for each period\n",
    "train_data = df[(df['datetime'] >= train_start) & (df['datetime'] < observation_start)]\n",
    "observation_data = df[(df['datetime'] >= observation_start) & (df['datetime'] < validation_start)]\n",
    "validation_data = df[(df['datetime'] >= validation_start) & (df['datetime'] <= validation_end)]\n",
    "\n",
    "print(f\"Training data: {len(train_data)} hours\")\n",
    "print(f\"Observation data: {len(observation_data)} hours\")\n",
    "print(f\"Validation data: {len(validation_data)} hours\")\n",
    "\n",
    "# Sort the data by datetime\n",
    "train_data = train_data.sort_values('datetime')\n",
    "observation_data = observation_data.sort_values('datetime')\n",
    "validation_data = validation_data.sort_values('datetime')\n",
    "\n",
    "# ---- SARIMAX Model Implementation with Available Features ----\n",
    "# Combine train and observation data for model fitting\n",
    "model_data = pd.concat([train_data, observation_data])\n",
    "model_data = model_data.sort_values('datetime')\n",
    "\n",
    "# Define desired features\n",
    "desired_features = [\n",
    "    # Weather and related\n",
    "    'temperature_2m', 'Total_Flow', 'ned.volume',\n",
    "    \n",
    "    # Check if we have these features\n",
    "    'wind_speed_10m', 'apparent_temperature', 'cloud_cover',\n",
    "    'snowfall', 'diffuse_radiation', 'direct_normal_irradiance', \n",
    "    'shortwave_radiation', 'ned.capacity', 'ned.percentage', 'Load'\n",
    "]\n",
    "\n",
    "# Check which features are available\n",
    "available_features = ['datetime', 'Price']\n",
    "for feature in desired_features:\n",
    "    if feature in model_data.columns:\n",
    "        available_features.append(feature)\n",
    "\n",
    "print(f\"Using {len(available_features) - 2} features in the model:\")  # -2 for datetime and Price\n",
    "print(available_features[2:])  # Skip datetime and Price\n",
    "\n",
    "# Set the datetime as index for the time series analysis\n",
    "model_data_ts = model_data[available_features].set_index('datetime')\n",
    "\n",
    "# Add basic time features\n",
    "model_data_ts['hour'] = model_data_ts.index.hour\n",
    "model_data_ts['day_of_week'] = model_data_ts.index.dayofweek\n",
    "model_data_ts['is_weekend'] = (model_data_ts.index.dayofweek >= 5).astype(int)\n",
    "\n",
    "# Add these time features to our available features list\n",
    "time_features = ['hour', 'day_of_week', 'is_weekend']\n",
    "all_exog_features = available_features[2:] + time_features  # Skip datetime and Price\n",
    "\n",
    "# Define features to scale (all except binary indicators)\n",
    "scale_features = [f for f in all_exog_features if f not in ['is_weekend']]\n",
    "\n",
    "# Initialize the scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Scale the features\n",
    "model_data_ts[scale_features] = scaler.fit_transform(model_data_ts[scale_features])\n",
    "\n",
    "# For this enhanced model, we'll use time features + scaled available variables\n",
    "exog = model_data_ts[all_exog_features]\n",
    "\n",
    "# Fit SARIMAX model\n",
    "print(\"Fitting SARIMAX model with available features...\")\n",
    "model = SARIMAX(\n",
    "    model_data_ts['Price'],\n",
    "    exog=exog,\n",
    "    order=(1, 1, 1),          # (p,d,q) - simple parameters\n",
    "    seasonal_order=(1, 1, 1, 24),  # (P,D,Q,s) - 24 for hourly data with daily seasonality\n",
    "    enforce_stationarity=False,\n",
    "    enforce_invertibility=False\n",
    ")\n",
    "\n",
    "# Fit the model\n",
    "results = model.fit(disp=False)\n",
    "print(\"Model fitted successfully.\")\n",
    "\n",
    "# Prepare exogenous variables for forecasting the validation period\n",
    "validation_exog = validation_data[['datetime'] + available_features[2:]].copy()\n",
    "validation_exog['hour'] = validation_exog['datetime'].dt.hour\n",
    "validation_exog['day_of_week'] = validation_exog['datetime'].dt.dayofweek\n",
    "validation_exog['is_weekend'] = (validation_exog['datetime'].dt.dayofweek >= 5).astype(int)\n",
    "\n",
    "# Scale the validation exogenous variables using the same scaler\n",
    "validation_exog[scale_features] = scaler.transform(validation_exog[scale_features])\n",
    "\n",
    "# Set the datetime as index\n",
    "validation_exog = validation_exog.set_index('datetime')[all_exog_features]\n",
    "\n",
    "# Forecast the validation period\n",
    "print(\"Forecasting validation period...\")\n",
    "forecast = results.get_forecast(steps=len(validation_exog), exog=validation_exog)\n",
    "forecast_mean = forecast.predicted_mean\n",
    "forecast_ci = forecast.conf_int()\n",
    "\n",
    "# Create a dataframe with the forecast\n",
    "sarimax_forecast = pd.DataFrame({\n",
    "    'datetime': validation_data['datetime'],\n",
    "    'Price_forecast': forecast_mean.values\n",
    "})\n",
    "\n",
    "# Merge with actual validation data to compare\n",
    "forecast_vs_actual = sarimax_forecast.merge(\n",
    "    validation_data[['datetime', 'Price']], \n",
    "    on='datetime', \n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# ----- Calculate RMSE -----\n",
    "# Overall RMSE\n",
    "overall_rmse = np.sqrt(mean_squared_error(forecast_vs_actual['Price'], forecast_vs_actual['Price_forecast']))\n",
    "print(f\"\\nOverall RMSE: {overall_rmse:.2f}\")\n",
    "\n",
    "# RMSE per hour (for each of the 168 hours in the validation period)\n",
    "forecast_vs_actual['hour_num'] = range(1, len(forecast_vs_actual) + 1)\n",
    "forecast_vs_actual['squared_error'] = (forecast_vs_actual['Price'] - forecast_vs_actual['Price_forecast']) ** 2\n",
    "forecast_vs_actual['rmse'] = np.sqrt(forecast_vs_actual['squared_error'])\n",
    "\n",
    "# RMSE per day\n",
    "forecast_vs_actual['date'] = forecast_vs_actual['datetime'].dt.date\n",
    "daily_rmse = forecast_vs_actual.groupby('date').apply(\n",
    "    lambda x: np.sqrt(mean_squared_error(x['Price'], x['Price_forecast']))\n",
    ")\n",
    "print(\"\\nRMSE per day:\")\n",
    "print(daily_rmse)\n",
    "\n",
    "# Calculate run time\n",
    "run_time_seconds = time.time() - start_time\n",
    "\n",
    "# Log the model performance\n",
    "log_model_performance(\n",
    "    model_name=\"SARIMAX\",\n",
    "    model_version=\"Available-Features\",\n",
    "    features_used=all_exog_features,\n",
    "    parameters={\n",
    "        \"order\": \"(1,1,1)\",\n",
    "        \"seasonal_order\": \"(1,1,1,24)\",\n",
    "        \"enforce_stationarity\": False,\n",
    "        \"enforce_invertibility\": False\n",
    "    },\n",
    "    train_start_date=train_start,\n",
    "    train_end_date=observation_start - pd.Timedelta(seconds=1),\n",
    "    validation_start_date=validation_start,\n",
    "    validation_end_date=validation_end,\n",
    "    forecast_vs_actual=forecast_vs_actual,\n",
    "    daily_rmse=daily_rmse,\n",
    "    run_time_seconds=run_time_seconds,\n",
    "    notes=f\"SARIMAX model with available features: {', '.join(all_exog_features)}\"\n",
    ")\n",
    "\n",
    "# ----- Create interactive Plotly figure without legend -----\n",
    "fig = make_subplots(\n",
    "    rows=1, cols=1,\n",
    "    specs=[[{\"secondary_y\": True}]]\n",
    ")\n",
    "\n",
    "# Add observation data (last week of training)\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=observation_data['datetime'],\n",
    "        y=observation_data['Price'],\n",
    "        mode='lines',\n",
    "        name='Observed Prices (Apr 17-23)',\n",
    "        line=dict(color='royalblue'),\n",
    "        showlegend=False\n",
    "    )\n",
    ")\n",
    "\n",
    "# Add actual validation data\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=validation_data['datetime'],\n",
    "        y=validation_data['Price'],\n",
    "        mode='lines',\n",
    "        name='Actual Prices (Apr 24-30)',\n",
    "        line=dict(color='green'),\n",
    "        showlegend=False\n",
    "    )\n",
    ")\n",
    "\n",
    "# Add SARIMAX forecast\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=sarimax_forecast['datetime'],\n",
    "        y=sarimax_forecast['Price_forecast'],\n",
    "        mode='lines',\n",
    "        name='SARIMAX Forecast (Apr 24-30)',\n",
    "        line=dict(color='red', dash='dash'),\n",
    "        showlegend=False\n",
    "    )\n",
    ")\n",
    "\n",
    "# Add a vertical line using shape\n",
    "fig.add_shape(\n",
    "    type=\"line\",\n",
    "    x0=validation_start.strftime('%Y-%m-%d %H:%M:%S+00:00'),\n",
    "    x1=validation_start.strftime('%Y-%m-%d %H:%M:%S+00:00'),\n",
    "    y0=0,\n",
    "    y1=1,\n",
    "    yref=\"paper\",\n",
    "    line=dict(color=\"black\", width=1, dash=\"dash\"),\n",
    ")\n",
    "\n",
    "# Add annotation for forecast start\n",
    "fig.add_annotation(\n",
    "    x=validation_start.strftime('%Y-%m-%d %H:%M:%S+00:00'),\n",
    "    y=1,\n",
    "    yref=\"paper\",\n",
    "    text=\"Forecast Start\",\n",
    "    showarrow=False,\n",
    "    xanchor=\"left\",\n",
    "    yanchor=\"bottom\"\n",
    ")\n",
    "\n",
    "# Update layout for white background and other styles\n",
    "fig.update_layout(\n",
    "    title='Price Time Series: Observation, Actual, and SARIMAX Forecast',\n",
    "    xaxis_title='Date',\n",
    "    yaxis_title='Price (â‚¬/MWh)',\n",
    "    hovermode='x unified',\n",
    "    plot_bgcolor='white',\n",
    "    paper_bgcolor='white',\n",
    "    font=dict(size=12),\n",
    "    height=600,\n",
    "    width=1000,\n",
    "    margin=dict(l=50, r=50, t=50, b=50),\n",
    "    showlegend=False\n",
    ")\n",
    "\n",
    "# Update x-axis to show all hours\n",
    "fig.update_xaxes(\n",
    "    tickformat='%b %d %H:%M',\n",
    "    tickangle=-45,\n",
    "    tickmode='auto',\n",
    "    nticks=24,\n",
    "    gridcolor='lightgrey'\n",
    ")\n",
    "\n",
    "fig.update_yaxes(gridcolor='lightgrey')\n",
    "\n",
    "# Show the interactive plot\n",
    "fig.show()\n",
    "\n",
    "# ----- Create a table with hourly RMSE -----\n",
    "# Prepare the data for the table\n",
    "hourly_rmse_table = forecast_vs_actual[['datetime', 'Price', 'Price_forecast', 'rmse']].copy()\n",
    "hourly_rmse_table['hour'] = hourly_rmse_table['datetime'].dt.hour\n",
    "hourly_rmse_table['date'] = hourly_rmse_table['datetime'].dt.date\n",
    "hourly_rmse_table['datetime_str'] = hourly_rmse_table['datetime'].dt.strftime('%Y-%m-%d %H:%M')\n",
    "\n",
    "# Create a nicer formatted table for display\n",
    "hourly_rmse_display = hourly_rmse_table[['datetime_str', 'Price', 'Price_forecast', 'rmse']].copy()\n",
    "hourly_rmse_display.columns = ['DateTime', 'Actual Price', 'Forecast Price', 'RMSE']\n",
    "\n",
    "# Print first few rows of the table\n",
    "print(\"\\nHourly RMSE (first 10 rows):\")\n",
    "print(hourly_rmse_display.head(10))\n",
    "\n",
    "# Create a full HTML table for all hours\n",
    "hourly_table = go.Figure(data=[go.Table(\n",
    "    header=dict(\n",
    "        values=list(hourly_rmse_display.columns),\n",
    "        fill_color='royalblue',\n",
    "        align='left',\n",
    "        font=dict(color='white', size=12)\n",
    "    ),\n",
    "    cells=dict(\n",
    "        values=[hourly_rmse_display[col] for col in hourly_rmse_display.columns],\n",
    "        fill_color='lavender',\n",
    "        align='left'\n",
    "    )\n",
    ")])\n",
    "\n",
    "hourly_table.update_layout(\n",
    "    title='Hourly RMSE for All 168 Hours - SARIMAX Model',\n",
    "    height=800,\n",
    "    width=1000\n",
    ")\n",
    "\n",
    "# Show the table\n",
    "hourly_table.show()\n",
    "\n",
    "# Try to get feature importance information\n",
    "try:\n",
    "    # Get the feature importance from the SARIMAX model coefficients\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'Feature': all_exog_features,\n",
    "        'Coefficient': results.params[-len(all_exog_features):]\n",
    "    })\n",
    "    \n",
    "    # Sort by absolute value of coefficients\n",
    "    feature_importance['Abs_Coefficient'] = feature_importance['Coefficient'].abs()\n",
    "    feature_importance = feature_importance.sort_values('Abs_Coefficient', ascending=False)\n",
    "    \n",
    "    print(\"\\nFeature Importance (top features):\")\n",
    "    print(feature_importance.head(10))\n",
    "except Exception as e:\n",
    "    print(f\"\\nCould not display feature importance: {e}\")\n",
    "\n",
    "# Get model comparison\n",
    "comparison_table = compare_models()\n",
    "print(\"\\nModel Comparison:\")\n",
    "print(comparison_table)\n",
    "\n",
    "print(\"\\nSARIMAX model analysis complete.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
