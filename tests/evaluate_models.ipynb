{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time Series Forecasting - Refactored Architecture\n",
    "\n",
    "This notebook demonstrates the new modular architecture for time series forecasting experiments.\n",
    "\n",
    "## Features:\n",
    "- 🔧 Configuration-driven experiments\n",
    "- 📊 Unified logging and metrics\n",
    "- 🎨 Interactive visualizations\n",
    "- 🔄 Rolling window validation\n",
    "- ⚡ Parallel model execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-28 11:18:06,108 - build_training_set - INFO - 🚀 Start build van trainingset\n",
      "2025-05-28 11:18:06,109 - build_training_set - INFO - 🧠 Actuals van 2025-01-01 00:00:00+00:00 t/m 2025-03-14 23:00:00+00:00\n",
      "2025-05-28 11:18:06,110 - build_training_set - INFO - 📅 Forecast van run_date 2025-03-15 12:00:00+00:00, target range: 2025-03-15 12:00:00+00:00 → 2025-03-22 11:00:00+00:00\n",
      "2025-05-28 11:18:06,113 - build_training_set - INFO - 📥 Loading actuals with selected columns only...\n",
      "2025-05-28 11:18:06,115 - build_training_set - INFO - 📋 Requested columns found: 20/20\n",
      "2025-05-28 11:18:06,115 - build_training_set - INFO - 📋 Using columns: ['Price', 'target_datetime', 'Load', 'shortwave_radiation', 'temperature_2m', 'direct_normal_irradiance', 'diffuse_radiation', 'Flow_NO', 'yearday_cos', 'Flow_GB', 'month', 'is_dst', 'yearday_sin', 'is_non_working_day', 'hour_cos', 'is_weekend', 'cloud_cover', 'weekday_sin', 'hour_sin', 'weekday_cos']\n",
      "2025-05-28 11:18:06,141 - build_training_set - INFO - ✅ Actuals loaded: 1752 rows with 20 selected columns\n",
      "2025-05-28 11:18:06,142 - build_training_set - INFO - 🔍 Checking for forecast data...\n",
      "2025-05-28 11:18:06,152 - build_training_set - INFO - 📊 Forecast rows available: 0\n",
      "2025-05-28 11:18:06,155 - build_training_set - INFO - 📦 Final table: 1752 rows, 20 columns\n",
      "2025-05-28 11:18:06,156 - build_training_set - INFO - 🧾 Final columns: ['Price', 'target_datetime', 'Load', 'shortwave_radiation', 'temperature_2m', 'direct_normal_irradiance', 'diffuse_radiation', 'Flow_NO', 'yearday_cos', 'Flow_GB', 'month', 'is_dst', 'yearday_sin', 'is_non_working_day', 'hour_cos', 'is_weekend', 'cloud_cover', 'weekday_sin', 'hour_sin', 'weekday_cos']\n",
      "2025-05-28 11:18:06,159 - build_training_set - INFO - 💰 Price NaN count: 0/1752 (0.0%)\n",
      "2025-05-28 11:18:06,162 - build_training_set - INFO - ✅ All columns have good data quality (<20% NaN)\n",
      "2025-05-28 11:18:06,182 - build_training_set - INFO - ✅ Saved as training_set in WARP.db\n",
      "2025-05-28 11:18:06,184 - build_training_set - INFO - 🔒 Connection closed\n",
      "2025-05-28 11:18:06,188 - __main__ - INFO - ✅ Data loading complete\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ All required columns present\n",
      "📊 Dataset shape: (1752, 19)\n",
      "📅 Date range: 2025-01-01 00:00:00+00:00 to 2025-03-14 23:00:00+00:00\n",
      "💰 Price range: -0.0204 to 0.5235\n",
      "🔍 Data quality: 100.0% complete\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Cell 1: Setup & Data Loading\n",
    "# ============================================================================\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import logging\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Setup paths\n",
    "current_dir = Path.cwd()\n",
    "if \"ENEXIS\" in str(current_dir):\n",
    "    while current_dir.name != \"ENEXIS\" and current_dir.parent != current_dir:\n",
    "        current_dir = current_dir.parent\n",
    "    project_root = current_dir\n",
    "else:\n",
    "    project_root = current_dir\n",
    "\n",
    "src_path = project_root / \"src\"\n",
    "if str(src_path) not in sys.path:\n",
    "    sys.path.insert(0, str(src_path))\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Load training data\n",
    "from utils.build_training_set import build_training_set\n",
    "\n",
    "training_data = build_training_set(\n",
    "    train_start=\"2025-01-01 00:00:00\",\n",
    "    train_end=\"2025-03-14 23:00:00\",\n",
    "    run_date=\"2025-03-15 12:00:00\"\n",
    ")\n",
    "\n",
    "if training_data is not None:\n",
    "    # Set target_datetime as index for time series analysis\n",
    "    training_data = training_data.set_index('target_datetime')\n",
    "    training_data.index = pd.to_datetime(training_data.index, utc=True)\n",
    "    \n",
    "    # Ensure we have all required columns\n",
    "    required_columns = ['Price', 'Load', 'shortwave_radiation', 'temperature_2m', \n",
    "                       'direct_normal_irradiance', 'diffuse_radiation', 'Flow_NO', \n",
    "                       'yearday_cos', 'Flow_GB', 'month', 'is_dst', 'yearday_sin', \n",
    "                       'is_non_working_day', 'hour_cos', 'is_weekend', 'cloud_cover', \n",
    "                       'weekday_sin', 'hour_sin', 'weekday_cos']\n",
    "    \n",
    "    missing_cols = [col for col in required_columns if col not in training_data.columns]\n",
    "    if missing_cols:\n",
    "        print(f\"⚠️ Missing required columns: {missing_cols}\")\n",
    "    else:\n",
    "        print(\"✅ All required columns present\")\n",
    "    \n",
    "    print(f\"📊 Dataset shape: {training_data.shape}\")\n",
    "    print(f\"📅 Date range: {training_data.index.min()} to {training_data.index.max()}\")\n",
    "    print(f\"💰 Price range: {training_data['Price'].min():.4f} to {training_data['Price'].max():.4f}\")\n",
    "    print(f\"🔍 Data quality: {(1 - training_data['Price'].isna().sum() / len(training_data)) * 100:.1f}% complete\")\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "else:\n",
    "    raise Exception(\"❌ Failed to load training data\")\n",
    "\n",
    "logger.info(\"✅ Data loading complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-28 11:18:07,123 - __main__ - INFO - ✅ Model configuration complete\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🤖 MODEL CONFIGURATIONS:\n",
      "  Naive: Previous day same hour forecast\n",
      "  SARIMA: Simple ARIMA model (fallback to moving average if needed)\n",
      "  SARIMAX: SARIMA with exogenous variables\n",
      "    Exogenous variables: 18/18 available\n",
      "✅ All exogenous variables available\n",
      "\n",
      "📋 Exogenous variables for SARIMAX:\n",
      "   1. Load\n",
      "   2. shortwave_radiation\n",
      "   3. temperature_2m\n",
      "   4. direct_normal_irradiance\n",
      "   5. diffuse_radiation\n",
      "   6. Flow_NO\n",
      "   7. yearday_cos\n",
      "   8. Flow_GB\n",
      "   9. month\n",
      "  10. is_dst\n",
      "  11. yearday_sin\n",
      "  12. is_non_working_day\n",
      "  13. hour_cos\n",
      "  14. is_weekend\n",
      "  15. cloud_cover\n",
      "  16. weekday_sin\n",
      "  17. hour_sin\n",
      "  18. weekday_cos\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Cell 2: Model Configuration\n",
    "# ============================================================================\n",
    "\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Define exogenous variables for SARIMAX\n",
    "EXOG_VARS = [\n",
    "    'Load', 'shortwave_radiation', 'temperature_2m', 'direct_normal_irradiance', \n",
    "    'diffuse_radiation', 'Flow_NO', 'yearday_cos', 'Flow_GB', 'month', 'is_dst', \n",
    "    'yearday_sin', 'is_non_working_day', 'hour_cos', 'is_weekend', 'cloud_cover', \n",
    "    'weekday_sin', 'hour_sin', 'weekday_cos'\n",
    "]\n",
    "\n",
    "# Model configurations\n",
    "MODEL_CONFIGS = {\n",
    "    'Naive': {\n",
    "        'description': 'Previous day same hour forecast',\n",
    "        'params': {'lag': 24}  # 24 hours = previous day same hour\n",
    "    },\n",
    "    'SARIMA': {\n",
    "        'description': 'Simple ARIMA model (fallback to moving average if needed)',\n",
    "        'params': {\n",
    "            'order': (1, 0, 0),           # Simple AR(1) model\n",
    "            'seasonal_order': None        # No seasonal component for robustness\n",
    "        }\n",
    "    },\n",
    "    'SARIMAX': {\n",
    "        'description': 'SARIMA with exogenous variables',\n",
    "        'params': {\n",
    "            'order': (1, 1, 1),           # (p, d, q)\n",
    "            'seasonal_order': (1, 1, 1, 24), # (P, D, Q, s)\n",
    "            'exog_vars': EXOG_VARS\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "def create_naive_model():\n",
    "    \"\"\"Create naive forecasting function\"\"\"\n",
    "    def forecast_naive(train_data, forecast_steps=1):\n",
    "        # Use same hour from previous day (24 hours ago)\n",
    "        if len(train_data) >= 24:\n",
    "            return [train_data.iloc[-24]] * forecast_steps\n",
    "        else:\n",
    "            return [train_data.iloc[-1]] * forecast_steps\n",
    "    return forecast_naive\n",
    "\n",
    "def create_sarima_model(train_data):\n",
    "    \"\"\"Create and fit SARIMA model\"\"\"\n",
    "    try:\n",
    "        model = ARIMA(\n",
    "            train_data, \n",
    "            order=MODEL_CONFIGS['SARIMA']['params']['order'],\n",
    "            seasonal_order=MODEL_CONFIGS['SARIMA']['params']['seasonal_order']\n",
    "        )\n",
    "        fitted_model = model.fit()\n",
    "        return fitted_model\n",
    "    except Exception as e:\n",
    "        print(f\"SARIMA model failed: {e}\")\n",
    "        return None\n",
    "\n",
    "def create_sarimax_model(train_data, exog_data):\n",
    "    \"\"\"Create and fit SARIMAX model with exogenous variables\"\"\"\n",
    "    try:\n",
    "        model = SARIMAX(\n",
    "            train_data,\n",
    "            exog=exog_data,\n",
    "            order=MODEL_CONFIGS['SARIMAX']['params']['order'],\n",
    "            seasonal_order=MODEL_CONFIGS['SARIMAX']['params']['seasonal_order']\n",
    "        )\n",
    "        fitted_model = model.fit(disp=False)\n",
    "        return fitted_model\n",
    "    except Exception as e:\n",
    "        print(f\"SARIMAX model failed: {e}\")\n",
    "        return None\n",
    "\n",
    "# Validate exogenous variables are available\n",
    "available_exog = [col for col in EXOG_VARS if col in training_data.columns]\n",
    "missing_exog = [col for col in EXOG_VARS if col not in training_data.columns]\n",
    "\n",
    "print(\"🤖 MODEL CONFIGURATIONS:\")\n",
    "for model_name, config in MODEL_CONFIGS.items():\n",
    "    print(f\"  {model_name}: {config['description']}\")\n",
    "    if 'exog_vars' in config['params']:\n",
    "        print(f\"    Exogenous variables: {len(available_exog)}/{len(EXOG_VARS)} available\")\n",
    "\n",
    "if missing_exog:\n",
    "    print(f\"⚠️ Missing exogenous variables: {missing_exog}\")\n",
    "else:\n",
    "    print(\"✅ All exogenous variables available\")\n",
    "\n",
    "print(f\"\\n📋 Exogenous variables for SARIMAX:\")\n",
    "for i, var in enumerate(available_exog, 1):\n",
    "    print(f\"  {i:2d}. {var}\")\n",
    "\n",
    "logger.info(\"✅ Model configuration complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Starting TRUE ROLLING WINDOW validation...\n",
      "Legend: N=Naive, S=SARIMA, X=SARIMAX | Values=RMSE | *=Fallback | FAIL=Model failed\n",
      "\n",
      "🔄 Starting TRUE rolling window validation (30 days)...\n",
      "🎯 Each day: entire dataset shifts by 1 day\n",
      "📊 Exogenous variables available: 18\n",
      "⚡ Warnings suppressed for clean output\n",
      "\n",
      "📊 Day  1: 2025-03-15 | N:0.0328 | S:0.0237* | X:0.0146\n",
      "📊 Day  2: 2025-03-16 | N:0.0246 | S:0.0189* | X:0.0140\n",
      "📊 Day  3: 2025-03-17 | N:0.0265 | S:0.0351* | X:0.0170\n",
      "📊 Day  4: 2025-03-18 | N:0.0528 | S:0.0463* | X:0.0132\n",
      "📊 Day  5: 2025-03-19 | N:0.0452 | S:0.0504* | X:0.0132\n",
      "\n",
      "📈 Progress: 5/30 days | Success rate: 100.0%\n",
      "\n",
      "📊 Day  6: 2025-03-20 | N:0.0544 | S:0.0508* | X:0.0140\n",
      "📊 Day  7: 2025-03-21 | N:0.0606 | S:0.0615* | X:0.0261\n",
      "📊 Day  8: 2025-03-22 | N:0.0609 | S:0.0608* | X:0.0210\n",
      "📊 Day  9: 2025-03-23 | N:0.0549 | S:0.0573* | X:0.0365\n",
      "📊 Day 10: 2025-03-24 | N:0.0798 | S:0.0584* | X:0.0383\n",
      "\n",
      "📈 Progress: 10/30 days | Success rate: 100.0%\n",
      "\n",
      "📊 Day 11: 2025-03-25 | N:0.0809 | S:0.0592* | X:0.0245\n",
      "📊 Day 12: 2025-03-26 | N:0.0540 | S:0.0531* | X:0.0322\n",
      "📊 Day 13: 2025-03-27 | N:0.0264 | S:0.0303* | X:0.0107\n",
      "📊 Day 14: 2025-03-28 | N:0.0267 | S:0.0233* | X:0.0165\n",
      "📊 Day 15: 2025-03-29 | N:0.0405 | S:0.0442* | X:0.0178\n",
      "\n",
      "📈 Progress: 15/30 days | Success rate: 100.0%\n",
      "\n",
      "📊 Day 16: 2025-03-30 | N:0.0487 | S:0.0481* | X:0.0382\n",
      "📊 Day 17: 2025-03-31 | N:0.0359 | S:0.0372* | X:0.0399\n",
      "📊 Day 18: 2025-04-01 | N:0.0890 | S:0.0694* | X:0.0377\n",
      "📊 Day 19: 2025-04-02 | N:0.0924 | S:0.0911* | X:0.0463\n",
      "📊 Day 20: 2025-04-03 | N:0.0534 | S:0.0527* | X:0.0225\n",
      "\n",
      "📈 Progress: 20/30 days | Success rate: 100.0%\n",
      "\n",
      "📊 Day 21: 2025-04-04 | N:0.0705 | S:0.0648* | X:0.0345\n",
      "📊 Day 22: 2025-04-05 | N:0.0574 | S:0.0611* | X:0.0230\n",
      "📊 Day 23: 2025-04-06 | N:0.0552 | S:0.0504* | X:0.0129\n",
      "📊 Day 24: 2025-04-07 | N:0.0752 | S:0.0648* | X:0.0275\n",
      "📊 Day 25: 2025-04-08 | N:0.0899 | S:0.0714* | X:0.0329\n",
      "\n",
      "📈 Progress: 25/30 days | Success rate: 100.0%\n",
      "\n",
      "📊 Day 26: 2025-04-09 | N:0.0545 | S:0.0703* | X:0.0293\n",
      "📊 Day 27: 2025-04-10 | N:0.0431 | S:0.0414* | X:0.0154\n",
      "📊 Day 28: 2025-04-11 | N:0.0405 | S:0.0418* | X:0.0072\n",
      "📊 Day 29: 2025-04-12 | N:0.0405 | S:0.0401* | X:0.0117\n",
      "📊 Day 30: 2025-04-13 | N:0.0519 | S:0.0512* | X:0.0173\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-28 11:36:27,840 - __main__ - INFO - ✅ TRUE rolling window validation complete\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📈 Progress: 30/30 days | Success rate: 100.0%\n",
      "\n",
      "\n",
      "✅ TRUE rolling window validation complete!\n",
      "⏱️ Total time: 1100.7 seconds (18.3 minutes)\n",
      "📊 Completed 30 validation windows\n",
      "⚡ Average time per day: 36.7 seconds\n",
      "\n",
      "📈 SUCCESS RATES:\n",
      "  Naive: 30/30 (100.0%) | Avg RMSE: 0.053970\n",
      "  SARIMA: 30/30 (100.0%) | Avg RMSE: 0.050976\n",
      "  SARIMAX: 30/30 (100.0%) | Avg RMSE: 0.023518\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Cell 3: 30-Day TRUE Rolling Window Validation\n",
    "# ============================================================================\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "\n",
    "# Suppress ALL warnings and logging for cleaner output\n",
    "warnings.filterwarnings('ignore')\n",
    "import logging\n",
    "logging.getLogger('build_training_set').setLevel(logging.ERROR)  # Suppress build_training_set INFO logs\n",
    "\n",
    "def run_true_rolling_window_validation(n_days=30):\n",
    "    \"\"\"\n",
    "    TRUE rolling window validation - rebuild dataset each day with shifted dates\n",
    "    Day 1: Jan 1 - Mar 14 (test Mar 15)\n",
    "    Day 2: Jan 2 - Mar 15 (test Mar 16)\n",
    "    Day 3: Jan 3 - Mar 16 (test Mar 17)\n",
    "    etc.\n",
    "    \"\"\"\n",
    "    from utils.build_training_set import build_training_set\n",
    "    \n",
    "    results_matrix = []\n",
    "    \n",
    "    # Get available exogenous variables (from initial load to check schema)\n",
    "    temp_data = build_training_set(\n",
    "        train_start=\"2025-01-01 00:00:00\",\n",
    "        train_end=\"2025-03-14 23:00:00\", \n",
    "        run_date=\"2025-03-15 12:00:00\"\n",
    "    )\n",
    "    available_exog = [col for col in EXOG_VARS if col in temp_data.columns]\n",
    "    \n",
    "    print(f\"🔄 Starting TRUE rolling window validation (30 days)...\")\n",
    "    print(f\"🎯 Each day: entire dataset shifts by 1 day\")\n",
    "    print(f\"📊 Exogenous variables available: {len(available_exog)}\")\n",
    "    print(\"⚡ Warnings suppressed for clean output\\n\")\n",
    "    \n",
    "    successful_days = 0\n",
    "    failed_days = 0\n",
    "    \n",
    "    for day in range(n_days):\n",
    "        # Calculate shifting dates for this iteration\n",
    "        train_start_date = datetime(2025, 1, 1) + timedelta(days=day)\n",
    "        train_end_date = datetime(2025, 3, 14) + timedelta(days=day)\n",
    "        run_date = datetime(2025, 3, 15) + timedelta(days=day)\n",
    "        \n",
    "        train_start_str = train_start_date.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        train_end_str = train_end_date.strftime(\"%Y-%m-%d %H:%M:%S\") \n",
    "        run_date_str = run_date.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        \n",
    "        print(f\"📊 Day {day+1:2d}: {run_date.strftime('%Y-%m-%d')}\", end=\"\", flush=True)\n",
    "        \n",
    "        # Load shifted dataset for this day\n",
    "        try:\n",
    "            daily_data = build_training_set(\n",
    "                train_start=train_start_str,\n",
    "                train_end=train_end_str,\n",
    "                run_date=run_date_str\n",
    "            )\n",
    "            \n",
    "            if daily_data is None or len(daily_data) == 0:\n",
    "                print(\" | DATA:FAIL\")\n",
    "                continue\n",
    "                \n",
    "            # Set index for time series\n",
    "            daily_data = daily_data.set_index('target_datetime')\n",
    "            daily_data.index = pd.to_datetime(daily_data.index, utc=True)\n",
    "            \n",
    "            # Split into train/test (last 24 hours as test)\n",
    "            split_point = daily_data.index[-24]  # Last 24 hours for testing\n",
    "            train_data = daily_data[daily_data.index < split_point]['Price'].copy()\n",
    "            test_data = daily_data[daily_data.index >= split_point]['Price'].copy()\n",
    "            \n",
    "            if len(train_data) == 0 or len(test_data) == 0:\n",
    "                print(\" | SPLIT:FAIL\")\n",
    "                continue\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(\" | LOAD:FAIL\")\n",
    "            continue\n",
    "        \n",
    "        # Start the results line\n",
    "        print(\" | \", end=\"\", flush=True)\n",
    "        \n",
    "        day_results = {\n",
    "            'Day': day + 1,\n",
    "            'Test_Date': run_date.strftime('%Y-%m-%d'),\n",
    "            'Train_Start': train_start_date.strftime('%Y-%m-%d'),\n",
    "            'Train_End': train_end_date.strftime('%Y-%m-%d'),\n",
    "            'Train_Samples': len(train_data),\n",
    "            'Test_Samples': len(test_data)\n",
    "        }\n",
    "        \n",
    "        day_success = True\n",
    "        \n",
    "        # 1. NAIVE MODEL\n",
    "        try:\n",
    "            naive_preds = [train_data.iloc[-24]] * len(test_data) if len(train_data) >= 24 else [train_data.iloc[-1]] * len(test_data)\n",
    "            naive_rmse = np.sqrt(mean_squared_error(test_data, naive_preds))\n",
    "            day_results['Naive'] = naive_rmse\n",
    "            print(f\"N:{naive_rmse:.4f}\", end=\" | \", flush=True)\n",
    "        except Exception:\n",
    "            day_results['Naive'] = np.nan\n",
    "            print(\"N:FAIL\", end=\" | \", flush=True)\n",
    "            day_success = False\n",
    "        \n",
    "        # 2. SARIMA MODEL (Simple ARIMA)\n",
    "        try:\n",
    "            with warnings.catch_warnings():\n",
    "                warnings.simplefilter(\"ignore\")\n",
    "                \n",
    "                model = ARIMA(train_data, order=(1, 0, 0), enforce_stationarity=False, enforce_invertibility=False)\n",
    "                fitted_model = model.fit(method='mle', maxiter=50, disp=False)\n",
    "                sarima_forecast = fitted_model.forecast(steps=len(test_data))\n",
    "                sarima_rmse = np.sqrt(mean_squared_error(test_data, sarima_forecast))\n",
    "                day_results['SARIMA'] = sarima_rmse\n",
    "                print(f\"S:{sarima_rmse:.4f}\", end=\" | \", flush=True)\n",
    "                \n",
    "        except Exception:\n",
    "            # Fallback to moving average\n",
    "            try:\n",
    "                window_size = min(24, len(train_data))\n",
    "                ma_pred = train_data.rolling(window=window_size).mean().iloc[-1]\n",
    "                ma_forecast = [ma_pred] * len(test_data)\n",
    "                sarima_rmse = np.sqrt(mean_squared_error(test_data, ma_forecast))\n",
    "                day_results['SARIMA'] = sarima_rmse\n",
    "                print(f\"S:{sarima_rmse:.4f}*\", end=\" | \", flush=True)\n",
    "            except:\n",
    "                day_results['SARIMA'] = np.nan\n",
    "                print(\"S:FAIL\", end=\" | \", flush=True)\n",
    "                day_success = False\n",
    "        \n",
    "        # 3. SARIMAX MODEL\n",
    "        try:\n",
    "            if available_exog:\n",
    "                # Get exogenous data for this iteration\n",
    "                train_exog = daily_data[daily_data.index < split_point][available_exog].copy()\n",
    "                test_exog = daily_data[daily_data.index >= split_point][available_exog].copy()\n",
    "                \n",
    "                if len(train_exog) == len(train_data) and len(test_exog) == len(test_data):\n",
    "                    with warnings.catch_warnings():\n",
    "                        warnings.simplefilter(\"ignore\")\n",
    "                        \n",
    "                        model = SARIMAX(\n",
    "                            train_data,\n",
    "                            exog=train_exog,\n",
    "                            order=(1, 0, 1),\n",
    "                            seasonal_order=(1, 0, 1, 24),\n",
    "                            enforce_stationarity=False,\n",
    "                            enforce_invertibility=False\n",
    "                        )\n",
    "                        \n",
    "                        fitted_model = model.fit(method='lbfgs', maxiter=50, disp=False)\n",
    "                        sarimax_forecast = fitted_model.forecast(steps=len(test_data), exog=test_exog)\n",
    "                        sarimax_rmse = np.sqrt(mean_squared_error(test_data, sarimax_forecast))\n",
    "                        day_results['SARIMAX'] = sarimax_rmse  \n",
    "                        print(f\"X:{sarimax_rmse:.4f}\", flush=True)\n",
    "                else:\n",
    "                    day_results['SARIMAX'] = np.nan\n",
    "                    print(\"X:SIZE\", flush=True)\n",
    "                    day_success = False\n",
    "            else:\n",
    "                day_results['SARIMAX'] = np.nan\n",
    "                print(\"X:NOEXOG\", flush=True)\n",
    "                day_success = False\n",
    "                \n",
    "        except Exception:\n",
    "            day_results['SARIMAX'] = np.nan\n",
    "            print(\"X:FAIL\", flush=True)\n",
    "            day_success = False\n",
    "        \n",
    "        if day_success:\n",
    "            successful_days += 1\n",
    "        else:\n",
    "            failed_days += 1\n",
    "            \n",
    "        results_matrix.append(day_results)\n",
    "        \n",
    "        # Progress update every 5 days\n",
    "        if (day + 1) % 5 == 0:\n",
    "            success_rate = successful_days / (successful_days + failed_days) * 100\n",
    "            print(f\"\\n📈 Progress: {day+1}/{n_days} days | Success rate: {success_rate:.1f}%\\n\")\n",
    "    \n",
    "    return pd.DataFrame(results_matrix)\n",
    "\n",
    "# Run TRUE rolling window validation\n",
    "print(\"🚀 Starting TRUE ROLLING WINDOW validation...\")\n",
    "print(\"Legend: N=Naive, S=SARIMA, X=SARIMAX | Values=RMSE | *=Fallback | FAIL=Model failed\\n\")\n",
    "\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "results_df = run_true_rolling_window_validation(n_days=30)\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "\n",
    "if not results_df.empty:\n",
    "    print(f\"\\n✅ TRUE rolling window validation complete!\")\n",
    "    print(f\"⏱️ Total time: {elapsed_time:.1f} seconds ({elapsed_time/60:.1f} minutes)\")\n",
    "    print(f\"📊 Completed {len(results_df)} validation windows\")\n",
    "    print(f\"⚡ Average time per day: {elapsed_time/len(results_df):.1f} seconds\")\n",
    "    \n",
    "    # Show success rates\n",
    "    model_cols = ['Naive', 'SARIMA', 'SARIMAX']\n",
    "    print(f\"\\n📈 SUCCESS RATES:\")\n",
    "    for model in model_cols:\n",
    "        if model in results_df.columns:\n",
    "            valid_results = results_df[model].dropna()\n",
    "            success_rate = len(valid_results) / len(results_df) * 100\n",
    "            if len(valid_results) > 0:\n",
    "                avg_rmse = valid_results.mean()\n",
    "                print(f\"  {model}: {len(valid_results)}/{len(results_df)} ({success_rate:.1f}%) | Avg RMSE: {avg_rmse:.6f}\")\n",
    "            else:\n",
    "                print(f\"  {model}: 0/{len(results_df)} (0.0%) | No successful runs\")\n",
    "else:\n",
    "    print(\"❌ No validation results generated\")\n",
    "\n",
    "logger.info(\"✅ TRUE rolling window validation complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 RESULTS MATRIX:\n",
      "================================================================================\n",
      "📅 Rolling Window Results (30 days):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Day</th>\n",
       "      <th>Test_Date</th>\n",
       "      <th>Naive</th>\n",
       "      <th>SARIMA</th>\n",
       "      <th>SARIMAX</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2025-03-15</td>\n",
       "      <td>0.032763</td>\n",
       "      <td>0.023730</td>\n",
       "      <td>0.014562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2025-03-16</td>\n",
       "      <td>0.024564</td>\n",
       "      <td>0.018892</td>\n",
       "      <td>0.013972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>2025-03-17</td>\n",
       "      <td>0.026530</td>\n",
       "      <td>0.035072</td>\n",
       "      <td>0.016972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>2025-03-18</td>\n",
       "      <td>0.052788</td>\n",
       "      <td>0.046339</td>\n",
       "      <td>0.013162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>2025-03-19</td>\n",
       "      <td>0.045184</td>\n",
       "      <td>0.050427</td>\n",
       "      <td>0.013158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>2025-03-20</td>\n",
       "      <td>0.054443</td>\n",
       "      <td>0.050817</td>\n",
       "      <td>0.014000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>2025-03-21</td>\n",
       "      <td>0.060640</td>\n",
       "      <td>0.061482</td>\n",
       "      <td>0.026083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>2025-03-22</td>\n",
       "      <td>0.060913</td>\n",
       "      <td>0.060756</td>\n",
       "      <td>0.021044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>2025-03-23</td>\n",
       "      <td>0.054950</td>\n",
       "      <td>0.057293</td>\n",
       "      <td>0.036470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>2025-03-24</td>\n",
       "      <td>0.079806</td>\n",
       "      <td>0.058384</td>\n",
       "      <td>0.038330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>2025-03-25</td>\n",
       "      <td>0.080854</td>\n",
       "      <td>0.059177</td>\n",
       "      <td>0.024464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>2025-03-26</td>\n",
       "      <td>0.053974</td>\n",
       "      <td>0.053117</td>\n",
       "      <td>0.032177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>2025-03-27</td>\n",
       "      <td>0.026445</td>\n",
       "      <td>0.030262</td>\n",
       "      <td>0.010663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>2025-03-28</td>\n",
       "      <td>0.026714</td>\n",
       "      <td>0.023286</td>\n",
       "      <td>0.016498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>2025-03-29</td>\n",
       "      <td>0.040492</td>\n",
       "      <td>0.044225</td>\n",
       "      <td>0.017834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16</td>\n",
       "      <td>2025-03-30</td>\n",
       "      <td>0.048693</td>\n",
       "      <td>0.048072</td>\n",
       "      <td>0.038226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17</td>\n",
       "      <td>2025-03-31</td>\n",
       "      <td>0.035935</td>\n",
       "      <td>0.037155</td>\n",
       "      <td>0.039932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18</td>\n",
       "      <td>2025-04-01</td>\n",
       "      <td>0.088973</td>\n",
       "      <td>0.069408</td>\n",
       "      <td>0.037687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19</td>\n",
       "      <td>2025-04-02</td>\n",
       "      <td>0.092428</td>\n",
       "      <td>0.091136</td>\n",
       "      <td>0.046331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20</td>\n",
       "      <td>2025-04-03</td>\n",
       "      <td>0.053370</td>\n",
       "      <td>0.052732</td>\n",
       "      <td>0.022471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>21</td>\n",
       "      <td>2025-04-04</td>\n",
       "      <td>0.070529</td>\n",
       "      <td>0.064800</td>\n",
       "      <td>0.034486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>22</td>\n",
       "      <td>2025-04-05</td>\n",
       "      <td>0.057443</td>\n",
       "      <td>0.061134</td>\n",
       "      <td>0.022997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>23</td>\n",
       "      <td>2025-04-06</td>\n",
       "      <td>0.055205</td>\n",
       "      <td>0.050417</td>\n",
       "      <td>0.012870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>24</td>\n",
       "      <td>2025-04-07</td>\n",
       "      <td>0.075198</td>\n",
       "      <td>0.064814</td>\n",
       "      <td>0.027479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>25</td>\n",
       "      <td>2025-04-08</td>\n",
       "      <td>0.089854</td>\n",
       "      <td>0.071421</td>\n",
       "      <td>0.032934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>26</td>\n",
       "      <td>2025-04-09</td>\n",
       "      <td>0.054498</td>\n",
       "      <td>0.070342</td>\n",
       "      <td>0.029259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>27</td>\n",
       "      <td>2025-04-10</td>\n",
       "      <td>0.043087</td>\n",
       "      <td>0.041420</td>\n",
       "      <td>0.015368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>28</td>\n",
       "      <td>2025-04-11</td>\n",
       "      <td>0.040477</td>\n",
       "      <td>0.041832</td>\n",
       "      <td>0.007165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>29</td>\n",
       "      <td>2025-04-12</td>\n",
       "      <td>0.040478</td>\n",
       "      <td>0.040129</td>\n",
       "      <td>0.011663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>30</td>\n",
       "      <td>2025-04-13</td>\n",
       "      <td>0.051859</td>\n",
       "      <td>0.051215</td>\n",
       "      <td>0.017284</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Day   Test_Date     Naive    SARIMA   SARIMAX\n",
       "0     1  2025-03-15  0.032763  0.023730  0.014562\n",
       "1     2  2025-03-16  0.024564  0.018892  0.013972\n",
       "2     3  2025-03-17  0.026530  0.035072  0.016972\n",
       "3     4  2025-03-18  0.052788  0.046339  0.013162\n",
       "4     5  2025-03-19  0.045184  0.050427  0.013158\n",
       "5     6  2025-03-20  0.054443  0.050817  0.014000\n",
       "6     7  2025-03-21  0.060640  0.061482  0.026083\n",
       "7     8  2025-03-22  0.060913  0.060756  0.021044\n",
       "8     9  2025-03-23  0.054950  0.057293  0.036470\n",
       "9    10  2025-03-24  0.079806  0.058384  0.038330\n",
       "10   11  2025-03-25  0.080854  0.059177  0.024464\n",
       "11   12  2025-03-26  0.053974  0.053117  0.032177\n",
       "12   13  2025-03-27  0.026445  0.030262  0.010663\n",
       "13   14  2025-03-28  0.026714  0.023286  0.016498\n",
       "14   15  2025-03-29  0.040492  0.044225  0.017834\n",
       "15   16  2025-03-30  0.048693  0.048072  0.038226\n",
       "16   17  2025-03-31  0.035935  0.037155  0.039932\n",
       "17   18  2025-04-01  0.088973  0.069408  0.037687\n",
       "18   19  2025-04-02  0.092428  0.091136  0.046331\n",
       "19   20  2025-04-03  0.053370  0.052732  0.022471\n",
       "20   21  2025-04-04  0.070529  0.064800  0.034486\n",
       "21   22  2025-04-05  0.057443  0.061134  0.022997\n",
       "22   23  2025-04-06  0.055205  0.050417  0.012870\n",
       "23   24  2025-04-07  0.075198  0.064814  0.027479\n",
       "24   25  2025-04-08  0.089854  0.071421  0.032934\n",
       "25   26  2025-04-09  0.054498  0.070342  0.029259\n",
       "26   27  2025-04-10  0.043087  0.041420  0.015368\n",
       "27   28  2025-04-11  0.040477  0.041832  0.007165\n",
       "28   29  2025-04-12  0.040478  0.040129  0.011663\n",
       "29   30  2025-04-13  0.051859  0.051215  0.017284"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 SUMMARY STATISTICS:\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Mean_RMSE</th>\n",
       "      <th>Std_RMSE</th>\n",
       "      <th>Min_RMSE</th>\n",
       "      <th>Max_RMSE</th>\n",
       "      <th>Success_Rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Naive</td>\n",
       "      <td>0.053970</td>\n",
       "      <td>0.019372</td>\n",
       "      <td>0.024564</td>\n",
       "      <td>0.092428</td>\n",
       "      <td>30/30 (100.0%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SARIMA</td>\n",
       "      <td>0.050976</td>\n",
       "      <td>0.015960</td>\n",
       "      <td>0.018892</td>\n",
       "      <td>0.091136</td>\n",
       "      <td>30/30 (100.0%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SARIMAX</td>\n",
       "      <td>0.023518</td>\n",
       "      <td>0.010723</td>\n",
       "      <td>0.007165</td>\n",
       "      <td>0.046331</td>\n",
       "      <td>30/30 (100.0%)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Model Mean_RMSE  Std_RMSE  Min_RMSE  Max_RMSE    Success_Rate\n",
       "0    Naive  0.053970  0.019372  0.024564  0.092428  30/30 (100.0%)\n",
       "1   SARIMA  0.050976  0.015960  0.018892  0.091136  30/30 (100.0%)\n",
       "2  SARIMAX  0.023518  0.010723  0.007165  0.046331  30/30 (100.0%)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-28 11:36:27,888 - __main__ - INFO - ✅ Results analysis complete\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🏆 MODEL RANKING (by Mean RMSE):\n",
      "----------------------------------------\n",
      "  🥇 SARIMAX: 0.023518 RMSE (based on 30 days)\n",
      "  🥈 SARIMA: 0.050976 RMSE (based on 30 days)\n",
      "  🥉 Naive: 0.053970 RMSE (based on 30 days)\n",
      "\n",
      "💾 Results saved to 'results_df' variable for further analysis\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Cell 4: Results Matrix & Analysis\n",
    "# ============================================================================\n",
    "\n",
    "def create_results_matrix(results_df):\n",
    "    \"\"\"Create clean results matrix with RMSE values\"\"\"\n",
    "    \n",
    "    if results_df.empty:\n",
    "        print(\"❌ No results to display\")\n",
    "        return None\n",
    "    \n",
    "    # Select columns for the matrix\n",
    "    display_cols = ['Day', 'Test_Date', 'Naive', 'SARIMA', 'SARIMAX']\n",
    "    available_cols = [col for col in display_cols if col in results_df.columns]\n",
    "    \n",
    "    matrix_df = results_df[available_cols].copy()\n",
    "    \n",
    "    # Round RMSE values to 6 decimal places\n",
    "    model_cols = ['Naive', 'SARIMA', 'SARIMAX']\n",
    "    for col in model_cols:\n",
    "        if col in matrix_df.columns:\n",
    "            matrix_df[col] = matrix_df[col].round(6)\n",
    "    \n",
    "    return matrix_df\n",
    "\n",
    "def calculate_summary_stats(results_df):\n",
    "    \"\"\"Calculate summary statistics for each model\"\"\"\n",
    "    \n",
    "    model_cols = ['Naive', 'SARIMA', 'SARIMAX']\n",
    "    available_models = [col for col in model_cols if col in results_df.columns]\n",
    "    \n",
    "    summary_stats = {}\n",
    "    \n",
    "    for model in available_models:\n",
    "        valid_results = results_df[model].dropna()\n",
    "        \n",
    "        if len(valid_results) > 0:\n",
    "            summary_stats[model] = {\n",
    "                'Mean': valid_results.mean(),\n",
    "                'Std': valid_results.std(),\n",
    "                'Min': valid_results.min(),\n",
    "                'Max': valid_results.max(),\n",
    "                'Valid_Days': len(valid_results),\n",
    "                'Total_Days': len(results_df)\n",
    "            }\n",
    "        else:\n",
    "            summary_stats[model] = {\n",
    "                'Mean': np.nan,\n",
    "                'Std': np.nan,\n",
    "                'Min': np.nan,\n",
    "                'Max': np.nan,\n",
    "                'Valid_Days': 0,\n",
    "                'Total_Days': len(results_df)\n",
    "            }\n",
    "    \n",
    "    return summary_stats\n",
    "\n",
    "# Create and display results matrix\n",
    "print(\"📊 RESULTS MATRIX:\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "results_matrix = create_results_matrix(results_df)\n",
    "\n",
    "if results_matrix is not None:\n",
    "    # Display the matrix\n",
    "    from IPython.display import display, HTML\n",
    "    \n",
    "    # Style the dataframe for better display\n",
    "    styled_matrix = results_matrix.copy()\n",
    "    \n",
    "    # Replace NaN with dash for cleaner display\n",
    "    model_cols = ['Naive', 'SARIMA', 'SARIMAX']\n",
    "    for col in model_cols:\n",
    "        if col in styled_matrix.columns:\n",
    "            styled_matrix[col] = styled_matrix[col].fillna('-')\n",
    "    \n",
    "    print(f\"📅 Rolling Window Results ({len(results_matrix)} days):\")\n",
    "    display(styled_matrix)\n",
    "    \n",
    "    # Calculate and display summary statistics\n",
    "    print(f\"\\n📊 SUMMARY STATISTICS:\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    summary_stats = calculate_summary_stats(results_df)\n",
    "    \n",
    "    # Create summary DataFrame\n",
    "    summary_rows = []\n",
    "    for model, stats in summary_stats.items():\n",
    "        summary_rows.append({\n",
    "            'Model': model,\n",
    "            'Mean_RMSE': f\"{stats['Mean']:.6f}\" if not np.isnan(stats['Mean']) else '-',\n",
    "            'Std_RMSE': f\"{stats['Std']:.6f}\" if not np.isnan(stats['Std']) else '-',\n",
    "            'Min_RMSE': f\"{stats['Min']:.6f}\" if not np.isnan(stats['Min']) else '-',\n",
    "            'Max_RMSE': f\"{stats['Max']:.6f}\" if not np.isnan(stats['Max']) else '-',\n",
    "            'Success_Rate': f\"{stats['Valid_Days']}/{stats['Total_Days']} ({100*stats['Valid_Days']/stats['Total_Days']:.1f}%)\"\n",
    "        })\n",
    "    \n",
    "    summary_df = pd.DataFrame(summary_rows)\n",
    "    display(summary_df)\n",
    "    \n",
    "    # Determine best model\n",
    "    print(f\"\\n🏆 MODEL RANKING (by Mean RMSE):\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    valid_models = []\n",
    "    for model, stats in summary_stats.items():\n",
    "        if not np.isnan(stats['Mean']) and stats['Valid_Days'] > 0:\n",
    "            valid_models.append((model, stats['Mean'], stats['Valid_Days']))\n",
    "    \n",
    "    # Sort by mean RMSE (ascending = better)\n",
    "    valid_models.sort(key=lambda x: x[1])\n",
    "    \n",
    "    for i, (model, mean_rmse, valid_days) in enumerate(valid_models, 1):\n",
    "        status = \"🥇\" if i == 1 else \"🥈\" if i == 2 else \"🥉\" if i == 3 else f\"{i}.\"\n",
    "        print(f\"  {status} {model}: {mean_rmse:.6f} RMSE (based on {valid_days} days)\")\n",
    "    \n",
    "    if not valid_models:\n",
    "        print(\"  ❌ No models produced valid results\")\n",
    "    \n",
    "    print(f\"\\n💾 Results saved to 'results_df' variable for further analysis\")\n",
    "    \n",
    "else:\n",
    "    print(\"❌ Unable to create results matrix\")\n",
    "\n",
    "logger.info(\"✅ Results analysis complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-28 11:36:27,918 - __main__ - INFO - ✅ Evaluation complete\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎯 FINAL RECOMMENDATIONS:\n",
      "============================================================\n",
      "🏆 RECOMMENDED MODEL: SARIMAX\n",
      "   Mean RMSE: 0.023518\n",
      "   Std RMSE:  0.010723\n",
      "   Success Rate: 100.0% (30/30 days)\n",
      "\n",
      "📈 PERFORMANCE INSIGHTS:\n",
      "------------------------------\n",
      "✅ SARIMAX improves over Naive by 56.4%\n",
      "\n",
      "🔍 MODEL-SPECIFIC INSIGHTS:\n",
      "------------------------------\n",
      "  SARIMAX:\n",
      "    • Uses 18 exogenous variables\n",
      "    • Most complex model, requires all input features\n",
      "    • Reasonably consistent performance\n",
      "  SARIMA:\n",
      "    • Captures seasonal patterns (24-hour cycle)\n",
      "    • No external variables required\n",
      "    • Reasonably consistent performance\n",
      "  Naive:\n",
      "    • Simple baseline using previous day same hour\n",
      "    • Always available, no fitting required\n",
      "    • Reasonably consistent performance\n",
      "\n",
      "🚀 PRODUCTION RECOMMENDATIONS:\n",
      "------------------------------\n",
      "• Consider using SARIMAX model for:\n",
      "  - Primary forecasting in production\n",
      "  - Applications requiring high accuracy\n",
      "• Recommended backup strategy:\n",
      "  - Use Naive model as fallback when advanced model fails\n",
      "  - Monitor model performance regularly\n",
      "\n",
      "📊 DATA QUALITY INSIGHTS:\n",
      "------------------------------\n",
      "\n",
      "🎯 CONCLUSION:\n",
      "---------------\n",
      "Based on 30 days of rolling window validation:\n",
      "• Best performing model: SARIMAX (RMSE: 0.023518)\n",
      "• Model complexity vs. performance trade-off analyzed\n",
      "• Ready for production deployment with appropriate monitoring\n",
      "\n",
      "💾 Results saved to: sarimax_evaluation_results_20250528_113627.csv\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Cell 5: Final Recommendations\n",
    "# ============================================================================\n",
    "\n",
    "def generate_recommendations(results_df, summary_stats):\n",
    "    \"\"\"Generate recommendations based on model performance\"\"\"\n",
    "    \n",
    "    print(\"🎯 FINAL RECOMMENDATIONS:\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Get valid models with their performance\n",
    "    valid_models = []\n",
    "    for model, stats in summary_stats.items():\n",
    "        if not np.isnan(stats['Mean']) and stats['Valid_Days'] > 0:\n",
    "            valid_models.append({\n",
    "                'model': model,\n",
    "                'mean_rmse': stats['Mean'],\n",
    "                'std_rmse': stats['Std'],\n",
    "                'success_rate': stats['Valid_Days'] / stats['Total_Days'],\n",
    "                'valid_days': stats['Valid_Days']\n",
    "            })\n",
    "    \n",
    "    if not valid_models:\n",
    "        print(\"❌ No models produced valid results for recommendation\")\n",
    "        return\n",
    "    \n",
    "    # Sort by mean RMSE\n",
    "    valid_models.sort(key=lambda x: x['mean_rmse'])\n",
    "    \n",
    "    best_model = valid_models[0]\n",
    "    \n",
    "    print(f\"🏆 RECOMMENDED MODEL: {best_model['model']}\")\n",
    "    print(f\"   Mean RMSE: {best_model['mean_rmse']:.6f}\")\n",
    "    print(f\"   Std RMSE:  {best_model['std_rmse']:.6f}\")\n",
    "    print(f\"   Success Rate: {best_model['success_rate']*100:.1f}% ({best_model['valid_days']}/{summary_stats[best_model['model']]['Total_Days']} days)\")\n",
    "    \n",
    "    # Performance insights\n",
    "    print(f\"\\n📈 PERFORMANCE INSIGHTS:\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    # Compare models\n",
    "    if len(valid_models) > 1:\n",
    "        naive_model = next((m for m in valid_models if m['model'] == 'Naive'), None)\n",
    "        best_advanced = valid_models[0] if valid_models[0]['model'] != 'Naive' else valid_models[1]\n",
    "        \n",
    "        if naive_model and best_advanced:\n",
    "            improvement = ((naive_model['mean_rmse'] - best_advanced['mean_rmse']) / naive_model['mean_rmse']) * 100\n",
    "            print(f\"✅ {best_advanced['model']} improves over Naive by {improvement:.1f}%\")\n",
    "        \n",
    "        # Consistency analysis\n",
    "        most_consistent = min(valid_models, key=lambda x: x['std_rmse'])\n",
    "        if most_consistent['model'] != best_model['model']:\n",
    "            print(f\"📊 Most consistent model: {most_consistent['model']} (Std RMSE: {most_consistent['std_rmse']:.6f})\")\n",
    "        \n",
    "        # Success rate analysis\n",
    "        most_reliable = max(valid_models, key=lambda x: x['success_rate'])\n",
    "        if most_reliable['success_rate'] < 1.0:\n",
    "            print(f\"⚠️ Model reliability: {most_reliable['model']} has highest success rate ({most_reliable['success_rate']*100:.1f}%)\")\n",
    "    \n",
    "    # Model-specific insights\n",
    "    print(f\"\\n🔍 MODEL-SPECIFIC INSIGHTS:\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    for model_info in valid_models:\n",
    "        model_name = model_info['model']\n",
    "        print(f\"  {model_name}:\")\n",
    "        \n",
    "        if model_name == 'Naive':\n",
    "            print(f\"    • Simple baseline using previous day same hour\")\n",
    "            print(f\"    • Always available, no fitting required\")\n",
    "        elif model_name == 'SARIMA':\n",
    "            print(f\"    • Captures seasonal patterns (24-hour cycle)\")\n",
    "            print(f\"    • No external variables required\")\n",
    "        elif model_name == 'SARIMAX':\n",
    "            print(f\"    • Uses {len(EXOG_VARS)} exogenous variables\")\n",
    "            print(f\"    • Most complex model, requires all input features\")\n",
    "        \n",
    "        # Performance characteristics\n",
    "        if model_info['std_rmse'] < 0.01:\n",
    "            print(f\"    • Very consistent performance\")\n",
    "        elif model_info['std_rmse'] < 0.02:\n",
    "            print(f\"    • Reasonably consistent performance\")\n",
    "        else:\n",
    "            print(f\"    • Variable performance across days\")\n",
    "        \n",
    "        if model_info['success_rate'] < 0.8:\n",
    "            print(f\"    • ⚠️ Some fitting failures ({model_info['success_rate']*100:.1f}% success rate)\")\n",
    "    \n",
    "    # Production recommendations\n",
    "    print(f\"\\n🚀 PRODUCTION RECOMMENDATIONS:\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    if best_model['model'] == 'Naive':\n",
    "        print(\"• Consider using Naive model for:\")\n",
    "        print(\"  - Quick baseline forecasts\")\n",
    "        print(\"  - Fallback when advanced models fail\")\n",
    "        print(\"  - Real-time applications requiring minimal computation\")\n",
    "    else:\n",
    "        print(f\"• Consider using {best_model['model']} model for:\")\n",
    "        print(\"  - Primary forecasting in production\")\n",
    "        print(\"  - Applications requiring high accuracy\")\n",
    "        \n",
    "        print(\"• Recommended backup strategy:\")\n",
    "        print(\"  - Use Naive model as fallback when advanced model fails\")\n",
    "        print(\"  - Monitor model performance regularly\")\n",
    "    \n",
    "    # Data quality insights\n",
    "    print(f\"\\n📊 DATA QUALITY INSIGHTS:\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    min_success_rate = min(m['success_rate'] for m in valid_models)\n",
    "    if min_success_rate < 0.9:\n",
    "        print(f\"⚠️ Some models have success rates below 90%\")\n",
    "        print(f\"   Consider investigating data quality issues\")\n",
    "    \n",
    "    # Overall conclusion\n",
    "    print(f\"\\n🎯 CONCLUSION:\")\n",
    "    print(\"-\" * 15)\n",
    "    print(f\"Based on {len(results_df)} days of rolling window validation:\")\n",
    "    print(f\"• Best performing model: {best_model['model']} (RMSE: {best_model['mean_rmse']:.6f})\")\n",
    "    print(f\"• Model complexity vs. performance trade-off analyzed\")\n",
    "    print(f\"• Ready for production deployment with appropriate monitoring\")\n",
    "\n",
    "# Generate recommendations\n",
    "if 'results_df' in locals() and not results_df.empty:\n",
    "    summary_stats = calculate_summary_stats(results_df)\n",
    "    generate_recommendations(results_df, summary_stats)\n",
    "    \n",
    "    # Save results for future reference\n",
    "    timestamp = pd.Timestamp.now().strftime('%Y%m%d_%H%M%S')\n",
    "    results_filename = f\"sarimax_evaluation_results_{timestamp}.csv\"\n",
    "    results_df.to_csv(results_filename, index=False)\n",
    "    print(f\"\\n💾 Results saved to: {results_filename}\")\n",
    "    \n",
    "else:\n",
    "    print(\"❌ No results available for recommendations\")\n",
    "    print(\"Please run the previous cells first\")\n",
    "\n",
    "logger.info(\"✅ Evaluation complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Creating detailed model comparison analysis...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'single_run_results' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 9\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Create dictionary of model predictions from single_run_results\u001b[39;00m\n\u001b[0;32m      8\u001b[0m model_predictions \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m----> 9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m model_name, result \u001b[38;5;129;01min\u001b[39;00m \u001b[43msingle_run_results\u001b[49m\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m result\u001b[38;5;241m.\u001b[39msuccess \u001b[38;5;129;01mand\u001b[39;00m result\u001b[38;5;241m.\u001b[39mpredictions \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     11\u001b[0m         \u001b[38;5;66;03m# Use a cleaner display name\u001b[39;00m\n\u001b[0;32m     12\u001b[0m         display_name \u001b[38;5;241m=\u001b[39m model_name\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mtitle()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'single_run_results' is not defined"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Cell 6: Detailed Model Comparison Analysis (FIXED)\n",
    "# ============================================================================\n",
    "\n",
    "print(\"🔍 Creating detailed model comparison analysis...\")\n",
    "\n",
    "# Create dictionary of model predictions from single_run_results\n",
    "model_predictions = {}\n",
    "for model_name, result in single_run_results.items():\n",
    "    if result.success and result.predictions is not None:\n",
    "        # Use a cleaner display name\n",
    "        display_name = model_name.replace('_', ' ').title()\n",
    "        model_predictions[display_name] = result.predictions\n",
    "\n",
    "print(f\"📊 Analyzing {len(model_predictions)} successful models:\")\n",
    "for name in model_predictions.keys():\n",
    "    print(f\"  • {name}\")\n",
    "\n",
    "if len(model_predictions) == 0:\n",
    "    print(\"❌ No successful model predictions available for detailed analysis\")\n",
    "else:\n",
    "    # Get actual values for the forecast period\n",
    "    y_actual = data_splits.y_test\n",
    "    \n",
    "    # Calculate RMSE per day\n",
    "    print(\"\\n📅 Calculating daily RMSE...\")\n",
    "    day_rmse_data = []\n",
    "    \n",
    "    for model_name, preds in model_predictions.items():\n",
    "        # Ensure preds is a Series and has the right index\n",
    "        if not isinstance(preds, pd.Series):\n",
    "            print(f\"⚠️ {model_name} predictions not available as a Series, skipping...\")\n",
    "            continue\n",
    "            \n",
    "        # Create dataframe with actual and predicted values\n",
    "        # Align indices to handle any mismatches\n",
    "        common_idx = y_actual.index.intersection(preds.index)\n",
    "        if len(common_idx) == 0:\n",
    "            print(f\"⚠️ No common timestamps between actual and {model_name} predictions, skipping...\")\n",
    "            continue\n",
    "            \n",
    "        df_day = pd.DataFrame({\n",
    "            'actual': y_actual.loc[common_idx],\n",
    "            'pred': preds.loc[common_idx]\n",
    "        })\n",
    "        \n",
    "        # Add date column\n",
    "        df_day[\"date\"] = df_day.index.date\n",
    "        \n",
    "        # Function to calculate RMSE for a group with error handling\n",
    "        def calc_group_rmse(group):\n",
    "            try:\n",
    "                from sklearn.metrics import mean_squared_error\n",
    "                return np.sqrt(mean_squared_error(group[\"actual\"], group[\"pred\"]))\n",
    "            except Exception as e:\n",
    "                print(f\"⚠️ Error calculating RMSE for {model_name} on {group.name}: {e}\")\n",
    "                return np.nan\n",
    "                \n",
    "        # FIXED: Group by date and calculate RMSE for each day (with include_groups=False)\n",
    "        daily_rmse = df_day.groupby(\"date\", include_groups=False).apply(calc_group_rmse)\n",
    "        day_rmse_data.append(daily_rmse.round(3).rename(model_name))\n",
    "    \n",
    "    if day_rmse_data:\n",
    "        # Combine all daily RMSE data\n",
    "        rmse_day_df = pd.concat(day_rmse_data, axis=1)\n",
    "        \n",
    "        from IPython.display import display, HTML\n",
    "        display(HTML(\"<h3>📅 RMSE per dag</h3>\"))\n",
    "        display(rmse_day_df)\n",
    "        \n",
    "        # Create empty DataFrame for hourly errors\n",
    "        print(\"\\n🕒 Calculating hourly absolute errors...\")\n",
    "        rmse_full_hourly_df = pd.DataFrame(index=y_actual.index)\n",
    "        \n",
    "        # Calculate absolute error at each timestamp\n",
    "        for model_name, preds in model_predictions.items():\n",
    "            if not isinstance(preds, pd.Series):\n",
    "                print(f\"⚠️ {model_name} predictions not available as a Series, skipping...\")\n",
    "                continue\n",
    "                \n",
    "            # Align indices\n",
    "            common_idx = y_actual.index.intersection(preds.index)\n",
    "            if len(common_idx) == 0:\n",
    "                continue\n",
    "                \n",
    "            # Calculate absolute error at each timestamp\n",
    "            actual_aligned = y_actual.loc[common_idx]\n",
    "            preds_aligned = preds.loc[common_idx]\n",
    "            abs_error = np.abs(actual_aligned - preds_aligned)\n",
    "            rmse_full_hourly_df.loc[common_idx, model_name] = abs_error.round(3)\n",
    "        \n",
    "        display(HTML(\"<h3>🕒 Absolute error per tijdstip (alle uur)</h3>\"))\n",
    "        display(rmse_full_hourly_df.head(20))  # Show first 20 hours to avoid overwhelming output\n",
    "        print(f\"... (showing first 20 of {len(rmse_full_hourly_df)} total hours)\")\n",
    "        \n",
    "        # Add summary table showing average daily RMSE\n",
    "        display(HTML(\"<h3>📊 Gemiddelde RMSE per dag</h3>\"))\n",
    "        avg_day_rmse = rmse_day_df.mean().to_frame(\"Avg Daily RMSE\").round(3)\n",
    "        \n",
    "        if not avg_day_rmse.empty:\n",
    "            best_day = avg_day_rmse[\"Avg Daily RMSE\"].idxmin()\n",
    "            avg_day_rmse[\"Rank\"] = avg_day_rmse[\"Avg Daily RMSE\"].rank().astype(int)\n",
    "            avg_day_rmse.loc[best_day, \"Note\"] = \"🏆 Best model\"\n",
    "            \n",
    "            # Sort by rank for better display\n",
    "            avg_day_rmse_sorted = avg_day_rmse.sort_values(\"Rank\")\n",
    "            display(avg_day_rmse_sorted)\n",
    "        \n",
    "        # Add summary table showing percentage of days each model is best\n",
    "        display(HTML(\"<h3>🥇 Aantal dagen dat model het best presteert</h3>\"))\n",
    "        if not rmse_day_df.empty and rmse_day_df.shape[0] > 0:\n",
    "            best_days = rmse_day_df.idxmin(axis=1).value_counts().to_frame(\"Number of days best\")\n",
    "            best_days[\"Percentage\"] = (best_days[\"Number of days best\"] / len(rmse_day_df) * 100).round(1)\n",
    "            best_days[\"Percentage_str\"] = best_days[\"Percentage\"].astype(str) + '%'\n",
    "            \n",
    "            # Sort by number of days best (descending)\n",
    "            best_days_sorted = best_days.sort_values(\"Number of days best\", ascending=False)\n",
    "            \n",
    "            # Add winner emoji\n",
    "            if len(best_days_sorted) > 0:\n",
    "                best_model = best_days_sorted.index[0]\n",
    "                best_days_sorted.loc[best_model, \"Note\"] = \"🥇 Most consistent\"\n",
    "            \n",
    "            display(best_days_sorted[[\"Number of days best\", \"Percentage_str\", \"Note\"]])\n",
    "        else:\n",
    "            print(\"⚠️ Not enough daily data to calculate best performing days\")\n",
    "        \n",
    "        # Additional insights\n",
    "        print(f\"\\n💡 DETAILED INSIGHTS:\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        if not rmse_day_df.empty:\n",
    "            # Overall best model\n",
    "            overall_best = avg_day_rmse[\"Avg Daily RMSE\"].idxmin()\n",
    "            overall_best_rmse = avg_day_rmse.loc[overall_best, \"Avg Daily RMSE\"]\n",
    "            print(f\"🏆 Best overall model: {overall_best} (Avg RMSE: {overall_best_rmse:.3f})\")\n",
    "            \n",
    "            # Most consistent model (lowest std deviation of daily RMSE)\n",
    "            daily_std = rmse_day_df.std().round(3)\n",
    "            most_consistent = daily_std.idxmin()\n",
    "            consistency_score = daily_std.loc[most_consistent]\n",
    "            print(f\"📈 Most consistent model: {most_consistent} (RMSE Std: {consistency_score:.3f})\")\n",
    "            \n",
    "            # Model with best single day performance\n",
    "            best_single_day_value = rmse_day_df.min().min()\n",
    "            best_single_day_model = rmse_day_df.min().idxmin()\n",
    "            print(f\"⚡ Best single day performance: {best_single_day_model} (RMSE: {best_single_day_value:.3f})\")\n",
    "            \n",
    "            # Performance spread analysis\n",
    "            print(f\"\\n📊 Performance Spread Analysis:\")\n",
    "            for model in rmse_day_df.columns:\n",
    "                model_rmse = rmse_day_df[model].dropna()\n",
    "                if len(model_rmse) > 0:\n",
    "                    print(f\"  {model}:\")\n",
    "                    print(f\"    Range: {model_rmse.min():.3f} - {model_rmse.max():.3f}\")\n",
    "                    print(f\"    Std Dev: {model_rmse.std():.3f}\")\n",
    "                    print(f\"    Days analyzed: {len(model_rmse)}\")\n",
    "        \n",
    "        else:\n",
    "            print(\"⚠️ No daily RMSE data available for detailed insights\")\n",
    "    \n",
    "    else:\n",
    "        print(\"❌ No valid daily RMSE data could be calculated\")\n",
    "\n",
    "print(f\"\\n🎉 DETAILED COMPARISON ANALYSIS COMPLETE!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Cell 6: Rolling Window Validation\n",
    "# ============================================================================\n",
    "\n",
    "logger.info(\"🔄 Starting rolling window validation...\")\n",
    "\n",
    "# Run rolling validation\n",
    "rolling_results = experiment.run_rolling_validation(\n",
    "    n_windows=config.rolling_windows,\n",
    "    parallel=config.parallel_execution\n",
    ")\n",
    "\n",
    "if not rolling_results.empty:\n",
    "    print(f\"\\n🔄 ROLLING WINDOW VALIDATION RESULTS:\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Summary statistics by model\n",
    "    print(\"\\n📊 Summary by Model:\")\n",
    "    summary_stats = rolling_results.groupby('model_name').agg({\n",
    "        'rmse': ['mean', 'std', 'min', 'max'],\n",
    "        'mae': ['mean', 'std'],\n",
    "        'execution_time': ['mean', 'sum'],\n",
    "        'status': lambda x: f\"{(x == 'completed').sum()}/{len(x)}\"\n",
    "    }).round(6)\n",
    "    \n",
    "    print(summary_stats)\n",
    "    \n",
    "    # Create rolling validation plot\n",
    "    print(\"\\n📈 Creating rolling validation plot...\")\n",
    "    rolling_plot = visualizer.create_rolling_validation_plot(rolling_results)\n",
    "    rolling_plot.show()\n",
    "    \n",
    "    # Performance trend analysis\n",
    "    trends = experiment.validator.analyze_performance_trends(rolling_results)\n",
    "    \n",
    "    print(f\"\\n📈 PERFORMANCE TRENDS:\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    for model_name, trend_info in trends.get('performance_trends', {}).items():\n",
    "        trend = trend_info.get('trend', 'UNKNOWN')\n",
    "        degradation = trend_info.get('degradation_percent', 0)\n",
    "        windows_completed = trend_info.get('windows_completed', 0)\n",
    "        \n",
    "        print(f\"  {model_name.replace('_', ' ').title()}:\")\n",
    "        print(f\"    Trend: {trend} ({degradation:+.1f}%)\")\n",
    "        print(f\"    Windows completed: {windows_completed}/{config.rolling_windows}\")\n",
    "        \n",
    "        if trend in ['SEVERE', 'SIGNIFICANT']:\n",
    "            print(f\"    ⚠️  Performance degradation detected!\")\n",
    "        elif trend == 'IMPROVING':\n",
    "            print(f\"    ✅ Performance improving over time\")\n",
    "    \n",
    "    # Success rates\n",
    "    print(f\"\\n📊 SUCCESS RATES:\")\n",
    "    for model_name, success_info in trends.get('success_rate', {}).items():\n",
    "        rate = success_info['success_rate']\n",
    "        count = success_info['success_count']\n",
    "        total = success_info['total_count']\n",
    "        print(f\"  {model_name.replace('_', ' ').title()}: {rate:.1f}% ({count}/{total})\")\n",
    "\n",
    "else:\n",
    "    print(\"\\n❌ No rolling validation results available\")\n",
    "    print(\"This could be due to insufficient data or all models failing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Cell 7: Full Experiment & Analysis\n",
    "# ============================================================================\n",
    "\n",
    "# Run complete experiment with comprehensive logging\n",
    "experiment_name = f\"Complete_Model_Comparison_{pd.Timestamp.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "\n",
    "print(f\"🎯 Running full experiment: {experiment_name}\")\n",
    "full_results = experiment.run_full_experiment(\n",
    "    experiment_name=experiment_name,\n",
    "    include_rolling=True\n",
    ")\n",
    "\n",
    "# Display experiment summary\n",
    "summary = full_results.get('summary', {})\n",
    "\n",
    "print(f\"\\n🎉 EXPERIMENT SUMMARY:\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Experiment ID: {full_results['experiment_id']}\")\n",
    "print(f\"Name: {full_results['experiment_name']}\")\n",
    "print(f\"Status: {full_results.get('status', 'completed')}\")\n",
    "\n",
    "# Single run summary\n",
    "single_summary = summary.get('single_run_summary', {})\n",
    "if single_summary:\n",
    "    print(f\"\\n📊 Single Run Results:\")\n",
    "    print(f\"  Total models: {single_summary.get('total_models', 0)}\")\n",
    "    print(f\"  Successful: {single_summary.get('successful_models', 0)}\")\n",
    "    print(f\"  Failed: {single_summary.get('failed_models', 0)}\")\n",
    "    print(f\"  Best model: {single_summary.get('best_model', 'Unknown')}\")\n",
    "    print(f\"  Best RMSE: {single_summary.get('best_rmse', 'N/A')}\")\n",
    "\n",
    "# Rolling validation summary\n",
    "rolling_summary = summary.get('rolling_validation_summary', {})\n",
    "if rolling_summary:\n",
    "    print(f\"\\n🔄 Rolling Validation Results:\")\n",
    "    print(f\"  Total windows: {rolling_summary.get('total_windows', 0)}\")\n",
    "    print(f\"  Models tested: {len(rolling_summary.get('models_tested', []))}\")\n",
    "    if 'best_model' in rolling_summary:\n",
    "        print(f\"  Best model (avg): {rolling_summary['best_model']}\")\n",
    "        print(f\"  Best avg RMSE: {rolling_summary.get('best_avg_rmse', 'N/A')}\")\n",
    "\n",
    "# Overall recommendation\n",
    "best_model = summary.get('overall_best_model')\n",
    "if best_model:\n",
    "    print(f\"\\n🏆 OVERALL BEST MODEL: {best_model.replace('_', ' ').title()}\")\n",
    "\n",
    "# Recommendations\n",
    "recommendations = summary.get('recommendations', [])\n",
    "if recommendations:\n",
    "    print(f\"\\n💡 RECOMMENDATIONS:\")\n",
    "    for rec in recommendations:\n",
    "        print(f\"  {rec}\")\n",
    "\n",
    "print(f\"\\n📋 Results saved to database with experiment ID: {full_results['experiment_id']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Cell 8: Advanced Analysis & Historical Comparison\n",
    "# ============================================================================\n",
    "\n",
    "# Advanced model comparison\n",
    "print(\"🔍 ADVANCED MODEL ANALYSIS:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "successful_results = {name: result for name, result in single_run_results.items() if result.success}\n",
    "if len(successful_results) > 1:\n",
    "    predictions_dict = {name: result.predictions for name, result in successful_results.items()}\n",
    "    \n",
    "    comparison = metrics_calc.compare_predictions(data_splits.y_test, predictions_dict)\n",
    "    \n",
    "    print(f\"\\nModels compared: {comparison['models']}\")\n",
    "    \n",
    "    # Detailed metrics comparison\n",
    "    print(f\"\\n📊 DETAILED METRICS COMPARISON:\")\n",
    "    for model_name, metrics in comparison['metrics_comparison'].items():\n",
    "        print(f\"\\n  {model_name.replace('_', ' ').title()}:\")\n",
    "        print(f\"    RMSE: {metrics.get('rmse', 'N/A'):.6f}\")\n",
    "        print(f\"    MAE:  {metrics.get('mae', 'N/A'):.6f}\")\n",
    "        print(f\"    MAPE: {metrics.get('mape', 'N/A'):.2f}%\")\n",
    "        print(f\"    R²:   {metrics.get('r_squared', 'N/A'):.4f}\")\n",
    "        print(f\"    Correlation: {metrics.get('correlation', 'N/A'):.4f}\")\n",
    "    \n",
    "    # Model ranking\n",
    "    ranking = comparison.get('ranking', {})\n",
    "    if ranking:\n",
    "        print(f\"\\n🏆 RANKING BY RMSE:\")\n",
    "        for i, entry in enumerate(ranking['by_rmse'], 1):\n",
    "            model_display = entry['model'].replace('_', ' ').title()\n",
    "            print(f\"  {i}. {model_display}: {entry['rmse']:.6f}\")\n",
    "\n",
    "# Compare with previous experiments\n",
    "print(f\"\\n🕒 HISTORICAL COMPARISON:\")\n",
    "comparison = experiment.compare_with_previous_experiments(limit=5)\n",
    "if 'error' not in comparison:\n",
    "    print(f\"Previous experiments analyzed: {comparison['previous_experiments_count']}\")\n",
    "    \n",
    "    for rec in comparison.get('recommendations', []):\n",
    "        print(f\"  {rec}\")\n",
    "else:\n",
    "    print(f\"  {comparison['error']}\")\n",
    "\n",
    "# Data quality final assessment\n",
    "print(f\"\\n📊 FINAL DATA QUALITY ASSESSMENT:\")\n",
    "print(f\"Overall quality score: {quality_report['quality_score']:.1f}%\")\n",
    "\n",
    "target_stats = quality_report.get('target_column_stats', {})\n",
    "if target_stats:\n",
    "    print(f\"\\nTarget column ({config.target_column}) statistics:\")\n",
    "    print(f\"  Mean: {target_stats.get('mean', 'N/A'):.4f}\")\n",
    "    print(f\"  Std:  {target_stats.get('std', 'N/A'):.4f}\")\n",
    "    print(f\"  Range: {target_stats.get('min', 'N/A'):.4f} to {target_stats.get('max', 'N/A'):.4f}\")\n",
    "    print(f\"  Missing: {target_stats.get('missing_count', 'N/A')} values\")\n",
    "\n",
    "# Feature importance (if available from models)\n",
    "print(f\"\\n🔧 MODEL CONFIGURATION SUMMARY:\")\n",
    "for model_name, result in single_run_results.items():\n",
    "    if result.success:\n",
    "        print(f\"\\n  {model_name.replace('_', ' ').title()}:\")\n",
    "        print(f\"    Hyperparameters: {result.hyperparameters}\")\n",
    "        if result.diagnostics:\n",
    "            print(f\"    Diagnostics available: {list(result.diagnostics.keys())}\")\n",
    "        if result.convergence_info:\n",
    "            converged = result.convergence_info.get('converged', True)\n",
    "            print(f\"    Convergence: {'✅ Yes' if converged else '⚠️ Issues detected'}\")\n",
    "\n",
    "print(f\"\\n🎉 ANALYSIS COMPLETE!\")\n",
    "print(f\"\\n💾 All results have been logged to the database.\")\n",
    "print(f\"📊 You can query the logs database for detailed historical analysis.\")\n",
    "\n",
    "logger.info(\"🔬 Advanced analysis complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
