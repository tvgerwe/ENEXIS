{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time Series Forecasting - Refactored Architecture\n",
    "\n",
    "This notebook demonstrates the new modular architecture for time series forecasting experiments.\n",
    "\n",
    "## Features:\n",
    "- üîß Configuration-driven experiments\n",
    "- üìä Unified logging and metrics\n",
    "- üé® Interactive visualizations\n",
    "- üîÑ Rolling window validation\n",
    "- ‚ö° Parallel model execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'logging' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 33\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mevaluation\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m MetricsCalculator\n\u001b[0;32m     32\u001b[0m \u001b[38;5;66;03m# Set up logging\u001b[39;00m\n\u001b[1;32m---> 33\u001b[0m \u001b[43mlogging\u001b[49m\u001b[38;5;241m.\u001b[39mbasicConfig(\n\u001b[0;32m     34\u001b[0m     level\u001b[38;5;241m=\u001b[39mlogging\u001b[38;5;241m.\u001b[39mINFO,\n\u001b[0;32m     35\u001b[0m     \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%(asctime)s\u001b[39;00m\u001b[38;5;124m - \u001b[39m\u001b[38;5;132;01m%(name)s\u001b[39;00m\u001b[38;5;124m - \u001b[39m\u001b[38;5;132;01m%(levelname)s\u001b[39;00m\u001b[38;5;124m - \u001b[39m\u001b[38;5;132;01m%(message)s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     36\u001b[0m )\n\u001b[0;32m     37\u001b[0m logger \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mgetLogger(\u001b[38;5;18m__name__\u001b[39m)\n\u001b[0;32m     39\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124müìö All imports loaded successfully\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'logging' is not defined"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Cell 1: Configuration and Imports\n",
    "# ============================================================================\n",
    "\n",
    "import sys\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "# Setup paths\n",
    "current_dir = Path.cwd()\n",
    "if \"ENEXIS\" in str(current_dir):\n",
    "    while current_dir.name != \"ENEXIS\" and current_dir.parent != current_dir:\n",
    "        current_dir = current_dir.parent\n",
    "    project_root = current_dir\n",
    "else:\n",
    "    project_root = current_dir\n",
    "\n",
    "src_path = project_root / \"src\"\n",
    "if str(src_path) not in sys.path:\n",
    "    sys.path.insert(0, str(src_path))\n",
    "\n",
    "# Import directly from files to avoid __init__.py issues\n",
    "from config.experiment_config import ExperimentConfig\n",
    "from core.data_manager import DataManager\n",
    "from core.logging_manager import ExperimentLogger\n",
    "\n",
    "# Import specific files instead of modules\n",
    "from core.experiment import TimeSeriesExperiment\n",
    "from models.factory import ModelFactory\n",
    "from evaluation.metrics import MetricsCalculator\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\"\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "logger.info(\"üìö All imports loaded successfully\")\n",
    "\n",
    "from utils.build_training_set import build_training_set\n",
    "\n",
    "build_training_set(\n",
    "    train_start=\"2025-01-01 00:00:00\",\n",
    "    train_end=\"2025-03-14 23:00:00\",\n",
    "    run_date=\"2025-03-15 00:00:00\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Cell 2: Initialize Components\n",
    "# ============================================================================\n",
    "\n",
    "# Option 1: Load from YAML config file (recommended)\n",
    "# config = ExperimentConfig.from_file(\"config/experiment_config.yaml\")\n",
    "\n",
    "# Option 2: Use default configuration (for quick testing)\n",
    "config = ExperimentConfig()\n",
    "\n",
    "# Initialize core components\n",
    "data_manager = DataManager(config)\n",
    "experiment_logger = ExperimentLogger(config.logs_database_path)\n",
    "model_factory = ModelFactory(config.model_configs)\n",
    "\n",
    "logger.info(\"üîß Components initialized\")\n",
    "logger.info(f\"üìä Loaded {len(config.model_configs)} model configurations\")\n",
    "logger.info(f\"üéØ Target: {config.target_column}\")\n",
    "logger.info(f\"üìÖ Period: {config.train_start} to {config.forecast_end}\")\n",
    "\n",
    "# Display model information\n",
    "model_info = model_factory.get_model_info()\n",
    "print(f\"\\nü§ñ MODEL CONFIGURATION:\")\n",
    "print(f\"Total models: {model_info['total_models']}\")\n",
    "print(f\"Enabled: {model_info['enabled_models']}\")\n",
    "if model_info['disabled_models']:\n",
    "    print(f\"Disabled: {model_info['disabled_models']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Cell 3: Fix Config Dates - Work Around Read-Only Properties\n",
    "# ============================================================================\n",
    "\n",
    "# Database is working, let's check actual date range and work with what we can set\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "\n",
    "with sqlite3.connect(config.database_path) as conn:\n",
    "    date_query = \"SELECT MIN(target_datetime) as min_date, MAX(target_datetime) as max_date FROM master_warp\"\n",
    "    date_info = pd.read_sql(date_query, conn)\n",
    "    min_date = pd.to_datetime(date_info['min_date'].iloc[0])\n",
    "    max_date = pd.to_datetime(date_info['max_date'].iloc[0])\n",
    "    \n",
    "    print(f\"üìÖ ACTUAL DATA RANGE:\")\n",
    "    print(f\"Min date: {min_date}\")\n",
    "    print(f\"Max date: {max_date}\")\n",
    "    print(f\"Total days: {(max_date - min_date).days}\")\n",
    "\n",
    "# Check what we can actually modify in config\n",
    "print(f\"\\n‚öôÔ∏è CURRENT CONFIG:\")\n",
    "print(f\"Train start: {config.train_start}\")\n",
    "print(f\"Train end: {config.train_end}\")\n",
    "print(f\"Forecast start: {config.forecast_start}\")\n",
    "if hasattr(config, 'forecast_end'):\n",
    "    print(f\"Forecast end: {config.forecast_end}\")\n",
    "print(f\"Horizon: {config.horizon}\")\n",
    "\n",
    "# Try to update what we can\n",
    "try:\n",
    "    # Use a forecast period that exists in your data (last week of data)\n",
    "    forecast_start = max_date - pd.Timedelta(days=7)\n",
    "    forecast_end = max_date\n",
    "    \n",
    "    config.train_start = min_date\n",
    "    config.train_end = forecast_start - pd.Timedelta(hours=1)\n",
    "    config.forecast_start = forecast_start\n",
    "    \n",
    "    # Calculate horizon in hours\n",
    "    horizon_hours = int((forecast_end - forecast_start).total_seconds() / 3600) + 1\n",
    "    config.horizon = horizon_hours\n",
    "    \n",
    "    print(f\"\\n‚úÖ UPDATED CONFIG (using last week for forecast):\")\n",
    "    print(f\"Train start: {config.train_start}\")\n",
    "    print(f\"Train end: {config.train_end}\")\n",
    "    print(f\"Forecast start: {config.forecast_start}\")\n",
    "    print(f\"Forecast period: {horizon_hours} hours ({(horizon_hours/24):.1f} days)\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Could not update config: {e}\")\n",
    "    print(\"Using original config dates - might need manual adjustment\")\n",
    "\n",
    "# Clear cache\n",
    "data_manager.clear_cache()\n",
    "\n",
    "# Try with current config\n",
    "try:\n",
    "    data_splits = data_manager.create_splits()\n",
    "    logger.info(\"‚úÖ Data splits created successfully\")\n",
    "    \n",
    "    # Get split info\n",
    "    split_info = data_splits.get_info()\n",
    "    print(f\"\\nüîÑ SUCCESSFUL DATA SPLITS:\")\n",
    "    print(f\"Training samples: {split_info['train_samples']}\")\n",
    "    print(f\"Test samples: {split_info['test_samples']}\")\n",
    "    print(f\"Training period: {split_info['train_period']}\")\n",
    "    print(f\"Forecast period: {split_info['forecast_period']}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå Still failed to create splits: {e}\")\n",
    "    \n",
    "    # Try to create splits manually with good dates\n",
    "    print(\"üîß Trying manual date override...\")\n",
    "    \n",
    "    try:\n",
    "        # Override the create_splits call with manual dates\n",
    "        manual_forecast_start = max_date - pd.Timedelta(days=7)\n",
    "        manual_train_end = manual_forecast_start - pd.Timedelta(hours=1)\n",
    "        \n",
    "        data_splits = data_manager.create_splits(\n",
    "            train_start=min_date,\n",
    "            train_end=manual_train_end,\n",
    "            forecast_start=manual_forecast_start,\n",
    "            forecast_horizon=168  # 7 days in hours\n",
    "        )\n",
    "        \n",
    "        logger.info(\"‚úÖ Manual data splits created successfully\")\n",
    "        \n",
    "        split_info = data_splits.get_info()\n",
    "        print(f\"\\nüîÑ MANUAL DATA SPLITS:\")\n",
    "        print(f\"Training samples: {split_info['train_samples']}\")\n",
    "        print(f\"Test samples: {split_info['test_samples']}\")\n",
    "        print(f\"Training period: {split_info['train_period']}\")\n",
    "        print(f\"Forecast period: {split_info['forecast_period']}\")\n",
    "        \n",
    "    except Exception as manual_error:\n",
    "        print(f\"‚ùå Manual override also failed: {manual_error}\")\n",
    "        raise Exception(\"Could not create data splits with any date configuration\")\n",
    "\n",
    "# If we get here, data_splits is working\n",
    "print(f\"\\nüìä DATA READY FOR MODELING!\")\n",
    "print(f\"Target column: {config.target_column}\")\n",
    "print(f\"Ready to proceed to next cells\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Cell 4: Run Single Experiments\n",
    "# ============================================================================\n",
    "\n",
    "# Initialize experiment\n",
    "experiment = TimeSeriesExperiment(config, data_manager, experiment_logger)\n",
    "\n",
    "# Run single experiment\n",
    "logger.info(\"üöÄ Starting single experiment...\")\n",
    "single_run_results = experiment.run_single_experiment(data_splits)\n",
    "\n",
    "# Display results summary\n",
    "print(\"\\nüìä SINGLE EXPERIMENT RESULTS:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "successful_models = [name for name, result in single_run_results.items() if result.success]\n",
    "failed_models = [name for name, result in single_run_results.items() if not result.success]\n",
    "\n",
    "print(f\"‚úÖ Successful models: {len(successful_models)}\")\n",
    "print(f\"‚ùå Failed models: {len(failed_models)}\")\n",
    "\n",
    "# Calculate and display metrics\n",
    "metrics_calc = MetricsCalculator()\n",
    "model_metrics = {}\n",
    "\n",
    "for name, result in single_run_results.items():\n",
    "    if result.success:\n",
    "        metrics = metrics_calc.calculate_all_metrics(data_splits.y_test, result.predictions)\n",
    "        model_metrics[name] = metrics\n",
    "        print(f\"  {name.replace('_', ' ').title()}:\")\n",
    "        print(f\"    RMSE: {metrics['rmse']:.6f}\")\n",
    "        print(f\"    MAE:  {metrics['mae']:.6f}\")\n",
    "        print(f\"    MAPE: {metrics['mape']:.2f}%\")\n",
    "        print(f\"    Time: {result.execution_time:.2f}s\")\n",
    "    else:\n",
    "        print(f\"  {name.replace('_', ' ').title()}: FAILED\")\n",
    "        print(f\"    Error: {result.error_message}\")\n",
    "        print(f\"    Time: {result.execution_time:.2f}s\")\n",
    "\n",
    "# Find best model\n",
    "if model_metrics:\n",
    "    best_model = min(model_metrics.items(), key=lambda x: x[1]['rmse'])\n",
    "    print(f\"\\nüèÜ BEST MODEL: {best_model[0].replace('_', ' ').title()}\")\n",
    "    print(f\"    RMSE: {best_model[1]['rmse']:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Cell 5: Visualization\n",
    "# ============================================================================\n",
    "\n",
    "# Create visualizer\n",
    "visualizer = ResultsVisualizer(use_plotly=True)\n",
    "\n",
    "# Create comparison plot\n",
    "print(\"üìà Creating comparison plot...\")\n",
    "comparison_plot = visualizer.create_comparison_plot(\n",
    "    actual_values=data_splits.y_test,\n",
    "    model_results=single_run_results,\n",
    "    training_data=data_splits.y_train,\n",
    "    title=\"Time Series Forecasting Model Comparison\"\n",
    ")\n",
    "comparison_plot.show()\n",
    "\n",
    "# Create performance summary table\n",
    "performance_summary = visualizer.create_performance_summary(\n",
    "    actual_values=data_splits.y_test,\n",
    "    model_results=single_run_results\n",
    ")\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<h3>üìä Model Performance Summary</h3>\"))\n",
    "display(performance_summary)\n",
    "\n",
    "# Model diagnostics plot (if available)\n",
    "has_diagnostics = any(result.diagnostics for result in single_run_results.values() if result.success)\n",
    "if has_diagnostics:\n",
    "    print(\"\\nüîç Creating diagnostics plot...\")\n",
    "    diagnostics_plot = visualizer.create_model_diagnostics_plot(single_run_results)\n",
    "    diagnostics_plot.show()\n",
    "else:\n",
    "    print(\"\\nüìä No diagnostic data available for visualization\")\n",
    "\n",
    "# Residuals analysis for successful models\n",
    "successful_results = {name: result for name, result in single_run_results.items() if result.success}\n",
    "if len(successful_results) > 0:\n",
    "    print(f\"\\nüî¨ Models available for residuals analysis: {len(successful_results)}\")\n",
    "    print(\"Note: Residuals analysis can be added with visualizer.create_residuals_analysis()\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Cell 6: Rolling Window Validation\n",
    "# ============================================================================\n",
    "\n",
    "logger.info(\"üîÑ Starting rolling window validation...\")\n",
    "\n",
    "# Run rolling validation\n",
    "rolling_results = experiment.run_rolling_validation(\n",
    "    n_windows=config.rolling_windows,\n",
    "    parallel=config.parallel_execution\n",
    ")\n",
    "\n",
    "if not rolling_results.empty:\n",
    "    print(f\"\\nüîÑ ROLLING WINDOW VALIDATION RESULTS:\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Summary statistics by model\n",
    "    print(\"\\nüìä Summary by Model:\")\n",
    "    summary_stats = rolling_results.groupby('model_name').agg({\n",
    "        'rmse': ['mean', 'std', 'min', 'max'],\n",
    "        'mae': ['mean', 'std'],\n",
    "        'execution_time': ['mean', 'sum'],\n",
    "        'status': lambda x: f\"{(x == 'completed').sum()}/{len(x)}\"\n",
    "    }).round(6)\n",
    "    \n",
    "    print(summary_stats)\n",
    "    \n",
    "    # Create rolling validation plot\n",
    "    print(\"\\nüìà Creating rolling validation plot...\")\n",
    "    rolling_plot = visualizer.create_rolling_validation_plot(rolling_results)\n",
    "    rolling_plot.show()\n",
    "    \n",
    "    # Performance trend analysis\n",
    "    trends = experiment.validator.analyze_performance_trends(rolling_results)\n",
    "    \n",
    "    print(f\"\\nüìà PERFORMANCE TRENDS:\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    for model_name, trend_info in trends.get('performance_trends', {}).items():\n",
    "        trend = trend_info.get('trend', 'UNKNOWN')\n",
    "        degradation = trend_info.get('degradation_percent', 0)\n",
    "        windows_completed = trend_info.get('windows_completed', 0)\n",
    "        \n",
    "        print(f\"  {model_name.replace('_', ' ').title()}:\")\n",
    "        print(f\"    Trend: {trend} ({degradation:+.1f}%)\")\n",
    "        print(f\"    Windows completed: {windows_completed}/{config.rolling_windows}\")\n",
    "        \n",
    "        if trend in ['SEVERE', 'SIGNIFICANT']:\n",
    "            print(f\"    ‚ö†Ô∏è  Performance degradation detected!\")\n",
    "        elif trend == 'IMPROVING':\n",
    "            print(f\"    ‚úÖ Performance improving over time\")\n",
    "    \n",
    "    # Success rates\n",
    "    print(f\"\\nüìä SUCCESS RATES:\")\n",
    "    for model_name, success_info in trends.get('success_rate', {}).items():\n",
    "        rate = success_info['success_rate']\n",
    "        count = success_info['success_count']\n",
    "        total = success_info['total_count']\n",
    "        print(f\"  {model_name.replace('_', ' ').title()}: {rate:.1f}% ({count}/{total})\")\n",
    "\n",
    "else:\n",
    "    print(\"\\n‚ùå No rolling validation results available\")\n",
    "    print(\"This could be due to insufficient data or all models failing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Cell 7: Full Experiment & Analysis\n",
    "# ============================================================================\n",
    "\n",
    "# Run complete experiment with comprehensive logging\n",
    "experiment_name = f\"Complete_Model_Comparison_{pd.Timestamp.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "\n",
    "print(f\"üéØ Running full experiment: {experiment_name}\")\n",
    "full_results = experiment.run_full_experiment(\n",
    "    experiment_name=experiment_name,\n",
    "    include_rolling=True\n",
    ")\n",
    "\n",
    "# Display experiment summary\n",
    "summary = full_results.get('summary', {})\n",
    "\n",
    "print(f\"\\nüéâ EXPERIMENT SUMMARY:\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Experiment ID: {full_results['experiment_id']}\")\n",
    "print(f\"Name: {full_results['experiment_name']}\")\n",
    "print(f\"Status: {full_results.get('status', 'completed')}\")\n",
    "\n",
    "# Single run summary\n",
    "single_summary = summary.get('single_run_summary', {})\n",
    "if single_summary:\n",
    "    print(f\"\\nüìä Single Run Results:\")\n",
    "    print(f\"  Total models: {single_summary.get('total_models', 0)}\")\n",
    "    print(f\"  Successful: {single_summary.get('successful_models', 0)}\")\n",
    "    print(f\"  Failed: {single_summary.get('failed_models', 0)}\")\n",
    "    print(f\"  Best model: {single_summary.get('best_model', 'Unknown')}\")\n",
    "    print(f\"  Best RMSE: {single_summary.get('best_rmse', 'N/A')}\")\n",
    "\n",
    "# Rolling validation summary\n",
    "rolling_summary = summary.get('rolling_validation_summary', {})\n",
    "if rolling_summary:\n",
    "    print(f\"\\nüîÑ Rolling Validation Results:\")\n",
    "    print(f\"  Total windows: {rolling_summary.get('total_windows', 0)}\")\n",
    "    print(f\"  Models tested: {len(rolling_summary.get('models_tested', []))}\")\n",
    "    if 'best_model' in rolling_summary:\n",
    "        print(f\"  Best model (avg): {rolling_summary['best_model']}\")\n",
    "        print(f\"  Best avg RMSE: {rolling_summary.get('best_avg_rmse', 'N/A')}\")\n",
    "\n",
    "# Overall recommendation\n",
    "best_model = summary.get('overall_best_model')\n",
    "if best_model:\n",
    "    print(f\"\\nüèÜ OVERALL BEST MODEL: {best_model.replace('_', ' ').title()}\")\n",
    "\n",
    "# Recommendations\n",
    "recommendations = summary.get('recommendations', [])\n",
    "if recommendations:\n",
    "    print(f\"\\nüí° RECOMMENDATIONS:\")\n",
    "    for rec in recommendations:\n",
    "        print(f\"  {rec}\")\n",
    "\n",
    "print(f\"\\nüìã Results saved to database with experiment ID: {full_results['experiment_id']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Cell 8: Advanced Analysis & Historical Comparison\n",
    "# ============================================================================\n",
    "\n",
    "# Advanced model comparison\n",
    "print(\"üîç ADVANCED MODEL ANALYSIS:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "successful_results = {name: result for name, result in single_run_results.items() if result.success}\n",
    "if len(successful_results) > 1:\n",
    "    predictions_dict = {name: result.predictions for name, result in successful_results.items()}\n",
    "    \n",
    "    comparison = metrics_calc.compare_predictions(data_splits.y_test, predictions_dict)\n",
    "    \n",
    "    print(f\"\\nModels compared: {comparison['models']}\")\n",
    "    \n",
    "    # Detailed metrics comparison\n",
    "    print(f\"\\nüìä DETAILED METRICS COMPARISON:\")\n",
    "    for model_name, metrics in comparison['metrics_comparison'].items():\n",
    "        print(f\"\\n  {model_name.replace('_', ' ').title()}:\")\n",
    "        print(f\"    RMSE: {metrics.get('rmse', 'N/A'):.6f}\")\n",
    "        print(f\"    MAE:  {metrics.get('mae', 'N/A'):.6f}\")\n",
    "        print(f\"    MAPE: {metrics.get('mape', 'N/A'):.2f}%\")\n",
    "        print(f\"    R¬≤:   {metrics.get('r_squared', 'N/A'):.4f}\")\n",
    "        print(f\"    Correlation: {metrics.get('correlation', 'N/A'):.4f}\")\n",
    "    \n",
    "    # Model ranking\n",
    "    ranking = comparison.get('ranking', {})\n",
    "    if ranking:\n",
    "        print(f\"\\nüèÜ RANKING BY RMSE:\")\n",
    "        for i, entry in enumerate(ranking['by_rmse'], 1):\n",
    "            model_display = entry['model'].replace('_', ' ').title()\n",
    "            print(f\"  {i}. {model_display}: {entry['rmse']:.6f}\")\n",
    "\n",
    "# Compare with previous experiments\n",
    "print(f\"\\nüïí HISTORICAL COMPARISON:\")\n",
    "comparison = experiment.compare_with_previous_experiments(limit=5)\n",
    "if 'error' not in comparison:\n",
    "    print(f\"Previous experiments analyzed: {comparison['previous_experiments_count']}\")\n",
    "    \n",
    "    for rec in comparison.get('recommendations', []):\n",
    "        print(f\"  {rec}\")\n",
    "else:\n",
    "    print(f\"  {comparison['error']}\")\n",
    "\n",
    "# Data quality final assessment\n",
    "print(f\"\\nüìä FINAL DATA QUALITY ASSESSMENT:\")\n",
    "print(f\"Overall quality score: {quality_report['quality_score']:.1f}%\")\n",
    "\n",
    "target_stats = quality_report.get('target_column_stats', {})\n",
    "if target_stats:\n",
    "    print(f\"\\nTarget column ({config.target_column}) statistics:\")\n",
    "    print(f\"  Mean: {target_stats.get('mean', 'N/A'):.4f}\")\n",
    "    print(f\"  Std:  {target_stats.get('std', 'N/A'):.4f}\")\n",
    "    print(f\"  Range: {target_stats.get('min', 'N/A'):.4f} to {target_stats.get('max', 'N/A'):.4f}\")\n",
    "    print(f\"  Missing: {target_stats.get('missing_count', 'N/A')} values\")\n",
    "\n",
    "# Feature importance (if available from models)\n",
    "print(f\"\\nüîß MODEL CONFIGURATION SUMMARY:\")\n",
    "for model_name, result in single_run_results.items():\n",
    "    if result.success:\n",
    "        print(f\"\\n  {model_name.replace('_', ' ').title()}:\")\n",
    "        print(f\"    Hyperparameters: {result.hyperparameters}\")\n",
    "        if result.diagnostics:\n",
    "            print(f\"    Diagnostics available: {list(result.diagnostics.keys())}\")\n",
    "        if result.convergence_info:\n",
    "            converged = result.convergence_info.get('converged', True)\n",
    "            print(f\"    Convergence: {'‚úÖ Yes' if converged else '‚ö†Ô∏è Issues detected'}\")\n",
    "\n",
    "print(f\"\\nüéâ ANALYSIS COMPLETE!\")\n",
    "print(f\"\\nüíæ All results have been logged to the database.\")\n",
    "print(f\"üìä You can query the logs database for detailed historical analysis.\")\n",
    "\n",
    "logger.info(\"üî¨ Advanced analysis complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
