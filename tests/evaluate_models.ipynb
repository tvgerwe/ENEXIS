{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time Series Forecasting\n",
    "\n",
    "This notebook demonstrates the new modular architecture for time series forecasting experiments.\n",
    "\n",
    "## Features:\n",
    "- üîß Configuration-driven experiments\n",
    "- üìä Unified logging and metrics\n",
    "- üé® Interactive visualizations\n",
    "- üîÑ Rolling window validation\n",
    "- ‚ö° Parallel model execution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cel 1 - import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded: 1752 rows, 19 columns\n",
      "Date range: 2025-01-01 to 2025-03-14\n",
      "Price range: -0.0204 to 0.5235\n",
      "Data completeness: 100.0%\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import logging\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "import sqlite3\n",
    "import hashlib\n",
    "import json\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "logging.getLogger('build_training_set').setLevel(logging.ERROR)\n",
    "\n",
    "current_dir = Path.cwd()\n",
    "if \"ENEXIS\" in str(current_dir):\n",
    "    while current_dir.name != \"ENEXIS\" and current_dir.parent != current_dir:\n",
    "        current_dir = current_dir.parent\n",
    "    project_root = current_dir\n",
    "else:\n",
    "    project_root = current_dir\n",
    "\n",
    "src_path = project_root / \"src\"\n",
    "if str(src_path) not in sys.path:\n",
    "    sys.path.insert(0, str(src_path))\n",
    "\n",
    "from utils.build_training_set import build_training_set\n",
    "\n",
    "training_data = build_training_set(\n",
    "    train_start=\"2025-01-01 00:00:00\",\n",
    "    train_end=\"2025-03-14 23:00:00\",\n",
    "    run_date=\"2025-03-15 12:00:00\"\n",
    ")\n",
    "\n",
    "training_data = training_data.set_index('target_datetime')\n",
    "training_data.index = pd.to_datetime(training_data.index, utc=True)\n",
    "\n",
    "print(f\"Dataset loaded: {training_data.shape[0]} rows, {training_data.shape[1]} columns\")\n",
    "print(f\"Date range: {training_data.index.min().date()} to {training_data.index.max().date()}\")\n",
    "print(f\"Price range: {training_data['Price'].min():.4f} to {training_data['Price'].max():.4f}\")\n",
    "print(f\"Data completeness: {(1 - training_data['Price'].isna().sum() / len(training_data)) * 100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cell 2 - model config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using optimized parameters: order=(2, 0, 0), seasonal=(1, 1, 0, 24)\n",
      "Expected improvement: 30.1%\n",
      "Exogenous variables available: 18/18\n"
     ]
    }
   ],
   "source": [
    "EXOG_VARS = [\n",
    "    'Load', 'shortwave_radiation', 'temperature_2m', 'direct_normal_irradiance', \n",
    "    'diffuse_radiation', 'Flow_NO', 'yearday_cos', 'Flow_GB', 'month', 'is_dst', \n",
    "    'yearday_sin', 'is_non_working_day', 'hour_cos', 'is_weekend', 'cloud_cover', \n",
    "    'weekday_sin', 'hour_sin', 'weekday_cos'\n",
    "]\n",
    "\n",
    "available_exog = [col for col in EXOG_VARS if col in training_data.columns]\n",
    "\n",
    "config_file = project_root / \"src\" / \"config\" / \"best_sarimax_params.json\"\n",
    "if config_file.exists():\n",
    "    with open(config_file, 'r') as f:\n",
    "        best_params = json.load(f)\n",
    "    current_order = tuple(best_params['order'])\n",
    "    current_seasonal = tuple(best_params['seasonal_order'])\n",
    "    print(f\"Using optimized parameters: order={current_order}, seasonal={current_seasonal}\")\n",
    "    print(f\"Expected improvement: {best_params.get('improvement_vs_baseline', 0):.1f}%\")\n",
    "else:\n",
    "    current_order = (1, 0, 1)\n",
    "    current_seasonal = (1, 1, 1, 24)\n",
    "    print(f\"Using default parameters: order={current_order}, seasonal={current_seasonal}\")\n",
    "\n",
    "print(f\"Exogenous variables available: {len(available_exog)}/{len(EXOG_VARS)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cell 3 - 30 day rolling window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running 30-day rolling window validation...\n",
      "Day  1: 2025-03-15 | N:0.0211 | S:0.0190* | X:0.0152\n",
      "Day  2: 2025-03-16 | N:0.0211 | S:0.0190* | X:0.0150\n",
      "Day  3: 2025-03-17 | N:0.0197 | S:0.0185* | X:0.0157\n",
      "Day  4: 2025-03-18 | N:0.0215 | S:0.0187* | X:0.0146\n",
      "Day  5: 2025-03-19 | N:0.0243 | S:0.0202* | X:0.0156\n",
      "Day  6: 2025-03-20 | N:0.0189 | S:0.0191* | X:0.0159\n",
      "Day  7: 2025-03-21 | N:0.0222 | S:0.0190* | X:0.0177\n",
      "Day  8: 2025-03-22 | N:0.0290 | S:0.0196* | X:0.0170\n",
      "Day  9: 2025-03-23 | N:0.0247 | S:0.0210* | X:0.0166\n",
      "Day 10: 2025-03-24 | N:0.0231 | S:0.0179* | X:0.0164\n",
      "Day 11: 2025-03-25 | N:0.0194 | S:0.0175* | X:0.0175\n",
      "Day 12: 2025-03-26 | N:0.0252 | S:0.0224* | X:0.0220\n",
      "Day 13: 2025-03-27 | N:0.0278 | S:0.0187* | X:0.0166\n",
      "Day 14: 2025-03-28 | N:0.0196 | S:0.0193* | X:0.0228\n",
      "Day 15: 2025-03-29 | N:0.0344 | S:0.0227* | X:0.0248\n",
      "Day 16: 2025-03-30 | N:0.0267 | S:0.0235* | X:0.0227\n",
      "Day 17: 2025-03-31 | N:0.0317 | S:0.0253* | X:0.0204\n",
      "Day 18: 2025-04-01 | N:0.0258 | S:0.0236* | X:0.0238\n",
      "Day 19: 2025-04-02 | N:0.0173 | S:0.0172* | X:0.0223\n",
      "Day 20: 2025-04-03 | N:0.0226 | S:0.0227* | X:0.0238\n",
      "Day 21: 2025-04-04 | N:0.0285 | S:0.0252* | X:0.0238\n",
      "Day 22: 2025-04-05 | N:0.0400 | S:0.0341* | X:0.0297\n",
      "Day 23: 2025-04-06 | N:0.0297 | S:0.0266* | X:0.0287\n",
      "Day 24: 2025-04-07 | N:0.0255 | S:0.0257* | X:0.0216\n",
      "Day 25: 2025-04-08 | N:0.0327 | S:0.0397* | X:0.0364\n",
      "Day 26: 2025-04-09 | N:0.0279 | S:0.0270* | X:0.0209\n",
      "Day 27: 2025-04-10 | N:0.0532 | S:0.0326* | X:0.0218\n",
      "Day 28: 2025-04-11 | N:0.0338 | S:0.0338* | X:0.0341\n",
      "Day 29: 2025-04-12 | N:0.0445 | S:0.0349* | X:0.0314\n",
      "Day 30: 2025-04-13 | N:0.0419 | S:0.0332* | X:0.0304\n",
      "\n",
      "Validation completed in 384.6 seconds\n"
     ]
    }
   ],
   "source": [
    "from utils.validation_utils import run_validation_experiment\n",
    "\n",
    "print(\"Running 30-day rolling window validation...\")\n",
    "\n",
    "start_time = time.time()\n",
    "results_df = run_validation_experiment(training_data, EXOG_VARS, n_days=30)\n",
    "elapsed_time = time.time() - start_time\n",
    "\n",
    "for _, row in results_df.iterrows():\n",
    "    day = int(row['Day'])\n",
    "    test_date = row['Test_Date']\n",
    "    \n",
    "    if row.get('Status') in ['SPLIT_FAIL', 'LOAD_FAIL']:\n",
    "        status_str = row['Status']\n",
    "    else:\n",
    "        naive_str = f\"N:{row['Naive']:.4f}\" if not pd.isna(row['Naive']) else \"N:FAIL\"\n",
    "        sarima_str = f\"S:{row['SARIMA']:.4f}\" if not pd.isna(row['SARIMA']) else \"S:FAIL\"\n",
    "        if row.get('SARIMA_Fallback'):\n",
    "            sarima_str += \"*\"\n",
    "        sarimax_str = f\"X:{row['SARIMAX']:.4f}\" if not pd.isna(row['SARIMAX']) else \"X:FAIL\"\n",
    "        status_str = f\"{naive_str} | {sarima_str} | {sarimax_str}\"\n",
    "    \n",
    "    print(f\"Day {day:2d}: {test_date} | {status_str}\")\n",
    "\n",
    "print(f\"\\nValidation completed in {elapsed_time:.1f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cell 4 - performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Performance Summary:\n",
      "Naive   : Mean=0.027800 | Std=0.008388 | Range=[0.017342, 0.053230]\n",
      "SARIMA  : Mean=0.023933 | Std=0.006239 | Range=[0.017226, 0.039747]\n",
      "SARIMAX : Mean=0.021836 | Std=0.006040 | Range=[0.014638, 0.036371]\n",
      "\n",
      "Model Improvements:\n",
      "SARIMA vs Naive: 13.9% improvement\n",
      "SARIMAX vs Naive: 21.5% improvement\n",
      "SARIMAX vs SARIMA: 8.8% improvement\n",
      "\n",
      "Top 10 Most Important Features:\n",
      "   1. hour_sin                  | Coefficient: 0.847852\n",
      "   2. weekday_cos               | Coefficient: 0.261831\n",
      "   3. diffuse_radiation         | Coefficient: 0.092824\n",
      "   4. is_non_working_day        | Coefficient: 0.066030\n",
      "   5. is_dst                    | Coefficient: 0.048277\n",
      "   6. month                     | Coefficient: 0.034975\n",
      "   7. yearday_sin               | Coefficient: 0.008029\n",
      "   8. is_weekend                | Coefficient: 0.005488\n",
      "   9. cloud_cover               | Coefficient: 0.004569\n",
      "  10. yearday_cos               | Coefficient: 0.002627\n",
      "\n",
      "Feature Concentration:\n",
      "Top 5 features account for 95.8% of total impact\n",
      "\n",
      "Model Quality Metrics:\n",
      "SARIMAX AIC: -9600.82 | BIC: -9486.30\n",
      "\n",
      "Recommendation: SARIMAX is the best performing model\n"
     ]
    }
   ],
   "source": [
    "from utils.validation_utils import analyze_feature_contributions, generate_validation_summary\n",
    "\n",
    "feature_importance, aic, bic = analyze_feature_contributions(training_data, EXOG_VARS)\n",
    "summary = generate_validation_summary(results_df, feature_importance, aic, bic, EXOG_VARS)\n",
    "\n",
    "print(\"Model Performance Summary:\")\n",
    "for model, stats in summary['performance'].items():\n",
    "    print(f\"{model:8s}: Mean={stats['mean']:.6f} | Std={stats['std']:.6f} | Range=[{stats['min']:.6f}, {stats['max']:.6f}]\")\n",
    "\n",
    "print(f\"\\nModel Improvements:\")\n",
    "improvements = summary['improvements']\n",
    "print(f\"SARIMA vs Naive: {improvements['sarima_vs_naive']:.1f}% improvement\")\n",
    "print(f\"SARIMAX vs Naive: {improvements['sarimax_vs_naive']:.1f}% improvement\") \n",
    "print(f\"SARIMAX vs SARIMA: {improvements['sarimax_vs_sarima']:.1f}% improvement\")\n",
    "\n",
    "if feature_importance:\n",
    "    print(f\"\\nTop 10 Most Important Features:\")\n",
    "    for i, (var, coef) in enumerate(feature_importance[:10], 1):\n",
    "        print(f\"  {i:2d}. {var:<25} | Coefficient: {coef:.6f}\")\n",
    "    \n",
    "    print(f\"\\nFeature Concentration:\")\n",
    "    print(f\"Top 5 features account for {summary['feature_concentration']:.1f}% of total impact\")\n",
    "\n",
    "print(f\"\\nModel Quality Metrics:\")\n",
    "if aic and bic:\n",
    "    print(f\"SARIMAX AIC: {aic:.2f} | BIC: {bic:.2f}\")\n",
    "\n",
    "print(f\"\\nRecommendation: {summary['best_model']} is the best performing model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Auto-ARIMA optimization...\n",
      "üì¶ CACHED (1, 0, 1), (1, 1, 1, 24): RMSE=0.021836, Improvement=21.5%\n",
      "üß™ TESTING (2, 0, 1), (1, 1, 1, 24)...\n"
     ]
    }
   ],
   "source": [
    "def get_data_hash(training_data):\n",
    "    data_str = f\"{training_data.shape}_{training_data.iloc[0, 0]}_{training_data.iloc[-1, 0]}\"\n",
    "    return hashlib.md5(data_str.encode()).hexdigest()[:8]\n",
    "\n",
    "def check_existing_results(order, seasonal_order, log_db):\n",
    "    conn = sqlite3.connect(log_db)\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(\"\"\"\n",
    "        SELECT mean_rmse, improvement_vs_baseline, overall_score, created_at\n",
    "        FROM arima_validation_results \n",
    "        WHERE order_params = ? AND seasonal_order_params = ?\n",
    "        AND created_at > datetime('now', '-7 days')\n",
    "        ORDER BY created_at DESC LIMIT 1\n",
    "    \"\"\", (str(order), str(seasonal_order)))\n",
    "    result = cursor.fetchone()\n",
    "    conn.close()\n",
    "    return result\n",
    "\n",
    "def test_parameter_fast(order, seasonal_order, training_data, exog_vars, baseline_rmse):\n",
    "    from utils.validation_utils import run_validation_experiment\n",
    "    import utils.validation_utils as val_utils\n",
    "    \n",
    "    original_validation = val_utils.run_single_day_validation\n",
    "    \n",
    "    def modified_validation(day, training_data, exog_vars):\n",
    "        train_start_date = datetime(2025, 1, 1) + timedelta(days=day)\n",
    "        run_date = datetime(2025, 3, 15) + timedelta(days=day)\n",
    "        \n",
    "        try:\n",
    "            if day == 0:\n",
    "                daily_data = training_data.copy()\n",
    "            else:\n",
    "                daily_data = training_data.copy()\n",
    "                np.random.seed(day)\n",
    "                noise_factor = 0.001 * day\n",
    "                daily_data['Price'] = daily_data['Price'] + np.random.normal(0, noise_factor, len(daily_data))\n",
    "            \n",
    "            split_point = daily_data.index[-24]\n",
    "            train_data = daily_data[daily_data.index < split_point]['Price'].copy()\n",
    "            test_data = daily_data[daily_data.index >= split_point]['Price'].copy()\n",
    "            \n",
    "            day_results = {\n",
    "                'Day': day + 1,\n",
    "                'Test_Date': run_date.strftime('%Y-%m-%d'),\n",
    "                'Train_Samples': len(train_data),\n",
    "                'Test_Samples': len(test_data)\n",
    "            }\n",
    "            \n",
    "            if exog_vars:\n",
    "                train_exog = daily_data[daily_data.index < split_point][exog_vars].copy()\n",
    "                test_exog = daily_data[daily_data.index >= split_point][exog_vars].copy()\n",
    "                \n",
    "                with warnings.catch_warnings():\n",
    "                    warnings.simplefilter(\"ignore\")\n",
    "                    model = SARIMAX(\n",
    "                        train_data, exog=train_exog, order=order, seasonal_order=seasonal_order,\n",
    "                        enforce_stationarity=False, enforce_invertibility=False\n",
    "                    )\n",
    "                    fitted_model = model.fit(method='lbfgs', maxiter=15, disp=False)\n",
    "                    forecast = fitted_model.forecast(steps=len(test_data), exog=test_exog)\n",
    "                    rmse = np.sqrt(mean_squared_error(test_data, forecast))\n",
    "                    day_results['SARIMAX'] = rmse\n",
    "            \n",
    "            return day_results\n",
    "        except Exception:\n",
    "            return {'Day': day + 1, 'Status': 'FAIL'}\n",
    "    \n",
    "    val_utils.run_single_day_validation = modified_validation\n",
    "    \n",
    "    try:\n",
    "        results_df = val_utils.run_validation_experiment(training_data, exog_vars, n_days=5)\n",
    "        valid_results = results_df['SARIMAX'].dropna()\n",
    "        \n",
    "        if len(valid_results) > 0:\n",
    "            mean_rmse = valid_results.mean()\n",
    "            std_rmse = valid_results.std()\n",
    "            improvement = ((baseline_rmse - mean_rmse) / baseline_rmse) * 100\n",
    "            score = 1 / (1 + mean_rmse * 100) + 1 / (1 + std_rmse * 1000)\n",
    "            \n",
    "            return {\n",
    "                'order': order,\n",
    "                'seasonal_order': seasonal_order,\n",
    "                'mean_rmse': mean_rmse,\n",
    "                'std_rmse': std_rmse,\n",
    "                'improvement_vs_baseline': improvement,\n",
    "                'overall_score': score,\n",
    "                'success': True\n",
    "            }\n",
    "    finally:\n",
    "        val_utils.run_single_day_validation = original_validation\n",
    "    \n",
    "    return {'success': False}\n",
    "\n",
    "def run_optimization():\n",
    "    print(\"Starting Auto-ARIMA optimization...\")\n",
    "    \n",
    "    log_db = project_root / \"src\" / \"data\" / \"logs.db\"\n",
    "    log_db.parent.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    conn = sqlite3.connect(log_db)\n",
    "    conn.execute(\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS arima_validation_results (\n",
    "            id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "            order_params TEXT, seasonal_order_params TEXT, mean_rmse REAL, std_rmse REAL,\n",
    "            improvement_vs_baseline REAL, overall_score REAL, created_at TEXT\n",
    "        )\n",
    "    \"\"\")\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "    \n",
    "    current_best_rmse = summary['performance']['SARIMAX']['mean']\n",
    "    \n",
    "    parameter_combinations = [\n",
    "        ((1, 0, 1), (1, 1, 1, 24)),\n",
    "        ((2, 0, 1), (1, 1, 1, 24)),\n",
    "        ((1, 0, 2), (1, 1, 1, 24)),\n",
    "        ((2, 0, 0), (1, 1, 0, 24)),\n",
    "        ((1, 1, 1), (1, 1, 1, 24)),\n",
    "        ((2, 0, 2), (0, 1, 0, 24))\n",
    "    ]\n",
    "    \n",
    "    best_config = None\n",
    "    tested_count = 0\n",
    "    cached_count = 0\n",
    "    \n",
    "    for order, seasonal in parameter_combinations:\n",
    "        cached_result = check_existing_results(order, seasonal, log_db)\n",
    "        \n",
    "        if cached_result:\n",
    "            cached_count += 1\n",
    "            mean_rmse, improvement, score, created_at = cached_result\n",
    "            print(f\"üì¶ CACHED {order}, {seasonal}: RMSE={mean_rmse:.6f}, Improvement={improvement:.1f}%\")\n",
    "            \n",
    "            if not best_config or score > best_config['overall_score']:\n",
    "                best_config = {\n",
    "                    'order': order, 'seasonal_order': seasonal, 'mean_rmse': mean_rmse,\n",
    "                    'improvement_vs_baseline': improvement, 'overall_score': score\n",
    "                }\n",
    "        else:\n",
    "            tested_count += 1\n",
    "            print(f\"üß™ TESTING {order}, {seasonal}...\")\n",
    "            \n",
    "            result = test_parameter_fast(order, seasonal, training_data, EXOG_VARS, current_best_rmse)\n",
    "            \n",
    "            if result['success']:\n",
    "                print(f\"‚úÖ RMSE={result['mean_rmse']:.6f}, Improvement={result['improvement_vs_baseline']:.1f}%\")\n",
    "                \n",
    "                conn = sqlite3.connect(log_db)\n",
    "                conn.execute(\"\"\"\n",
    "                    INSERT INTO arima_validation_results \n",
    "                    (order_params, seasonal_order_params, mean_rmse, std_rmse, \n",
    "                     improvement_vs_baseline, overall_score, created_at)\n",
    "                    VALUES (?, ?, ?, ?, ?, ?, ?)\n",
    "                \"\"\", (str(order), str(seasonal), result['mean_rmse'], result['std_rmse'],\n",
    "                      result['improvement_vs_baseline'], result['overall_score'], \n",
    "                      datetime.utcnow().isoformat()))\n",
    "                conn.commit()\n",
    "                conn.close()\n",
    "                \n",
    "                if not best_config or result['overall_score'] > best_config['overall_score']:\n",
    "                    best_config = result\n",
    "            else:\n",
    "                print(\"‚ùå Test failed\")\n",
    "    \n",
    "    print(f\"\\nOptimization complete: {cached_count} cached, {tested_count} new tests\")\n",
    "    \n",
    "    if best_config:\n",
    "        config_file = project_root / \"src\" / \"config\" / \"best_sarimax_params.json\"\n",
    "        config_file.parent.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        config_data = {\n",
    "            'order': best_config['order'],\n",
    "            'seasonal_order': best_config['seasonal_order'],\n",
    "            'mean_rmse': best_config['mean_rmse'],\n",
    "            'improvement_vs_baseline': best_config.get('improvement_vs_baseline', 0),\n",
    "            'overall_score': best_config.get('overall_score', 0),\n",
    "            'updated_at': datetime.utcnow().isoformat()\n",
    "        }\n",
    "        \n",
    "        with open(config_file, 'w') as f:\n",
    "            json.dump(config_data, f, indent=2)\n",
    "        \n",
    "        print(f\"\\nBest configuration found:\")\n",
    "        print(f\"Order: {best_config['order']}\")\n",
    "        print(f\"Seasonal: {best_config['seasonal_order']}\")\n",
    "        print(f\"RMSE: {best_config['mean_rmse']:.6f}\")\n",
    "        print(f\"Improvement: {best_config.get('improvement_vs_baseline', 0):.1f}%\")\n",
    "        print(f\"Config saved to: {config_file}\")\n",
    "    \n",
    "    return best_config\n",
    "\n",
    "optimization_result = run_optimization()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# FORCE FRESH OPTIMIZATION - Ignore cache and run everything fresh\n",
    "# ============================================================================\n",
    "\n",
    "def run_fresh_optimization(training_data, exog_vars, force_auto_arima=True):\n",
    "    \"\"\"Run optimization ignoring cache - for testing new approaches\"\"\"\n",
    "    \n",
    "    print(\"üî• FORCING FRESH OPTIMIZATION (IGNORING CACHE)\")\n",
    "    print(\"=\"*60)\n",
    "    print(\"‚ö†Ô∏è  This will take 3-5 minutes and test everything fresh\")\n",
    "    \n",
    "    from pathlib import Path\n",
    "    import time\n",
    "    \n",
    "    PROJECT_ROOT = Path.cwd()\n",
    "    if \"ENEXIS\" in str(PROJECT_ROOT):\n",
    "        while PROJECT_ROOT.name != \"ENEXIS\" and PROJECT_ROOT.parent != PROJECT_ROOT:\n",
    "            PROJECT_ROOT = PROJECT_ROOT.parent\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Get baseline\n",
    "    baseline_rmse = 0.025\n",
    "    \n",
    "    # Test fewer combinations but fresh\n",
    "    parameter_combinations = [\n",
    "        ((1, 0, 1), (1, 1, 1, 24)),  # Current standard\n",
    "        ((2, 0, 1), (1, 1, 1, 24)),  # More AR\n",
    "        ((1, 0, 2), (1, 1, 1, 24)),  # More MA\n",
    "        ((1, 1, 1), (1, 1, 1, 24)),  # With differencing\n",
    "        ((2, 0, 0), (1, 1, 0, 24)),  # AR only\n",
    "    ]\n",
    "    \n",
    "    print(f\"üß™ Testing {len(parameter_combinations)} fresh combinations...\")\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    # Test each combination fresh (no cache lookup)\n",
    "    for i, (order, seasonal) in enumerate(parameter_combinations, 1):\n",
    "        print(f\"\\nüß™ {i}/{len(parameter_combinations)}: Testing {order}, {seasonal}\")\n",
    "        \n",
    "        # Force fresh test by modifying the test function\n",
    "        args = (order, seasonal, training_data, exog_vars, baseline_rmse, \"FRESH\")\n",
    "        \n",
    "        # Use our fast test but without cache checking\n",
    "        try:\n",
    "            from utils.validation_utils import run_validation_experiment\n",
    "            import utils.validation_utils as val_utils\n",
    "            \n",
    "            # Modified validation for this specific test\n",
    "            original_validation = val_utils.run_single_day_validation\n",
    "            \n",
    "            def test_validation(day, training_data, exog_vars):\n",
    "                from datetime import datetime, timedelta\n",
    "                import warnings\n",
    "                from sklearn.metrics import mean_squared_error\n",
    "                from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "                \n",
    "                train_start_date = datetime(2025, 1, 1) + timedelta(days=day)\n",
    "                run_date = datetime(2025, 3, 15) + timedelta(days=day)\n",
    "                \n",
    "                try:\n",
    "                    if day == 0:\n",
    "                        daily_data = training_data.copy()\n",
    "                    else:\n",
    "                        daily_data = training_data.copy()\n",
    "                        np.random.seed(day)\n",
    "                        noise_factor = 0.001 * day\n",
    "                        daily_data['Price'] = daily_data['Price'] + np.random.normal(0, noise_factor, len(daily_data))\n",
    "                    \n",
    "                    split_point = daily_data.index[-24]\n",
    "                    train_data = daily_data[daily_data.index < split_point]['Price'].copy()\n",
    "                    test_data = daily_data[daily_data.index >= split_point]['Price'].copy()\n",
    "                    \n",
    "                    day_results = {\n",
    "                        'Day': day + 1,\n",
    "                        'Test_Date': run_date.strftime('%Y-%m-%d'),\n",
    "                        'Train_Samples': len(train_data),\n",
    "                        'Test_Samples': len(test_data)\n",
    "                    }\n",
    "                    \n",
    "                    # Test with our specific parameters\n",
    "                    if exog_vars:\n",
    "                        train_exog = daily_data[daily_data.index < split_point][exog_vars].copy()\n",
    "                        test_exog = daily_data[daily_data.index >= split_point][exog_vars].copy()\n",
    "                        \n",
    "                        with warnings.catch_warnings():\n",
    "                            warnings.simplefilter(\"ignore\")\n",
    "                            model = SARIMAX(\n",
    "                                train_data,\n",
    "                                exog=train_exog,\n",
    "                                order=order,\n",
    "                                seasonal_order=seasonal,\n",
    "                                enforce_stationarity=False,\n",
    "                                enforce_invertibility=False\n",
    "                            )\n",
    "                            fitted_model = model.fit(method='lbfgs', maxiter=20, disp=False)\n",
    "                            forecast = fitted_model.forecast(steps=len(test_data), exog=test_exog)\n",
    "                            rmse = np.sqrt(mean_squared_error(test_data, forecast))\n",
    "                            day_results['SARIMAX'] = rmse\n",
    "                    \n",
    "                    return day_results\n",
    "                    \n",
    "                except Exception:\n",
    "                    return {'Day': day + 1, 'Status': 'FAIL'}\n",
    "            \n",
    "            val_utils.run_single_day_validation = test_validation\n",
    "            \n",
    "            test_start = time.time()\n",
    "            results_df = val_utils.run_validation_experiment(training_data, exog_vars, n_days=7)  # 7 days\n",
    "            test_time = time.time() - test_start\n",
    "            \n",
    "            val_utils.run_single_day_validation = original_validation\n",
    "            \n",
    "            valid_results = results_df['SARIMAX'].dropna()\n",
    "            \n",
    "            if len(valid_results) > 0:\n",
    "                mean_rmse = valid_results.mean()\n",
    "                std_rmse = valid_results.std()\n",
    "                improvement = ((baseline_rmse - mean_rmse) / baseline_rmse) * 100\n",
    "                score = 1 / (1 + mean_rmse * 100) + 1 / (1 + std_rmse * 1000)\n",
    "                \n",
    "                result = {\n",
    "                    'order': order,\n",
    "                    'seasonal_order': seasonal,\n",
    "                    'mean_rmse': mean_rmse,\n",
    "                    'std_rmse': std_rmse,\n",
    "                    'improvement_vs_baseline': improvement,\n",
    "                    'overall_score': score,\n",
    "                    'test_time': test_time\n",
    "                }\n",
    "                \n",
    "                results.append(result)\n",
    "                print(f\"‚úÖ RMSE: {mean_rmse:.6f} | Improvement: {improvement:.1f}% | Time: {test_time:.1f}s\")\n",
    "            else:\n",
    "                print(\"‚ùå Test failed\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error: {e}\")\n",
    "    \n",
    "    # Auto-ARIMA phase\n",
    "    if force_auto_arima:\n",
    "        print(f\"\\nü§ñ Running fresh Auto-ARIMA...\")\n",
    "        auto_start = time.time()\n",
    "        \n",
    "        try:\n",
    "            from pmdarima import auto_arima\n",
    "            \n",
    "            split_point = training_data.index[-24]\n",
    "            train_data = training_data[training_data.index < split_point]['Price'].dropna()\n",
    "            \n",
    "            model = auto_arima(\n",
    "                train_data,\n",
    "                seasonal=True,\n",
    "                m=24,\n",
    "                stepwise=True,\n",
    "                max_p=2,\n",
    "                max_q=2,\n",
    "                max_P=1, \n",
    "                max_Q=1,\n",
    "                suppress_warnings=True,\n",
    "                error_action=\"warn\"\n",
    "            )\n",
    "            \n",
    "            auto_time = time.time() - auto_start\n",
    "            auto_order = model.order\n",
    "            auto_seasonal = model.seasonal_order\n",
    "            \n",
    "            print(f\"ü§ñ Auto-ARIMA result: {auto_order}, {auto_seasonal} (took {auto_time:.1f}s)\")\n",
    "            \n",
    "            # Test auto-ARIMA result\n",
    "            print(\"üß™ Testing Auto-ARIMA suggestion...\")\n",
    "            # [Test auto-ARIMA the same way as above]\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Auto-ARIMA failed: {e}\")\n",
    "    \n",
    "    total_time = time.time() - start_time\n",
    "    \n",
    "    print(f\"\\nüèÜ FRESH OPTIMIZATION RESULTS:\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    if results:\n",
    "        best_result = min(results, key=lambda x: x['mean_rmse'])\n",
    "        \n",
    "        print(f\"Best Configuration:\")\n",
    "        print(f\"  Order: {best_result['order']}\")\n",
    "        print(f\"  Seasonal: {best_result['seasonal_order']}\")\n",
    "        print(f\"  RMSE: {best_result['mean_rmse']:.6f}\")\n",
    "        print(f\"  Improvement: {best_result['improvement_vs_baseline']:.1f}%\")\n",
    "        \n",
    "        print(f\"\\nAll Results:\")\n",
    "        for r in sorted(results, key=lambda x: x['mean_rmse']):\n",
    "            print(f\"  {r['order']}, {r['seasonal_order']}: RMSE={r['mean_rmse']:.6f}, Time={r['test_time']:.1f}s\")\n",
    "    \n",
    "    print(f\"\\n‚è±Ô∏è Total fresh optimization time: {total_time:.1f} seconds\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Run fresh optimization\n",
    "print(\"Choose your optimization mode:\")\n",
    "print(\"1. üî• FRESH (ignore all cache, ~3-5 minutes)\")  \n",
    "print(\"2. ‚ö° SMART (use cache where possible, ~30 seconds)\")\n",
    "\n",
    "mode = input(\"Enter 1 or 2: \").strip()\n",
    "\n",
    "if mode == \"1\":\n",
    "    fresh_results = run_fresh_optimization(training_data, EXOG_VARS, force_auto_arima=True)\n",
    "else:\n",
    "    print(\"Running smart optimization with cache...\")\n",
    "    # Run the previous cached version"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
