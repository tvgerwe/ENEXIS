{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time Series Forecasting - Refactored Architecture\n",
    "\n",
    "This notebook demonstrates the new modular architecture for time series forecasting experiments.\n",
    "\n",
    "## Features:\n",
    "- üîß Configuration-driven experiments\n",
    "- üìä Unified logging and metrics\n",
    "- üé® Interactive visualizations\n",
    "- üîÑ Rolling window validation\n",
    "- ‚ö° Parallel model execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-25 13:46:27,680 - __main__ - INFO - üìö All imports loaded successfully\n",
      "2025-05-25 13:46:27,681 - build_training_set - INFO - üöÄ Start build van trainingset\n",
      "2025-05-25 13:46:27,682 - build_training_set - INFO - üß† Actuals van 2025-01-01 00:00:00+00:00 t/m 2025-03-14 23:00:00+00:00\n",
      "2025-05-25 13:46:27,682 - build_training_set - INFO - üìÖ Forecast van run_date 2025-03-15 00:00:00+00:00, target range: 2025-03-15 00:00:00+00:00 ‚Üí 2025-03-21 23:00:00+00:00\n",
      "2025-05-25 13:46:27,707 - build_training_set - INFO - ‚úÖ Actuals geladen: 1752 rijen\n",
      "2025-05-25 13:46:27,783 - build_training_set - INFO - üì¶ Eindtabel bevat: 1920 rijen, 35 kolommen\n",
      "2025-05-25 13:46:27,783 - build_training_set - INFO - üßæ Kolommen: ['hour', 'day_of_week', 'month', 'day_of_year', 'hour_sin', 'hour_cos', 'weekday_sin', 'weekday_cos', 'yearday_sin', 'yearday_cos', 'local_datetime', 'is_dst', 'is_holiday', 'is_weekend', 'is_non_working_day', 'target_datetime', 'Load', 'Price', 'Flow_BE', 'Flow_DE', 'Flow_GB', 'Flow_DK', 'Flow_NO', 'Total_Flow', 'temperature_2m', 'wind_speed_10m', 'apparent_temperature', 'cloud_cover', 'snowfall', 'diffuse_radiation', 'direct_normal_irradiance', 'shortwave_radiation', 'run_date', 'wind_direction_10m', 'direct_radiation']\n",
      "2025-05-25 13:46:27,792 - build_training_set - INFO - ‚úÖ Opgeslagen als training_set in WARP.db\n",
      "2025-05-25 13:46:27,793 - build_training_set - INFO - üîí Verbinding gesloten\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Cell 1: Configuration and Imports\n",
    "# ============================================================================\n",
    "\n",
    "import sys\n",
    "import logging\n",
    "from pathlib import Path\n",
    "\n",
    "# Setup paths\n",
    "current_dir = Path.cwd()\n",
    "if \"ENEXIS\" in str(current_dir):\n",
    "    while current_dir.name != \"ENEXIS\" and current_dir.parent != current_dir:\n",
    "        current_dir = current_dir.parent\n",
    "    project_root = current_dir\n",
    "else:\n",
    "    project_root = current_dir\n",
    "\n",
    "src_path = project_root / \"src\"\n",
    "if str(src_path) not in sys.path:\n",
    "    sys.path.insert(0, str(src_path))\n",
    "\n",
    "# Import directly from files to avoid __init__.py issues\n",
    "from config.experiment_config import ExperimentConfig\n",
    "from core.data_manager import DataManager\n",
    "from core.logging_manager import ExperimentLogger\n",
    "\n",
    "# Import specific files instead of modules\n",
    "from core.experiment import TimeSeriesExperiment\n",
    "from models.factory import ModelFactory\n",
    "from evaluation.metrics import MetricsCalculator\n",
    "from visualization.results import ResultsVisualizer\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\"\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "logger.info(\"üìö All imports loaded successfully\")\n",
    "\n",
    "from utils.build_training_set import build_training_set\n",
    "\n",
    "build_training_set(\n",
    "    train_start=\"2025-01-01 00:00:00\",\n",
    "    train_end=\"2025-03-14 23:00:00\",\n",
    "    run_date=\"2025-03-15 00:00:00\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-25 13:43:54,258 - __main__ - INFO - üîß Components initialized\n",
      "2025-05-25 13:43:54,259 - __main__ - INFO - üìä Loaded 3 model configurations\n",
      "2025-05-25 13:43:54,259 - __main__ - INFO - üéØ Target: Price\n",
      "2025-05-25 13:43:54,259 - __main__ - INFO - üìÖ Period: 2025-01-01 00:00:00+00:00 to 2025-03-21 23:00:00+00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ü§ñ MODEL CONFIGURATION:\n",
      "Total models: 3\n",
      "Enabled: ['naive', 'sarimax_no_exog', 'sarimax_with_exog']\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Cell 2: Initialize Components\n",
    "# ============================================================================\n",
    "\n",
    "# Option 1: Load from YAML config file (recommended)\n",
    "# config = ExperimentConfig.from_file(\"config/experiment_config.yaml\")\n",
    "\n",
    "# Option 2: Use default configuration (for quick testing)\n",
    "config = ExperimentConfig()\n",
    "\n",
    "# Initialize core components\n",
    "data_manager = DataManager(config)\n",
    "experiment_logger = ExperimentLogger(config.logs_database_path)\n",
    "model_factory = ModelFactory(config.model_configs)\n",
    "\n",
    "logger.info(\"üîß Components initialized\")\n",
    "logger.info(f\"üìä Loaded {len(config.model_configs)} model configurations\")\n",
    "logger.info(f\"üéØ Target: {config.target_column}\")\n",
    "logger.info(f\"üìÖ Period: {config.train_start} to {config.forecast_end}\")\n",
    "\n",
    "# Display model information\n",
    "model_info = model_factory.get_model_info()\n",
    "print(f\"\\nü§ñ MODEL CONFIGURATION:\")\n",
    "print(f\"Total models: {model_info['total_models']}\")\n",
    "print(f\"Enabled: {model_info['enabled_models']}\")\n",
    "if model_info['disabled_models']:\n",
    "    print(f\"Disabled: {model_info['disabled_models']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-25 13:43:54,268 - DataManager - INFO - üóëÔ∏è Data cache cleared\n",
      "2025-05-25 13:43:54,274 - DataManager - INFO - ‚úÖ Loaded training_set: 1920 rows, 35 columns\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÖ ACTUAL DATA RANGE:\n",
      "Min date: 2025-01-01 00:00:00+00:00\n",
      "Max date: 2025-05-30 23:00:00+00:00\n",
      "Total days: 149\n",
      "\n",
      "‚öôÔ∏è CURRENT CONFIG:\n",
      "Train start: 2025-01-01 00:00:00+00:00\n",
      "Train end: 2025-03-14 23:00:00+00:00\n",
      "Forecast start: 2025-03-15 00:00:00+00:00\n",
      "Forecast end: 2025-03-21 23:00:00+00:00\n",
      "Horizon: 168\n",
      "\n",
      "‚úÖ UPDATED CONFIG (using last week for forecast):\n",
      "Train start: 2025-01-01 00:00:00+00:00\n",
      "Train end: 2025-05-23 22:00:00+00:00\n",
      "Forecast start: 2025-05-23 23:00:00+00:00\n",
      "Forecast period: 169 hours (7.0 days)\n",
      "\n",
      "‚ùå Still failed to create splits: No test data found between 2025-05-23 23:00:00+00:00 and 2025-05-30 23:00:00+00:00\n",
      "üîß Trying manual date override...\n",
      "‚ùå Manual override also failed: No test data found between 2025-05-23 23:00:00+00:00 and 2025-05-30 22:00:00+00:00\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "Could not create data splits with any date configuration",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 58\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 58\u001b[0m     data_splits \u001b[38;5;241m=\u001b[39m \u001b[43mdata_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_splits\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     59\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m‚úÖ Data splits created successfully\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/ENEXIS/src/core/data_manager.py:153\u001b[0m, in \u001b[0;36mDataManager.create_splits\u001b[0;34m(self, train_start, train_end, forecast_start, forecast_horizon, use_training_set)\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(y_test) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 153\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo test data found between \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mforecast_start\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mforecast_end\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    155\u001b[0m split \u001b[38;5;241m=\u001b[39m DataSplit(\n\u001b[1;32m    156\u001b[0m     y_train\u001b[38;5;241m=\u001b[39my_train,\n\u001b[1;32m    157\u001b[0m     X_train\u001b[38;5;241m=\u001b[39mX_train,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    163\u001b[0m     forecast_end\u001b[38;5;241m=\u001b[39mforecast_end\n\u001b[1;32m    164\u001b[0m )\n",
      "\u001b[0;31mValueError\u001b[0m: No test data found between 2025-05-23 23:00:00+00:00 and 2025-05-30 23:00:00+00:00",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 80\u001b[0m\n\u001b[1;32m     78\u001b[0m manual_train_end \u001b[38;5;241m=\u001b[39m manual_forecast_start \u001b[38;5;241m-\u001b[39m pd\u001b[38;5;241m.\u001b[39mTimedelta(hours\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 80\u001b[0m data_splits \u001b[38;5;241m=\u001b[39m \u001b[43mdata_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_splits\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     81\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmin_date\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     82\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_end\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmanual_train_end\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     83\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforecast_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmanual_forecast_start\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     84\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforecast_horizon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m168\u001b[39;49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# 7 days in hours\u001b[39;49;00m\n\u001b[1;32m     85\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     87\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m‚úÖ Manual data splits created successfully\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/ENEXIS/src/core/data_manager.py:153\u001b[0m, in \u001b[0;36mDataManager.create_splits\u001b[0;34m(self, train_start, train_end, forecast_start, forecast_horizon, use_training_set)\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(y_test) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 153\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo test data found between \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mforecast_start\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mforecast_end\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    155\u001b[0m split \u001b[38;5;241m=\u001b[39m DataSplit(\n\u001b[1;32m    156\u001b[0m     y_train\u001b[38;5;241m=\u001b[39my_train,\n\u001b[1;32m    157\u001b[0m     X_train\u001b[38;5;241m=\u001b[39mX_train,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    163\u001b[0m     forecast_end\u001b[38;5;241m=\u001b[39mforecast_end\n\u001b[1;32m    164\u001b[0m )\n",
      "\u001b[0;31mValueError\u001b[0m: No test data found between 2025-05-23 23:00:00+00:00 and 2025-05-30 22:00:00+00:00",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 98\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m manual_error:\n\u001b[1;32m     97\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m‚ùå Manual override also failed: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmanual_error\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 98\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not create data splits with any date configuration\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    100\u001b[0m \u001b[38;5;66;03m# If we get here, data_splits is working\u001b[39;00m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124müìä DATA READY FOR MODELING!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mException\u001b[0m: Could not create data splits with any date configuration"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Cell 3: Fix Config Dates - Work Around Read-Only Properties\n",
    "# ============================================================================\n",
    "\n",
    "# Database is working, let's check actual date range and work with what we can set\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "\n",
    "with sqlite3.connect(config.database_path) as conn:\n",
    "    date_query = \"SELECT MIN(target_datetime) as min_date, MAX(target_datetime) as max_date FROM master_warp\"\n",
    "    date_info = pd.read_sql(date_query, conn)\n",
    "    min_date = pd.to_datetime(date_info['min_date'].iloc[0])\n",
    "    max_date = pd.to_datetime(date_info['max_date'].iloc[0])\n",
    "    \n",
    "    print(f\"üìÖ ACTUAL DATA RANGE:\")\n",
    "    print(f\"Min date: {min_date}\")\n",
    "    print(f\"Max date: {max_date}\")\n",
    "    print(f\"Total days: {(max_date - min_date).days}\")\n",
    "\n",
    "# Check what we can actually modify in config\n",
    "print(f\"\\n‚öôÔ∏è CURRENT CONFIG:\")\n",
    "print(f\"Train start: {config.train_start}\")\n",
    "print(f\"Train end: {config.train_end}\")\n",
    "print(f\"Forecast start: {config.forecast_start}\")\n",
    "if hasattr(config, 'forecast_end'):\n",
    "    print(f\"Forecast end: {config.forecast_end}\")\n",
    "print(f\"Horizon: {config.horizon}\")\n",
    "\n",
    "# Try to update what we can\n",
    "try:\n",
    "    # Use a forecast period that exists in your data (last week of data)\n",
    "    forecast_start = max_date - pd.Timedelta(days=7)\n",
    "    forecast_end = max_date\n",
    "    \n",
    "    config.train_start = min_date\n",
    "    config.train_end = forecast_start - pd.Timedelta(hours=1)\n",
    "    config.forecast_start = forecast_start\n",
    "    \n",
    "    # Calculate horizon in hours\n",
    "    horizon_hours = int((forecast_end - forecast_start).total_seconds() / 3600) + 1\n",
    "    config.horizon = horizon_hours\n",
    "    \n",
    "    print(f\"\\n‚úÖ UPDATED CONFIG (using last week for forecast):\")\n",
    "    print(f\"Train start: {config.train_start}\")\n",
    "    print(f\"Train end: {config.train_end}\")\n",
    "    print(f\"Forecast start: {config.forecast_start}\")\n",
    "    print(f\"Forecast period: {horizon_hours} hours ({(horizon_hours/24):.1f} days)\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Could not update config: {e}\")\n",
    "    print(\"Using original config dates - might need manual adjustment\")\n",
    "\n",
    "# Clear cache\n",
    "data_manager.clear_cache()\n",
    "\n",
    "# Try with current config\n",
    "try:\n",
    "    data_splits = data_manager.create_splits()\n",
    "    logger.info(\"‚úÖ Data splits created successfully\")\n",
    "    \n",
    "    # Get split info\n",
    "    split_info = data_splits.get_info()\n",
    "    print(f\"\\nüîÑ SUCCESSFUL DATA SPLITS:\")\n",
    "    print(f\"Training samples: {split_info['train_samples']}\")\n",
    "    print(f\"Test samples: {split_info['test_samples']}\")\n",
    "    print(f\"Training period: {split_info['train_period']}\")\n",
    "    print(f\"Forecast period: {split_info['forecast_period']}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå Still failed to create splits: {e}\")\n",
    "    \n",
    "    # Try to create splits manually with good dates\n",
    "    print(\"üîß Trying manual date override...\")\n",
    "    \n",
    "    try:\n",
    "        # Override the create_splits call with manual dates\n",
    "        manual_forecast_start = max_date - pd.Timedelta(days=7)\n",
    "        manual_train_end = manual_forecast_start - pd.Timedelta(hours=1)\n",
    "        \n",
    "        data_splits = data_manager.create_splits(\n",
    "            train_start=min_date,\n",
    "            train_end=manual_train_end,\n",
    "            forecast_start=manual_forecast_start,\n",
    "            forecast_horizon=168  # 7 days in hours\n",
    "        )\n",
    "        \n",
    "        logger.info(\"‚úÖ Manual data splits created successfully\")\n",
    "        \n",
    "        split_info = data_splits.get_info()\n",
    "        print(f\"\\nüîÑ MANUAL DATA SPLITS:\")\n",
    "        print(f\"Training samples: {split_info['train_samples']}\")\n",
    "        print(f\"Test samples: {split_info['test_samples']}\")\n",
    "        print(f\"Training period: {split_info['train_period']}\")\n",
    "        print(f\"Forecast period: {split_info['forecast_period']}\")\n",
    "        \n",
    "    except Exception as manual_error:\n",
    "        print(f\"‚ùå Manual override also failed: {manual_error}\")\n",
    "        raise Exception(\"Could not create data splits with any date configuration\")\n",
    "\n",
    "# If we get here, data_splits is working\n",
    "print(f\"\\nüìä DATA READY FOR MODELING!\")\n",
    "print(f\"Target column: {config.target_column}\")\n",
    "print(f\"Ready to proceed to next cells\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Cell 4: Run Single Experiments\n",
    "# ============================================================================\n",
    "\n",
    "# Initialize experiment\n",
    "experiment = TimeSeriesExperiment(config, data_manager, experiment_logger)\n",
    "\n",
    "# Run single experiment\n",
    "logger.info(\"üöÄ Starting single experiment...\")\n",
    "single_run_results = experiment.run_single_experiment(data_splits)\n",
    "\n",
    "# Display results summary\n",
    "print(\"\\nüìä SINGLE EXPERIMENT RESULTS:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "successful_models = [name for name, result in single_run_results.items() if result.success]\n",
    "failed_models = [name for name, result in single_run_results.items() if not result.success]\n",
    "\n",
    "print(f\"‚úÖ Successful models: {len(successful_models)}\")\n",
    "print(f\"‚ùå Failed models: {len(failed_models)}\")\n",
    "\n",
    "# Calculate and display metrics\n",
    "metrics_calc = MetricsCalculator()\n",
    "model_metrics = {}\n",
    "\n",
    "for name, result in single_run_results.items():\n",
    "    if result.success:\n",
    "        metrics = metrics_calc.calculate_all_metrics(data_splits.y_test, result.predictions)\n",
    "        model_metrics[name] = metrics\n",
    "        print(f\"  {name.replace('_', ' ').title()}:\")\n",
    "        print(f\"    RMSE: {metrics['rmse']:.6f}\")\n",
    "        print(f\"    MAE:  {metrics['mae']:.6f}\")\n",
    "        print(f\"    MAPE: {metrics['mape']:.2f}%\")\n",
    "        print(f\"    Time: {result.execution_time:.2f}s\")\n",
    "    else:\n",
    "        print(f\"  {name.replace('_', ' ').title()}: FAILED\")\n",
    "        print(f\"    Error: {result.error_message}\")\n",
    "        print(f\"    Time: {result.execution_time:.2f}s\")\n",
    "\n",
    "# Find best model\n",
    "if model_metrics:\n",
    "    best_model = min(model_metrics.items(), key=lambda x: x[1]['rmse'])\n",
    "    print(f\"\\nüèÜ BEST MODEL: {best_model[0].replace('_', ' ').title()}\")\n",
    "    print(f\"    RMSE: {best_model[1]['rmse']:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Cell 5: Visualization\n",
    "# ============================================================================\n",
    "\n",
    "# Create visualizer\n",
    "visualizer = ResultsVisualizer(use_plotly=True)\n",
    "\n",
    "# Create comparison plot\n",
    "print(\"üìà Creating comparison plot...\")\n",
    "comparison_plot = visualizer.create_comparison_plot(\n",
    "    actual_values=data_splits.y_test,\n",
    "    model_results=single_run_results,\n",
    "    training_data=data_splits.y_train,\n",
    "    title=\"Time Series Forecasting Model Comparison\"\n",
    ")\n",
    "comparison_plot.show()\n",
    "\n",
    "# Create performance summary table\n",
    "performance_summary = visualizer.create_performance_summary(\n",
    "    actual_values=data_splits.y_test,\n",
    "    model_results=single_run_results\n",
    ")\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<h3>üìä Model Performance Summary</h3>\"))\n",
    "display(performance_summary)\n",
    "\n",
    "# Model diagnostics plot (if available)\n",
    "has_diagnostics = any(result.diagnostics for result in single_run_results.values() if result.success)\n",
    "if has_diagnostics:\n",
    "    print(\"\\nüîç Creating diagnostics plot...\")\n",
    "    diagnostics_plot = visualizer.create_model_diagnostics_plot(single_run_results)\n",
    "    diagnostics_plot.show()\n",
    "else:\n",
    "    print(\"\\nüìä No diagnostic data available for visualization\")\n",
    "\n",
    "# Residuals analysis for successful models\n",
    "successful_results = {name: result for name, result in single_run_results.items() if result.success}\n",
    "if len(successful_results) > 0:\n",
    "    print(f\"\\nüî¨ Models available for residuals analysis: {len(successful_results)}\")\n",
    "    print(\"Note: Residuals analysis can be added with visualizer.create_residuals_analysis()\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Cell 6: Rolling Window Validation\n",
    "# ============================================================================\n",
    "\n",
    "logger.info(\"üîÑ Starting rolling window validation...\")\n",
    "\n",
    "# Run rolling validation\n",
    "rolling_results = experiment.run_rolling_validation(\n",
    "    n_windows=config.rolling_windows,\n",
    "    parallel=config.parallel_execution\n",
    ")\n",
    "\n",
    "if not rolling_results.empty:\n",
    "    print(f\"\\nüîÑ ROLLING WINDOW VALIDATION RESULTS:\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Summary statistics by model\n",
    "    print(\"\\nüìä Summary by Model:\")\n",
    "    summary_stats = rolling_results.groupby('model_name').agg({\n",
    "        'rmse': ['mean', 'std', 'min', 'max'],\n",
    "        'mae': ['mean', 'std'],\n",
    "        'execution_time': ['mean', 'sum'],\n",
    "        'status': lambda x: f\"{(x == 'completed').sum()}/{len(x)}\"\n",
    "    }).round(6)\n",
    "    \n",
    "    print(summary_stats)\n",
    "    \n",
    "    # Create rolling validation plot\n",
    "    print(\"\\nüìà Creating rolling validation plot...\")\n",
    "    rolling_plot = visualizer.create_rolling_validation_plot(rolling_results)\n",
    "    rolling_plot.show()\n",
    "    \n",
    "    # Performance trend analysis\n",
    "    trends = experiment.validator.analyze_performance_trends(rolling_results)\n",
    "    \n",
    "    print(f\"\\nüìà PERFORMANCE TRENDS:\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    for model_name, trend_info in trends.get('performance_trends', {}).items():\n",
    "        trend = trend_info.get('trend', 'UNKNOWN')\n",
    "        degradation = trend_info.get('degradation_percent', 0)\n",
    "        windows_completed = trend_info.get('windows_completed', 0)\n",
    "        \n",
    "        print(f\"  {model_name.replace('_', ' ').title()}:\")\n",
    "        print(f\"    Trend: {trend} ({degradation:+.1f}%)\")\n",
    "        print(f\"    Windows completed: {windows_completed}/{config.rolling_windows}\")\n",
    "        \n",
    "        if trend in ['SEVERE', 'SIGNIFICANT']:\n",
    "            print(f\"    ‚ö†Ô∏è  Performance degradation detected!\")\n",
    "        elif trend == 'IMPROVING':\n",
    "            print(f\"    ‚úÖ Performance improving over time\")\n",
    "    \n",
    "    # Success rates\n",
    "    print(f\"\\nüìä SUCCESS RATES:\")\n",
    "    for model_name, success_info in trends.get('success_rate', {}).items():\n",
    "        rate = success_info['success_rate']\n",
    "        count = success_info['success_count']\n",
    "        total = success_info['total_count']\n",
    "        print(f\"  {model_name.replace('_', ' ').title()}: {rate:.1f}% ({count}/{total})\")\n",
    "\n",
    "else:\n",
    "    print(\"\\n‚ùå No rolling validation results available\")\n",
    "    print(\"This could be due to insufficient data or all models failing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Cell 7: Full Experiment & Analysis\n",
    "# ============================================================================\n",
    "\n",
    "# Run complete experiment with comprehensive logging\n",
    "experiment_name = f\"Complete_Model_Comparison_{pd.Timestamp.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "\n",
    "print(f\"üéØ Running full experiment: {experiment_name}\")\n",
    "full_results = experiment.run_full_experiment(\n",
    "    experiment_name=experiment_name,\n",
    "    include_rolling=True\n",
    ")\n",
    "\n",
    "# Display experiment summary\n",
    "summary = full_results.get('summary', {})\n",
    "\n",
    "print(f\"\\nüéâ EXPERIMENT SUMMARY:\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Experiment ID: {full_results['experiment_id']}\")\n",
    "print(f\"Name: {full_results['experiment_name']}\")\n",
    "print(f\"Status: {full_results.get('status', 'completed')}\")\n",
    "\n",
    "# Single run summary\n",
    "single_summary = summary.get('single_run_summary', {})\n",
    "if single_summary:\n",
    "    print(f\"\\nüìä Single Run Results:\")\n",
    "    print(f\"  Total models: {single_summary.get('total_models', 0)}\")\n",
    "    print(f\"  Successful: {single_summary.get('successful_models', 0)}\")\n",
    "    print(f\"  Failed: {single_summary.get('failed_models', 0)}\")\n",
    "    print(f\"  Best model: {single_summary.get('best_model', 'Unknown')}\")\n",
    "    print(f\"  Best RMSE: {single_summary.get('best_rmse', 'N/A')}\")\n",
    "\n",
    "# Rolling validation summary\n",
    "rolling_summary = summary.get('rolling_validation_summary', {})\n",
    "if rolling_summary:\n",
    "    print(f\"\\nüîÑ Rolling Validation Results:\")\n",
    "    print(f\"  Total windows: {rolling_summary.get('total_windows', 0)}\")\n",
    "    print(f\"  Models tested: {len(rolling_summary.get('models_tested', []))}\")\n",
    "    if 'best_model' in rolling_summary:\n",
    "        print(f\"  Best model (avg): {rolling_summary['best_model']}\")\n",
    "        print(f\"  Best avg RMSE: {rolling_summary.get('best_avg_rmse', 'N/A')}\")\n",
    "\n",
    "# Overall recommendation\n",
    "best_model = summary.get('overall_best_model')\n",
    "if best_model:\n",
    "    print(f\"\\nüèÜ OVERALL BEST MODEL: {best_model.replace('_', ' ').title()}\")\n",
    "\n",
    "# Recommendations\n",
    "recommendations = summary.get('recommendations', [])\n",
    "if recommendations:\n",
    "    print(f\"\\nüí° RECOMMENDATIONS:\")\n",
    "    for rec in recommendations:\n",
    "        print(f\"  {rec}\")\n",
    "\n",
    "print(f\"\\nüìã Results saved to database with experiment ID: {full_results['experiment_id']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Cell 8: Advanced Analysis & Historical Comparison\n",
    "# ============================================================================\n",
    "\n",
    "# Advanced model comparison\n",
    "print(\"üîç ADVANCED MODEL ANALYSIS:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "successful_results = {name: result for name, result in single_run_results.items() if result.success}\n",
    "if len(successful_results) > 1:\n",
    "    predictions_dict = {name: result.predictions for name, result in successful_results.items()}\n",
    "    \n",
    "    comparison = metrics_calc.compare_predictions(data_splits.y_test, predictions_dict)\n",
    "    \n",
    "    print(f\"\\nModels compared: {comparison['models']}\")\n",
    "    \n",
    "    # Detailed metrics comparison\n",
    "    print(f\"\\nüìä DETAILED METRICS COMPARISON:\")\n",
    "    for model_name, metrics in comparison['metrics_comparison'].items():\n",
    "        print(f\"\\n  {model_name.replace('_', ' ').title()}:\")\n",
    "        print(f\"    RMSE: {metrics.get('rmse', 'N/A'):.6f}\")\n",
    "        print(f\"    MAE:  {metrics.get('mae', 'N/A'):.6f}\")\n",
    "        print(f\"    MAPE: {metrics.get('mape', 'N/A'):.2f}%\")\n",
    "        print(f\"    R¬≤:   {metrics.get('r_squared', 'N/A'):.4f}\")\n",
    "        print(f\"    Correlation: {metrics.get('correlation', 'N/A'):.4f}\")\n",
    "    \n",
    "    # Model ranking\n",
    "    ranking = comparison.get('ranking', {})\n",
    "    if ranking:\n",
    "        print(f\"\\nüèÜ RANKING BY RMSE:\")\n",
    "        for i, entry in enumerate(ranking['by_rmse'], 1):\n",
    "            model_display = entry['model'].replace('_', ' ').title()\n",
    "            print(f\"  {i}. {model_display}: {entry['rmse']:.6f}\")\n",
    "\n",
    "# Compare with previous experiments\n",
    "print(f\"\\nüïí HISTORICAL COMPARISON:\")\n",
    "comparison = experiment.compare_with_previous_experiments(limit=5)\n",
    "if 'error' not in comparison:\n",
    "    print(f\"Previous experiments analyzed: {comparison['previous_experiments_count']}\")\n",
    "    \n",
    "    for rec in comparison.get('recommendations', []):\n",
    "        print(f\"  {rec}\")\n",
    "else:\n",
    "    print(f\"  {comparison['error']}\")\n",
    "\n",
    "# Data quality final assessment\n",
    "print(f\"\\nüìä FINAL DATA QUALITY ASSESSMENT:\")\n",
    "print(f\"Overall quality score: {quality_report['quality_score']:.1f}%\")\n",
    "\n",
    "target_stats = quality_report.get('target_column_stats', {})\n",
    "if target_stats:\n",
    "    print(f\"\\nTarget column ({config.target_column}) statistics:\")\n",
    "    print(f\"  Mean: {target_stats.get('mean', 'N/A'):.4f}\")\n",
    "    print(f\"  Std:  {target_stats.get('std', 'N/A'):.4f}\")\n",
    "    print(f\"  Range: {target_stats.get('min', 'N/A'):.4f} to {target_stats.get('max', 'N/A'):.4f}\")\n",
    "    print(f\"  Missing: {target_stats.get('missing_count', 'N/A')} values\")\n",
    "\n",
    "# Feature importance (if available from models)\n",
    "print(f\"\\nüîß MODEL CONFIGURATION SUMMARY:\")\n",
    "for model_name, result in single_run_results.items():\n",
    "    if result.success:\n",
    "        print(f\"\\n  {model_name.replace('_', ' ').title()}:\")\n",
    "        print(f\"    Hyperparameters: {result.hyperparameters}\")\n",
    "        if result.diagnostics:\n",
    "            print(f\"    Diagnostics available: {list(result.diagnostics.keys())}\")\n",
    "        if result.convergence_info:\n",
    "            converged = result.convergence_info.get('converged', True)\n",
    "            print(f\"    Convergence: {'‚úÖ Yes' if converged else '‚ö†Ô∏è Issues detected'}\")\n",
    "\n",
    "print(f\"\\nüéâ ANALYSIS COMPLETE!\")\n",
    "print(f\"\\nüíæ All results have been logged to the database.\")\n",
    "print(f\"üìä You can query the logs database for detailed historical analysis.\")\n",
    "\n",
    "logger.info(\"üî¨ Advanced analysis complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.10.13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
