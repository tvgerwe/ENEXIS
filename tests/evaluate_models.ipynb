{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time Series Forecasting\n",
    "\n",
    "This notebook demonstrates the new modular architecture for time series forecasting experiments.\n",
    "\n",
    "## Features:\n",
    "- üîß Configuration-driven experiments\n",
    "- üìä Unified logging and metrics\n",
    "- üé® Interactive visualizations\n",
    "- üîÑ Rolling window validation\n",
    "- ‚ö° Parallel model execution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cel 1 - import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-30 12:13:45,690 - build_training_set - INFO - üöÄ Start build van trainingset\n",
      "2025-05-30 12:13:45,691 - build_training_set - INFO - üß† Actuals van 2025-01-01 00:00:00+00:00 t/m 2025-03-14 23:00:00+00:00\n",
      "2025-05-30 12:13:45,691 - build_training_set - INFO - üìÖ Forecast van run_date 2025-03-15 12:00:00+00:00, target range: 2025-03-15 12:00:00+00:00 ‚Üí 2025-03-22 11:00:00+00:00\n",
      "2025-05-30 12:13:45,692 - build_training_set - INFO - üì• Loading actuals with selected columns only...\n",
      "2025-05-30 12:13:45,694 - build_training_set - INFO - üìã Requested columns found: 20/20\n",
      "2025-05-30 12:13:45,694 - build_training_set - INFO - üìã Using columns: ['Price', 'target_datetime', 'Load', 'shortwave_radiation', 'temperature_2m', 'direct_normal_irradiance', 'diffuse_radiation', 'Flow_NO', 'yearday_cos', 'Flow_GB', 'month', 'is_dst', 'yearday_sin', 'is_non_working_day', 'hour_cos', 'is_weekend', 'cloud_cover', 'weekday_sin', 'hour_sin', 'weekday_cos']\n",
      "2025-05-30 12:13:45,735 - build_training_set - INFO - ‚úÖ Actuals loaded: 1752 rows with 20 selected columns\n",
      "2025-05-30 12:13:45,740 - build_training_set - INFO - üîç Checking for forecast data...\n",
      "2025-05-30 12:13:45,744 - build_training_set - INFO - üìä Forecast rows available: 0\n",
      "2025-05-30 12:13:45,746 - build_training_set - INFO - üì¶ Final table: 1752 rows, 20 columns\n",
      "2025-05-30 12:13:45,747 - build_training_set - INFO - üßæ Final columns: ['Price', 'target_datetime', 'Load', 'shortwave_radiation', 'temperature_2m', 'direct_normal_irradiance', 'diffuse_radiation', 'Flow_NO', 'yearday_cos', 'Flow_GB', 'month', 'is_dst', 'yearday_sin', 'is_non_working_day', 'hour_cos', 'is_weekend', 'cloud_cover', 'weekday_sin', 'hour_sin', 'weekday_cos']\n",
      "2025-05-30 12:13:45,747 - build_training_set - INFO - üí∞ Price NaN count: 0/1752 (0.0%)\n",
      "2025-05-30 12:13:45,749 - build_training_set - INFO - ‚úÖ All columns have good data quality (<20% NaN)\n",
      "2025-05-30 12:13:45,756 - build_training_set - INFO - ‚úÖ Saved as training_set in WARP.db\n",
      "2025-05-30 12:13:45,757 - build_training_set - INFO - üîí Connection closed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: 1752 rows, ends 2025-03-14\n",
      "Validation: 2025-03-15 to 2025-04-14 (30 days)\n"
     ]
    }
   ],
   "source": [
    "# Cell 1 - Imports & Dynamic Validation Setup\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "from datetime import datetime, timedelta\n",
    "import sqlite3\n",
    "import json\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Setup paths\n",
    "current_dir = Path.cwd()\n",
    "if \"ENEXIS\" in str(current_dir):\n",
    "    while current_dir.name != \"ENEXIS\" and current_dir.parent != current_dir:\n",
    "        current_dir = current_dir.parent\n",
    "    project_root = current_dir\n",
    "else:\n",
    "    project_root = current_dir\n",
    "\n",
    "sys.path.insert(0, str(project_root / \"src\"))\n",
    "from utils.build_training_set import build_training_set\n",
    "\n",
    "# Load training data\n",
    "training_data = build_training_set(\n",
    "    train_start=\"2025-01-01 00:00:00\",\n",
    "    train_end=\"2025-03-14 23:00:00\",\n",
    "    run_date=\"2025-03-15 12:00:00\"\n",
    ").set_index('target_datetime')\n",
    "training_data.index = pd.to_datetime(training_data.index, utc=True)\n",
    "\n",
    "def load_validation_data(start_date: str, end_date: str) -> pd.DataFrame:\n",
    "    \"\"\"Load actual price data from master_warp table\"\"\"\n",
    "    db_path = project_root / \"src\" / \"data\" / \"WARP.db\"\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    \n",
    "    query = \"SELECT target_datetime, Price FROM master_warp WHERE target_datetime >= ? AND target_datetime <= ? ORDER BY target_datetime\"\n",
    "    validation_data = pd.read_sql_query(query, conn, params=[start_date, end_date], parse_dates=['target_datetime'])\n",
    "    conn.close()\n",
    "    \n",
    "    if len(validation_data) > 0:\n",
    "        validation_data = validation_data.set_index('target_datetime')\n",
    "        validation_data.index = pd.to_datetime(validation_data.index, utc=True)\n",
    "        return validation_data\n",
    "    return None\n",
    "\n",
    "# Calculate validation periods\n",
    "train_end = training_data.index.max()\n",
    "validation_start = train_end + timedelta(hours=1)\n",
    "validation_end = validation_start + timedelta(days=30)\n",
    "\n",
    "print(f\"Training: {training_data.shape[0]} rows, ends {train_end.date()}\")\n",
    "print(f\"Validation: {validation_start.date()} to {validation_end.date()} (30 days)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cell 2 - model config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using optimized: order=(2, 0, 0), seasonal=(1, 1, 0, 24)\n",
      "Exog variables: 18/18\n"
     ]
    }
   ],
   "source": [
    "# Cell 2 - Model Configuration\n",
    "\n",
    "EXOG_VARS = [\n",
    "    'Load', 'shortwave_radiation', 'temperature_2m', 'direct_normal_irradiance', \n",
    "    'diffuse_radiation', 'Flow_NO', 'yearday_cos', 'Flow_GB', 'month', 'is_dst', \n",
    "    'yearday_sin', 'is_non_working_day', 'hour_cos', 'is_weekend', 'cloud_cover', \n",
    "    'weekday_sin', 'hour_sin', 'weekday_cos'\n",
    "]\n",
    "\n",
    "available_exog = [col for col in EXOG_VARS if col in training_data.columns]\n",
    "\n",
    "# Load your optimized SARIMAX parameters\n",
    "config_file = project_root / \"src\" / \"config\" / \"best_sarimax_params.json\"\n",
    "if config_file.exists():\n",
    "    with open(config_file, 'r') as f:\n",
    "        best_params = json.load(f)\n",
    "    current_order = tuple(best_params['order'])\n",
    "    current_seasonal = tuple(best_params['seasonal_order'])\n",
    "    print(f\"Using optimized: order={current_order}, seasonal={current_seasonal}\")\n",
    "else:\n",
    "    # Fallback defaults\n",
    "    current_order = (1, 0, 1)\n",
    "    current_seasonal = (1, 1, 1, 24)\n",
    "    print(f\"Using defaults: order={current_order}, seasonal={current_seasonal}\")\n",
    "\n",
    "print(f\"Exog variables: {len(available_exog)}/{len(EXOG_VARS)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cell 3 - 30 day rolling window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting MINIMAL 30-day validation...\n",
      "Running MINIMAL 30-day validation (forecast horizon: 7 days)...\n",
      "Focus: Maximum speed with basic model validation\n",
      "============================================================\n",
      "Day  1: 2025-03-15 | N:0.0617 | S:0.0611 | X:0.0533\n",
      "Day  2: 2025-03-16 | N:0.0610 | S:0.0604 | X:0.0524\n",
      "Day  3: 2025-03-17 | N:0.0596 | S:0.0589 | X:0.0501\n",
      "Day  4: 2025-03-18 | N:0.0568 | S:0.0561 | X:0.0479\n",
      "Day  5: 2025-03-19 | N:0.0536 | S:0.0529 | X:0.0424\n",
      "Day  6: 2025-03-20 | N:0.0517 | S:0.0513 | X:0.0344\n",
      "Day  7: 2025-03-21 | "
     ]
    }
   ],
   "source": [
    "# Cell 3 - Minimal 30-Day Validation (Maximum Speed)\n",
    "\n",
    "import time\n",
    "\n",
    "def run_minimal_30day_validation(training_data, exog_vars, forecast_days=7):\n",
    "    \"\"\"Minimal validation - prioritizes speed over model sophistication\"\"\"\n",
    "    \n",
    "    print(f\"Running MINIMAL 30-day validation (forecast horizon: {forecast_days} days)...\")\n",
    "    print(\"Focus: Maximum speed with basic model validation\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    results = []\n",
    "    train_end = training_data.index.max()\n",
    "    \n",
    "    # Pre-cache some common calculations\n",
    "    full_train_data = training_data['Price'].values\n",
    "    full_train_index = training_data.index\n",
    "    \n",
    "    for day in range(1, 31):\n",
    "        forecast_start = train_end + timedelta(days=day)\n",
    "        forecast_end = forecast_start + timedelta(days=forecast_days-1, hours=23)\n",
    "        \n",
    "        print(f\"Day {day:2d}: {forecast_start.date()}\", end=\" | \")\n",
    "        \n",
    "        # Load validation data for this day only\n",
    "        validation_data = load_validation_data(\n",
    "            forecast_start.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "            forecast_end.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        )\n",
    "        \n",
    "        if validation_data is None or len(validation_data) == 0:\n",
    "            print(\"NO_DATA\")\n",
    "            results.append({\n",
    "                'Day': day, 'Date': forecast_start.date(),\n",
    "                'Naive': np.nan, 'SARIMA': np.nan, 'SARIMAX': np.nan,\n",
    "                'Status': 'NO_DATA'\n",
    "            })\n",
    "            continue\n",
    "        \n",
    "        # Get training data up to forecast start - FIXED LOGIC\n",
    "        train_cutoff = forecast_start - timedelta(hours=1)\n",
    "        \n",
    "        # Use ALL available training data up to cutoff (not just recent)\n",
    "        available_train = training_data[training_data.index <= train_cutoff]\n",
    "        \n",
    "        if len(available_train) < 168:  # Need at least 1 week\n",
    "            print(\"INSUFFICIENT\")\n",
    "            results.append({\n",
    "                'Day': day, 'Date': forecast_start.date(),\n",
    "                'Naive': np.nan, 'SARIMA': np.nan, 'SARIMAX': np.nan,\n",
    "                'Status': 'INSUFFICIENT'\n",
    "            })\n",
    "            continue\n",
    "        \n",
    "        # For speed: use only last 336 hours (2 weeks) of available training data\n",
    "        if len(available_train) > 336:\n",
    "            recent_train_data = available_train.iloc[-336:]\n",
    "        else:\n",
    "            recent_train_data = available_train\n",
    "        y_train = recent_train_data['Price'].values\n",
    "        y_actual = validation_data['Price'].values\n",
    "        forecast_hours = len(y_actual)\n",
    "        \n",
    "        result = {'Day': day, 'Date': forecast_start.date(), 'Status': 'SUCCESS'}\n",
    "        \n",
    "        # 1. NAIVE MODEL (very fast)\n",
    "        try:\n",
    "            if len(y_train) >= 24:\n",
    "                # Simple daily seasonality\n",
    "                daily_pattern = y_train[-24:]\n",
    "                naive_forecast = np.tile(daily_pattern, (forecast_hours // 24) + 1)[:forecast_hours]\n",
    "            else:\n",
    "                naive_forecast = np.full(forecast_hours, y_train[-1])\n",
    "            \n",
    "            result['Naive'] = np.sqrt(mean_squared_error(y_actual, naive_forecast))\n",
    "        except:\n",
    "            result['Naive'] = np.nan\n",
    "        \n",
    "        # 2. SARIMA MODEL (using YOUR optimized parameters)\n",
    "        try:\n",
    "            with warnings.catch_warnings():\n",
    "                warnings.simplefilter(\"ignore\")\n",
    "                \n",
    "                # Use YOUR optimized parameters: order=(2,0,0), seasonal=(1,1,0,24)\n",
    "                sarima_model = SARIMAX(\n",
    "                    y_train,\n",
    "                    order=current_order,  # Your optimized (2,0,0)\n",
    "                    seasonal_order=current_seasonal,  # Your optimized (1,1,0,24)\n",
    "                    enforce_stationarity=False,\n",
    "                    enforce_invertibility=False\n",
    "                )\n",
    "                \n",
    "                # Fast fitting with reduced iterations\n",
    "                fitted = sarima_model.fit(method='lbfgs', maxiter=20, disp=False)\n",
    "                forecast = fitted.forecast(steps=forecast_hours)\n",
    "                result['SARIMA'] = np.sqrt(mean_squared_error(y_actual, forecast))\n",
    "        except:\n",
    "            # Fallback to simple ARIMA if your parameters fail\n",
    "            try:\n",
    "                from statsmodels.tsa.arima.model import ARIMA\n",
    "                simple_model = ARIMA(y_train, order=(1, 0, 1))\n",
    "                fitted = simple_model.fit(method='css', maxiter=10)\n",
    "                forecast = fitted.forecast(steps=forecast_hours)\n",
    "                result['SARIMA'] = np.sqrt(mean_squared_error(y_actual, forecast))\n",
    "            except:\n",
    "                result['SARIMA'] = np.nan\n",
    "\n",
    "        # 3. SARIMAX MODEL (using YOUR optimized parameters + ALL features)\n",
    "        try:\n",
    "            with warnings.catch_warnings():\n",
    "                warnings.simplefilter(\"ignore\")\n",
    "                \n",
    "                # Get exogenous data\n",
    "                X_train = recent_train_data[exog_vars].values\n",
    "                \n",
    "                # Use YOUR optimized SARIMAX parameters\n",
    "                sarimax_model = SARIMAX(\n",
    "                    y_train,\n",
    "                    exog=X_train,\n",
    "                    order=current_order,  # Your optimized (2,0,0)\n",
    "                    seasonal_order=current_seasonal,  # Your optimized (1,1,0,24)\n",
    "                    enforce_stationarity=False,\n",
    "                    enforce_invertibility=False\n",
    "                )\n",
    "                \n",
    "                # Fast fitting\n",
    "                fitted = sarimax_model.fit(method='lbfgs', maxiter=20, disp=False)\n",
    "                \n",
    "                # Create future exogenous variables (weekly pattern)\n",
    "                X_future = np.zeros((forecast_hours, len(exog_vars)))\n",
    "                recent_X = X_train[-168:] if len(X_train) >= 168 else X_train[-24:]\n",
    "                \n",
    "                for i in range(forecast_hours):\n",
    "                    X_future[i] = recent_X[i % len(recent_X)]\n",
    "                \n",
    "                forecast = fitted.forecast(steps=forecast_hours, exog=X_future)\n",
    "                result['SARIMAX'] = np.sqrt(mean_squared_error(y_actual, forecast))\n",
    "                \n",
    "        except:\n",
    "            # Fallback to LinearRegression if SARIMAX fails\n",
    "            try:\n",
    "                from sklearn.linear_model import LinearRegression\n",
    "                X_train = recent_train_data[exog_vars].values\n",
    "                lr_model = LinearRegression().fit(X_train, y_train)\n",
    "                X_future = np.tile(X_train[-24:], ((forecast_hours // 24) + 1, 1))[:forecast_hours]\n",
    "                lr_forecast = lr_model.predict(X_future)\n",
    "                result['SARIMAX'] = np.sqrt(mean_squared_error(y_actual, lr_forecast))\n",
    "            except:\n",
    "                result['SARIMAX'] = np.nan\n",
    "        \n",
    "        # Print results\n",
    "        naive_str = f\"N:{result['Naive']:.4f}\" if not pd.isna(result['Naive']) else \"N:FAIL\"\n",
    "        sarima_str = f\"S:{result['SARIMA']:.4f}\" if not pd.isna(result['SARIMA']) else \"S:FAIL\"\n",
    "        sarimax_str = f\"X:{result['SARIMAX']:.4f}\" if not pd.isna(result['SARIMAX']) else \"X:FAIL\"\n",
    "        \n",
    "        print(f\"{naive_str} | {sarima_str} | {sarimax_str}\")\n",
    "        \n",
    "        results.append(result)\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Alternative: Even simpler version if above is still slow\n",
    "def run_super_simple_validation(training_data, exog_vars, forecast_days=7):\n",
    "    \"\"\"Super simple validation - only naive and linear models\"\"\"\n",
    "    \n",
    "    print(\"Running SUPER SIMPLE validation (Naive + Linear only)...\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    results = []\n",
    "    train_end = training_data.index.max()\n",
    "    \n",
    "    for day in range(1, 31):\n",
    "        forecast_start = train_end + timedelta(days=day)\n",
    "        forecast_end = forecast_start + timedelta(days=forecast_days-1, hours=23)\n",
    "        \n",
    "        print(f\"Day {day:2d}: {forecast_start.date()}\", end=\" | \")\n",
    "        \n",
    "        validation_data = load_validation_data(\n",
    "            forecast_start.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "            forecast_end.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        )\n",
    "        \n",
    "        if validation_data is None or len(validation_data) == 0:\n",
    "            print(\"NO_DATA\")\n",
    "            continue\n",
    "        \n",
    "        # Get last week of training data\n",
    "        train_cutoff = forecast_start - timedelta(hours=1)\n",
    "        recent_train = training_data[\n",
    "            training_data.index > (train_cutoff - timedelta(hours=168))\n",
    "        ][training_data.index <= train_cutoff]\n",
    "        \n",
    "        if len(recent_train) < 24:\n",
    "            print(\"INSUFFICIENT\")\n",
    "            continue\n",
    "        \n",
    "        y_actual = validation_data['Price'].values\n",
    "        forecast_hours = len(y_actual)\n",
    "        \n",
    "        # Naive model\n",
    "        try:\n",
    "            daily_pattern = recent_train['Price'].values[-24:]\n",
    "            naive_forecast = np.tile(daily_pattern, (forecast_hours // 24) + 1)[:forecast_hours]\n",
    "            naive_rmse = np.sqrt(mean_squared_error(y_actual, naive_forecast))\n",
    "        except:\n",
    "            naive_rmse = np.nan\n",
    "        \n",
    "        # Simple linear model\n",
    "        try:\n",
    "            from sklearn.linear_model import LinearRegression\n",
    "            X = recent_train[exog_vars].values\n",
    "            y = recent_train['Price'].values\n",
    "            \n",
    "            lr = LinearRegression().fit(X, y)\n",
    "            X_future = np.tile(X[-24:], ((forecast_hours // 24) + 1, 1))[:forecast_hours]\n",
    "            lr_forecast = lr.predict(X_future)\n",
    "            lr_rmse = np.sqrt(mean_squared_error(y_actual, lr_forecast))\n",
    "        except:\n",
    "            lr_rmse = np.nan\n",
    "        \n",
    "        print(f\"N:{naive_rmse:.4f} | L:{lr_rmse:.4f}\")\n",
    "        \n",
    "        results.append({\n",
    "            'Day': day,\n",
    "            'Date': forecast_start.date(),\n",
    "            'Naive': naive_rmse,\n",
    "            'Linear': lr_rmse,\n",
    "            'Status': 'SUCCESS'\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Try the minimal version first\n",
    "print(\"üöÄ Starting MINIMAL 30-day validation...\")\n",
    "start_time = time.time()\n",
    "\n",
    "try:\n",
    "    results_df = run_minimal_30day_validation(training_data, available_exog, forecast_days=7)\n",
    "    elapsed_time = time.time() - start_time\n",
    "    \n",
    "    if elapsed_time > 300:  # If still taking > 5 minutes\n",
    "        print(f\"\\n‚ö† Still slow ({elapsed_time:.1f}s), trying SUPER SIMPLE version...\")\n",
    "        results_df = run_super_simple_validation(training_data, available_exog, forecast_days=7)\n",
    "        elapsed_time = time.time() - start_time\n",
    "    \n",
    "    print(f\"\\n‚úÖ Validation completed in {elapsed_time:.1f} seconds\")\n",
    "    \n",
    "    # Quick summary\n",
    "    if len(results_df) > 0:\n",
    "        successful = results_df[results_df['Status'] == 'SUCCESS']\n",
    "        print(f\"\\nQuick Results ({len(successful)} successful validations):\")\n",
    "        \n",
    "        for col in ['Naive', 'SARIMA', 'SARIMAX', 'Linear']:\n",
    "            if col in successful.columns:\n",
    "                values = successful[col].dropna()\n",
    "                if len(values) > 0:\n",
    "                    print(f\"{col:8s}: {values.mean():.4f} ¬± {values.std():.4f}\")\n",
    "    \n",
    "    print(f\"\\nTotal time: {elapsed_time:.1f} seconds\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Validation failed: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cell 4 - performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4 - Performance Analysis\n",
    "\n",
    "if 'results_df' in locals() and len(results_df) > 0:\n",
    "    \n",
    "    # Model comparison\n",
    "    models = ['naive', 'sarima', 'sarimax']\n",
    "    comparison = {}\n",
    "    \n",
    "    for model in models:\n",
    "        if model in results_df.columns:\n",
    "            valid_results = results_df[model].dropna()\n",
    "            if len(valid_results) > 0:\n",
    "                comparison[model] = {\n",
    "                    'mean': valid_results.mean(),\n",
    "                    'std': valid_results.std(),\n",
    "                    'count': len(valid_results)\n",
    "                }\n",
    "    \n",
    "    # Calculate improvements\n",
    "    if 'naive' in comparison and 'sarimax' in comparison:\n",
    "        improvement = (comparison['naive']['mean'] - comparison['sarimax']['mean']) / comparison['naive']['mean'] * 100\n",
    "        print(f\"SARIMAX vs Naive: {improvement:+.1f}% improvement\")\n",
    "    \n",
    "    if 'sarima' in comparison and 'sarimax' in comparison:\n",
    "        improvement = (comparison['sarima']['mean'] - comparison['sarimax']['mean']) / comparison['sarima']['mean'] * 100\n",
    "        print(f\"SARIMAX vs SARIMA: {improvement:+.1f}% improvement\")\n",
    "    \n",
    "    # Best model recommendation\n",
    "    if comparison:\n",
    "        best_model = min(comparison.keys(), key=lambda x: comparison[x]['mean'])\n",
    "        print(f\"Best model: {best_model.upper()} (RMSE: {comparison[best_model]['mean']:.4f})\")\n",
    "        \n",
    "        # Show trend over time\n",
    "        if len(results_df) > 5:\n",
    "            sarimax_results = results_df['sarimax'].dropna()\n",
    "            if len(sarimax_results) > 3:\n",
    "                recent_performance = sarimax_results.tail(3).mean()\n",
    "                early_performance = sarimax_results.head(3).mean()\n",
    "                trend = \"improving\" if recent_performance < early_performance else \"stable/degrading\"\n",
    "                print(f\"Performance trend: {trend}\")\n",
    "\n",
    "else:\n",
    "    print(\"No validation results to analyze\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
