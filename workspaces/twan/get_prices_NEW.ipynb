{
 "cells": [
  {
   "cell_type": "code",
<<<<<<< HEAD:workspaces/twan/get_prices.ipynb
=======
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
<<<<<<< HEAD
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Next fetch scheduled at: 2025-04-01T18:00:05.000018+00:00\n",
      "Status Code: 200\n",
      "CSV file 'oxygent_data/prices_20250401_1800.CSV' integrity check passed.\n",
      "Next fetch scheduled at: 2025-04-02T00:00:05.000048+00:00\n",
      "Status Code: 200\n",
      "CSV file 'oxygent_data/prices_20250402_0000.CSV' integrity check passed.\n",
      "Next fetch scheduled at: 2025-04-02T06:00:05.000025+00:00\n",
      "Status Code: 200\n",
      "CSV file 'oxygent_data/prices_20250402_0600.CSV' integrity check passed.\n",
      "Next fetch scheduled at: 2025-04-02T12:00:05.000028+00:00\n"
=======
     "ename": "SyntaxError",
     "evalue": "unterminated string literal (detected at line 2) (2283835801.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[20], line 2\u001b[0;36m\u001b[0m\n\u001b[0;31m    '# fetching 4x daily, at 1  & 7 AM/PM, exactly.\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unterminated string literal (detected at line 2)\n"
>>>>>>> 5210027a7358a072ad427391d7187e56de6f40d1
     ]
    }
   ],
   "source": [
<<<<<<< HEAD
    "# fetching 4x daily, at 2  & 8 AM/PM, exactly.\n",
    "# data is no longer appended into new CSV file due to issues\n",
=======
    "# code gave errors, reverting to preceding code, without fetching on exactly right hour. files of  26 or 27 March should be used to redevelop code'\n",
    "'# fetching 4x daily, at 1  & 7 AM/PM, exactly.\n",
    "# data is appended into new CSV file\n",
>>>>>>> 5210027a7358a072ad427391d7187e56de6f40d1
    "import os\n",
    "import requests\n",
    "import csv\n",
    "from datetime import datetime, timezone, timedelta\n",
    "import time\n",
    "\n",
    "# Create a session with a browser-like User-Agent\n",
    "session = requests.Session()\n",
    "session.headers.update({\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/110.0.0.0 Safari/537.36\"\n",
    "})\n",
    "\n",
    "def fetch_data_from_api(url):\n",
    "    try:\n",
    "        response = session.get(url, allow_redirects=True)\n",
    "        print(f\"Status Code: {response.status_code}\")  # Debugging output\n",
    "        if response.status_code == 200:\n",
    "            return response.json()\n",
    "        else:\n",
    "            print(f\"Failed to fetch data. Status: {response.status_code}\\nResponse Text: {response.text}\")\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(f\"Request failed: {e}\")\n",
    "        return None\n",
    "\n",
    "def add_timestamp_to_data(data, timestamp):\n",
    "    if data:\n",
    "        for subarray_index, subarray in enumerate(data):\n",
    "            for entry in subarray:\n",
    "                entry['timestamp'] = timestamp\n",
    "                entry['subarray'] = subarray_index\n",
    "        return data\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def save_to_csv(data, file_path):\n",
    "    # Check if file exists to determine if header should be written\n",
    "    file_exists = os.path.isfile(file_path)\n",
    "    with open(file_path, 'a', newline='') as csvfile:\n",
    "        fieldnames = ['x', 'y', 'timestamp', 'subarray']\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "        if not file_exists:\n",
    "            writer.writeheader()\n",
    "        for subarray in data:\n",
    "            writer.writerows(subarray)\n",
    "\n",
    "def check_integrity(data, file_path):\n",
    "    try:\n",
    "        with open(file_path, 'r') as csvfile:\n",
    "            csv_reader = csv.DictReader(csvfile)\n",
    "            csv_data = list(csv_reader)\n",
    "            original_row_count = sum(len(subarray) for subarray in data)\n",
    "            csv_row_count = len(csv_data)\n",
    "            if original_row_count == csv_row_count:\n",
    "                print(f\"CSV file '{file_path}' integrity check passed.\")\n",
    "            else:\n",
    "                print(f\"CSV file '{file_path}' integrity check failed. Expected {original_row_count} rows, found {csv_row_count}.\")\n",
    "def seconds_until_next_schedule():\n",
    "    \"\"\"Calculate seconds until the next scheduled fetch time at 12 AM, 6 AM, 12 PM, and 6 PM UTC.\"\"\"\n",
    "    now = datetime.now(timezone.utc)\n",
<<<<<<< HEAD
    "    schedule_hours = [0, 6, 12, 18]  # Corresponds to 112 AM, 6 AM, 12 PM, and 6 PM UTC\n",
=======
    "    schedule_hours = [1, 7, 13, 19, 20]  # Corresponds to 1 AM, 7 AM, 1 PM, 7 & 8 PM UTC\n",
    "    \n",
>>>>>>> 5210027a7358a072ad427391d7187e56de6f40d1
    "    # Determine the next scheduled hour today\n",
    "    next_time = None\n",
    "    min_seconds = float('inf')\n",
    "    \n",
    "    for h in schedule_hours:\n",
    "        next_run = now.replace(hour=h, minute=0, second=0, microsecond=0)\n",
<<<<<<< HEAD
    "        if next_run > now:\n",
    "            return (next_run - now).total_seconds()\n",
    "    # If all scheduled times today have passed, schedule for the first time tomorrow.\n",
    "    next_run = now.replace(hour=schedule_hours[0], minute=0, second=0, microsecond=0) + timedelta(days=1)\n",
    "    return (next_run - now).total_seconds()\n",
    "\n",
    "api_url = 'https://energie.theoxygent.nl/api/prices_v2.php'\n",
    "\n",
    "# Create the subfolder if it doesn't exist\n",
    "subfolder = 'oxygent_data'\n",
    "if not os.path.exists(subfolder):\n",
    "    os.makedirs(subfolder)\n",
    "\n",
    "while True:\n",
    "    # Calculate delay until the next scheduled fetch time\n",
    "    delay = seconds_until_next_schedule() + 5\n",
    "    next_run_time = datetime.now(timezone.utc) + timedelta(seconds=delay)\n",
    "    print(f\"Next fetch scheduled at: {next_run_time.isoformat()}\")\n",
    "    time.sleep(delay)\n",
    "\n",
    "    current_time_gmt = datetime.now(timezone.utc)\n",
    "    timestamp_str = current_time_gmt.strftime(\"%Y%m%d_%H%M\")\n",
    "    # Create an individual CSV file per fetch with UTC timestamp in its name\n",
    "    csv_file_path = os.path.join(subfolder, f'prices_{timestamp_str}.CSV')\n",
    "\n",
    "    time_series_data = fetch_data_from_api(api_url)\n",
    "    if time_series_data:\n",
    "        current_time_gmt_iso = current_time_gmt.isoformat()\n",
    "        time_series_data_with_timestamp = add_timestamp_to_data(time_series_data, current_time_gmt_iso)\n",
    "        # Save data to a new CSV file, overwriting if the file already exists\n",
    "        with open(csv_file_path, 'w', newline='') as csvfile:\n",
    "            fieldnames = ['x', 'y', 'timestamp', 'subarray']\n",
    "            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "            writer.writeheader()\n",
    "            for subarray in time_series_data_with_timestamp:\n",
    "                writer.writerows(subarray)\n",
    "        # Add a short delay to ensure the file is completely written before integrity check\n",
    "        time.sleep(1)\n",
    "        check_integrity(time_series_data_with_timestamp, csv_file_path)\n",
    "    else:\n",
    "        print(\"No data fetched; skipping CSV creation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status Code: 200\n",
      "CSV file 'oxygent_data/prices_20250401_1733.CSV' integrity check passed.\n"
     ]
    }
   ],
   "source": [
    "# fetching MANUALLY\n",
    "# data is no longer appended into new CSV file due to issues\n",
    "import os\n",
    "import requests\n",
    "import csv\n",
    "from datetime import datetime, timezone, timedelta\n",
    "import time\n",
    "\n",
    "# Create a session with a browser-like User-Agent\n",
    "session = requests.Session()\n",
    "session.headers.update({\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/110.0.0.0 Safari/537.36\"\n",
    "})\n",
    "\n",
    "def fetch_data_from_api(url):\n",
    "    try:\n",
    "        response = session.get(url, allow_redirects=True)\n",
    "        print(f\"Status Code: {response.status_code}\")  # Debugging output\n",
    "        if response.status_code == 200:\n",
    "            return response.json()\n",
    "        else:\n",
    "            print(f\"Failed to fetch data. Status: {response.status_code}\\nResponse Text: {response.text}\")\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(f\"Request failed: {e}\")\n",
    "        return None\n",
    "\n",
    "def add_timestamp_to_data(data, timestamp):\n",
    "    if data:\n",
    "        for subarray_index, subarray in enumerate(data):\n",
    "            for entry in subarray:\n",
    "                entry['timestamp'] = timestamp\n",
    "                entry['subarray'] = subarray_index\n",
    "        return data\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def save_to_csv(data, file_path):\n",
    "    # Check if file exists to determine if header should be written\n",
    "    file_exists = os.path.isfile(file_path)\n",
    "    with open(file_path, 'a', newline='') as csvfile:\n",
    "        fieldnames = ['x', 'y', 'timestamp', 'subarray']\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "        if not file_exists:\n",
    "            writer.writeheader()\n",
    "        for subarray in data:\n",
    "            writer.writerows(subarray)\n",
    "\n",
    "def check_integrity(data, file_path):\n",
    "    with open(file_path, 'r') as csvfile:\n",
    "        csv_reader = csv.DictReader(csvfile)\n",
    "        csv_data = list(csv_reader)\n",
    "        original_row_count = sum(len(subarray) for subarray in data)\n",
    "        csv_row_count = len(csv_data)\n",
    "        if original_row_count == csv_row_count:\n",
    "            print(f\"CSV file '{file_path}' integrity check passed.\")\n",
    "        else:\n",
    "            print(f\"CSV file '{file_path}' integrity check failed.\")\n",
    "\n",
    "def seconds_until_next_schedule():\n",
    "    \"\"\"Direct fetching: no wait time.\"\"\"\n",
    "    return 0\n",
    "\n",
    "api_url = 'https://energie.theoxygent.nl/api/prices_v2.php'\n",
    "\n",
    "# Create the subfolder if it doesn't exist\n",
    "subfolder = 'oxygent_data'\n",
    "if not os.path.exists(subfolder):\n",
    "    os.makedirs(subfolder)\n",
    "\n",
    "\n",
    "current_time_gmt = datetime.now(timezone.utc).isoformat()\n",
    "timestamp_str = datetime.now(timezone.utc).strftime(\"%Y%m%d_%H%M\")\n",
    "time_series_data = fetch_data_from_api(api_url)\n",
    "if time_series_data:\n",
    "    time_series_data_with_timestamp = add_timestamp_to_data(time_series_data, current_time_gmt)\n",
    "    # Create a fetch-specific CSV file using the current timestamp\n",
    "    fetch_csv_file_path = os.path.join(subfolder, f'prices_{timestamp_str}.CSV')\n",
    "    save_to_csv(time_series_data_with_timestamp, fetch_csv_file_path)\n",
    "    # Add a short delay to ensure the file is completely written before integrity check\n",
    "    time.sleep(1)\n",
    "    check_integrity(time_series_data_with_timestamp, fetch_csv_file_path)\n",
    "else:\n",
    "    print(\"No data fetched; skipping CSV update.\")\n"
=======
    "        if next_run <= now:  # If this time has already passed today\n",
    "            next_run += timedelta(days=1)  # Try again tomorrow\n",
    "        \n",
    "        seconds = (next_run - now).total_seconds()\n",
    "        if seconds < min_seconds:\n",
    "            min_seconds = seconds\n",
    "            next_time = next_run\n",
    "    \n",
    "    print(f\"Scheduling next run at {next_time.isoformat()}\")\n",
    "    return min_seconds\n",
    "while True:\n",
    "    try:\n",
    "while True:\n",
    "    try:\n",
    "        # Calculate delay until the next scheduled fetch time\n",
    "        delay = seconds_until_next_schedule() + 5\n",
    "        next_run_time = datetime.now(timezone.utc) + timedelta(seconds=delay)\n",
    "        print(f\"Next fetch scheduled at: {next_run_time.isoformat()}\")\n",
    "        \n",
    "        # Sleep in smaller intervals to allow for keyboard interruption\n",
    "        remaining_delay = delay\n",
    "        while remaining_delay > 0:\n",
    "            sleep_interval = min(60, remaining_delay)  # Sleep at most 60 seconds at a time\n",
    "            time.sleep(sleep_interval)\n",
    "            remaining_delay -= sleep_interval\n",
    "            \n",
    "        current_time_gmt = datetime.now(timezone.utc).isoformat()\n",
    "        print(f\"Fetching data at {current_time_gmt}\")\n",
    "        time_series_data = fetch_data_from_api(api_url)\n",
    "        \n",
    "        if time_series_data:\n",
    "            time_series_data_with_timestamp = add_timestamp_to_data(time_series_data, current_time_gmt)\n",
    "            save_to_csv(time_series_data_with_timestamp, csv_file_path)\n",
    "            # Add a short delay to ensure the file is completely written before integrity check\n",
    "            time.sleep(1)\n",
    "            check_integrity(time_series_data_with_timestamp, csv_file_path)\n",
    "        else:\n",
    "            print(\"No data fetched; skipping CSV update.\")\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"Process interrupted by user. Exiting gracefully.\")\n",
    "        break\n",
    "    except Exception as e:\n",
    "        print(f\"Error in main loop: {e}\")\n",
    "        # Wait 5 minutes before retrying after an error\n",
    "        print(\"Waiting 5 minutes before retrying...\")\n",
    "        time.sleep(300)"
>>>>>>> 5210027a7358a072ad427391d7187e56de6f40d1
   ]
  },
  {
   "cell_type": "code",
>>>>>>> ed3dfe700e957fbe59cd286cf598a33a1b94897c:workspaces/twan/get_prices_NEW.ipynb
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status Code: 200\n",
      "CSV file 'oxygent_data/time_series_data_20250404 07:01.csv' integrity check passed.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "import csv\n",
    "from datetime import datetime, timezone\n",
    "import time\n",
    "\n",
    "# Create a session with a browser-like User-Agent\n",
    "session = requests.Session()\n",
    "session.headers.update({\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/110.0.0.0 Safari/537.36\"\n",
    "})\n",
    "\n",
    "def fetch_data_from_api(url):\n",
    "    try:\n",
    "        response = session.get(url, allow_redirects=True)\n",
    "        print(f\"Status Code: {response.status_code}\")  # Debugging output\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            return response.json()\n",
    "        else:\n",
    "            print(f\"Failed to fetch data. Status: {response.status_code}\\nResponse Text: {response.text}\")\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(f\"Request failed: {e}\")\n",
    "        return None\n",
    "\n",
    "def add_timestamp_to_data(data, timestamp):\n",
    "    if data:\n",
    "        for subarray_index, subarray in enumerate(data):\n",
    "            for entry in subarray:\n",
    "                entry['timestamp'] = timestamp\n",
    "                entry['subarray'] = subarray_index\n",
    "        return data\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def save_to_csv(data, file_path):\n",
    "    if data:\n",
    "        with open(file_path, 'w', newline='') as csvfile:\n",
    "            fieldnames = ['x', 'y', 'timestamp', 'subarray']\n",
    "            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "            writer.writeheader()\n",
    "            for subarray in data:\n",
    "                writer.writerows(subarray)\n",
    "\n",
    "def check_integrity(data, file_path):\n",
    "    with open(file_path, 'r') as csvfile:\n",
    "        csv_reader = csv.DictReader(csvfile)\n",
    "        csv_data = list(csv_reader)\n",
    "        original_row_count = sum(len(subarray) for subarray in data)\n",
    "        csv_row_count = len(csv_data)\n",
    "        if original_row_count == csv_row_count:\n",
    "            print(f\"CSV file '{file_path}' integrity check passed.\")\n",
    "        else:\n",
    "            print(f\"CSV file '{file_path}' integrity check failed.\")\n",
    "\n",
    "api_url = 'https://energie.theoxygent.nl/api/prices_v2.php'\n",
    "\n",
    "# Create subfolder if it doesn't exist\n",
    "subfolder = 'oxygent_data'\n",
    "if not os.path.exists(subfolder):\n",
    "    os.makedirs(subfolder)\n",
    "\n",
    "while True:\n",
    "    current_time_gmt = datetime.now(timezone.utc).isoformat()\n",
    "    file_timestamp = datetime.now(timezone.utc).strftime(\"%Y%m%d %H:%M\")\n",
    "    time_series_data = fetch_data_from_api(api_url) \n",
    "    time_series_data_with_timestamp = add_timestamp_to_data(time_series_data, current_time_gmt)\n",
    "    csv_file_path = os.path.join(subfolder, f'time_series_data_{file_timestamp}.csv')\n",
    "    save_to_csv(time_series_data_with_timestamp, csv_file_path)\n",
    "    # Add a short delay to ensure the file is written before reading\n",
    "    time.sleep(1)\n",
    "    check_integrity(time_series_data_with_timestamp, csv_file_path)\n",
    "    # Fetch data every 4 hour\n",
    "    time.sleep(4*3600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to fetch data\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'oxygent_data/time_series_data_20250318 13:56.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 60\u001b[0m\n\u001b[1;32m     58\u001b[0m csv_file_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(subfolder, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtime_series_data_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_timestamp\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     59\u001b[0m save_to_csv(time_series_data_with_timestamp, csv_file_path)\n\u001b[0;32m---> 60\u001b[0m \u001b[43mcheck_integrity\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtime_series_data_with_timestamp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcsv_file_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;66;03m# Fetch data every hour\u001b[39;00m\n\u001b[1;32m     62\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m3600\u001b[39m)\n",
      "Cell \u001b[0;32mIn[4], line 36\u001b[0m, in \u001b[0;36mcheck_integrity\u001b[0;34m(data, file_path)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcheck_integrity\u001b[39m(data, file_path):\n\u001b[0;32m---> 36\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m csvfile:\n\u001b[1;32m     37\u001b[0m         csv_reader \u001b[38;5;241m=\u001b[39m csv\u001b[38;5;241m.\u001b[39mDictReader(csvfile)\n\u001b[1;32m     38\u001b[0m         csv_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(csv_reader)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.2/lib/python3.12/site-packages/IPython/core/interactiveshell.py:324\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    319\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    320\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    322\u001b[0m     )\n\u001b[0;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'oxygent_data/time_series_data_20250318 13:56.csv'"
     ]
    }
   ],
   "source": [
    "''' This code stopped working 18 March 2024, giving the following error message: _Status Code: 403\n",
    "Response Text: <html><body><h1>403 Forbidden</h1>\n",
    "Request forbidden by administrative rules.\n",
    "</body></html>\n",
    "\n",
    "Failed to fetch data. Status: 403_''\n",
    "''\n",
    "\n",
    "''\n",
    "import os\n",
    "import requests\n",
    "import csv\n",
    "from datetime import datetime, timezone\n",
    "import time\n",
    "\n",
    "def fetch_data_from_api(url):\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        return data\n",
    "    else:\n",
    "        print(\"Failed to fetch data\")\n",
    "        return None\n",
    "\n",
    "def add_timestamp_to_data(data, timestamp):\n",
    "    if data:\n",
    "        for subarray_index, subarray in enumerate(data):\n",
    "            for entry in subarray:\n",
    "                entry['timestamp'] = timestamp\n",
    "                entry['subarray'] = subarray_index\n",
    "        return data\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def save_to_csv(data, file_path):\n",
    "    if data:\n",
    "        with open(file_path, 'w', newline='') as csvfile:\n",
    "            fieldnames = ['x', 'y', 'timestamp', 'subarray']\n",
    "            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "            writer.writeheader()\n",
    "            for subarray in data:\n",
    "                writer.writerows(subarray)\n",
    "\n",
    "def check_integrity(data, file_path):\n",
    "    with open(file_path, 'r') as csvfile:\n",
    "        csv_reader = csv.DictReader(csvfile)\n",
    "        csv_data = list(csv_reader)\n",
    "        original_row_count = sum(len(subarray) for subarray in data)\n",
    "        csv_row_count = len(csv_data)\n",
    "        if original_row_count == csv_row_count:\n",
    "            print(f\"CSV file '{file_path}' integrity check passed.\")\n",
    "        else:\n",
    "            print(f\"CSV file '{file_path}' integrity check failed.\")\n",
    "\n",
    "api_url = 'https://energie.theoxygent.nl/api/prices_v2.php'\n",
    "\n",
    "# Create subfolder if it doesn't exist\n",
    "subfolder = 'oxygent_data'\n",
    "if not os.path.exists(subfolder):\n",
    "    os.makedirs(subfolder)\n",
    "\n",
    "while True:\n",
    "    current_time_gmt = datetime.now(timezone.utc).isoformat()\n",
    "    file_timestamp = datetime.now(timezone.utc).strftime(\"%Y%m%d %H:%M\")\n",
    "    time_series_data = fetch_data_from_api(api_url) \n",
    "    time_series_data_with_timestamp = add_timestamp_to_data(time_series_data, current_time_gmt)\n",
    "    csv_file_path = os.path.join(subfolder, f'time_series_data_{file_timestamp}.csv')\n",
    "    save_to_csv(time_series_data_with_timestamp, csv_file_path)\n",
    "    check_integrity(time_series_data_with_timestamp, csv_file_path)\n",
    "    # Fetch data every hour\n",
    "    time.sleep(3600)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
