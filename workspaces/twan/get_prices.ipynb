{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status Code: 200\n",
      "CSV file 'oxygent_data/time_series_data_20250404 07:01.csv' integrity check passed.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "import csv\n",
    "from datetime import datetime, timezone\n",
    "import time\n",
    "\n",
    "# Create a session with a browser-like User-Agent\n",
    "session = requests.Session()\n",
    "session.headers.update({\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/110.0.0.0 Safari/537.36\"\n",
    "})\n",
    "\n",
    "def fetch_data_from_api(url):\n",
    "    try:\n",
    "        response = session.get(url, allow_redirects=True)\n",
    "        print(f\"Status Code: {response.status_code}\")  # Debugging output\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            return response.json()\n",
    "        else:\n",
    "            print(f\"Failed to fetch data. Status: {response.status_code}\\nResponse Text: {response.text}\")\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(f\"Request failed: {e}\")\n",
    "        return None\n",
    "\n",
    "def add_timestamp_to_data(data, timestamp):\n",
    "    if data:\n",
    "        for subarray_index, subarray in enumerate(data):\n",
    "            for entry in subarray:\n",
    "                entry['timestamp'] = timestamp\n",
    "                entry['subarray'] = subarray_index\n",
    "        return data\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def save_to_csv(data, file_path):\n",
    "    if data:\n",
    "        with open(file_path, 'w', newline='') as csvfile:\n",
    "            fieldnames = ['x', 'y', 'timestamp', 'subarray']\n",
    "            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "            writer.writeheader()\n",
    "            for subarray in data:\n",
    "                writer.writerows(subarray)\n",
    "\n",
    "def check_integrity(data, file_path):\n",
    "    with open(file_path, 'r') as csvfile:\n",
    "        csv_reader = csv.DictReader(csvfile)\n",
    "        csv_data = list(csv_reader)\n",
    "        original_row_count = sum(len(subarray) for subarray in data)\n",
    "        csv_row_count = len(csv_data)\n",
    "        if original_row_count == csv_row_count:\n",
    "            print(f\"CSV file '{file_path}' integrity check passed.\")\n",
    "        else:\n",
    "            print(f\"CSV file '{file_path}' integrity check failed.\")\n",
    "\n",
    "api_url = 'https://energie.theoxygent.nl/api/prices_v2.php'\n",
    "\n",
    "# Create subfolder if it doesn't exist\n",
    "subfolder = 'oxygent_data'\n",
    "if not os.path.exists(subfolder):\n",
    "    os.makedirs(subfolder)\n",
    "\n",
    "while True:\n",
    "    current_time_gmt = datetime.now(timezone.utc).isoformat()\n",
    "    file_timestamp = datetime.now(timezone.utc).strftime(\"%Y%m%d %H:%M\")\n",
    "    time_series_data = fetch_data_from_api(api_url) \n",
    "    time_series_data_with_timestamp = add_timestamp_to_data(time_series_data, current_time_gmt)\n",
    "    csv_file_path = os.path.join(subfolder, f'time_series_data_{file_timestamp}.csv')\n",
    "    save_to_csv(time_series_data_with_timestamp, csv_file_path)\n",
    "    # Add a short delay to ensure the file is written before reading\n",
    "    time.sleep(1)\n",
    "    check_integrity(time_series_data_with_timestamp, csv_file_path)\n",
    "    # Fetch data every 4 hour\n",
    "    time.sleep(4*3600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to fetch data\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'oxygent_data/time_series_data_20250318 13:56.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 60\u001b[0m\n\u001b[1;32m     58\u001b[0m csv_file_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(subfolder, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtime_series_data_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_timestamp\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     59\u001b[0m save_to_csv(time_series_data_with_timestamp, csv_file_path)\n\u001b[0;32m---> 60\u001b[0m \u001b[43mcheck_integrity\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtime_series_data_with_timestamp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcsv_file_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;66;03m# Fetch data every hour\u001b[39;00m\n\u001b[1;32m     62\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m3600\u001b[39m)\n",
      "Cell \u001b[0;32mIn[4], line 36\u001b[0m, in \u001b[0;36mcheck_integrity\u001b[0;34m(data, file_path)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcheck_integrity\u001b[39m(data, file_path):\n\u001b[0;32m---> 36\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m csvfile:\n\u001b[1;32m     37\u001b[0m         csv_reader \u001b[38;5;241m=\u001b[39m csv\u001b[38;5;241m.\u001b[39mDictReader(csvfile)\n\u001b[1;32m     38\u001b[0m         csv_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(csv_reader)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.2/lib/python3.12/site-packages/IPython/core/interactiveshell.py:324\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    319\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    320\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    322\u001b[0m     )\n\u001b[0;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'oxygent_data/time_series_data_20250318 13:56.csv'"
     ]
    }
   ],
   "source": [
    "''' This code stopped working 18 March 2024, giving the following error message: _Status Code: 403\n",
    "Response Text: <html><body><h1>403 Forbidden</h1>\n",
    "Request forbidden by administrative rules.\n",
    "</body></html>\n",
    "\n",
    "Failed to fetch data. Status: 403_''\n",
    "''\n",
    "\n",
    "''\n",
    "import os\n",
    "import requests\n",
    "import csv\n",
    "from datetime import datetime, timezone\n",
    "import time\n",
    "\n",
    "def fetch_data_from_api(url):\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        return data\n",
    "    else:\n",
    "        print(\"Failed to fetch data\")\n",
    "        return None\n",
    "\n",
    "def add_timestamp_to_data(data, timestamp):\n",
    "    if data:\n",
    "        for subarray_index, subarray in enumerate(data):\n",
    "            for entry in subarray:\n",
    "                entry['timestamp'] = timestamp\n",
    "                entry['subarray'] = subarray_index\n",
    "        return data\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def save_to_csv(data, file_path):\n",
    "    if data:\n",
    "        with open(file_path, 'w', newline='') as csvfile:\n",
    "            fieldnames = ['x', 'y', 'timestamp', 'subarray']\n",
    "            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "            writer.writeheader()\n",
    "            for subarray in data:\n",
    "                writer.writerows(subarray)\n",
    "\n",
    "def check_integrity(data, file_path):\n",
    "    with open(file_path, 'r') as csvfile:\n",
    "        csv_reader = csv.DictReader(csvfile)\n",
    "        csv_data = list(csv_reader)\n",
    "        original_row_count = sum(len(subarray) for subarray in data)\n",
    "        csv_row_count = len(csv_data)\n",
    "        if original_row_count == csv_row_count:\n",
    "            print(f\"CSV file '{file_path}' integrity check passed.\")\n",
    "        else:\n",
    "            print(f\"CSV file '{file_path}' integrity check failed.\")\n",
    "\n",
    "api_url = 'https://energie.theoxygent.nl/api/prices_v2.php'\n",
    "\n",
    "# Create subfolder if it doesn't exist\n",
    "subfolder = 'oxygent_data'\n",
    "if not os.path.exists(subfolder):\n",
    "    os.makedirs(subfolder)\n",
    "\n",
    "while True:\n",
    "    current_time_gmt = datetime.now(timezone.utc).isoformat()\n",
    "    file_timestamp = datetime.now(timezone.utc).strftime(\"%Y%m%d %H:%M\")\n",
    "    time_series_data = fetch_data_from_api(api_url) \n",
    "    time_series_data_with_timestamp = add_timestamp_to_data(time_series_data, current_time_gmt)\n",
    "    csv_file_path = os.path.join(subfolder, f'time_series_data_{file_timestamp}.csv')\n",
    "    save_to_csv(time_series_data_with_timestamp, csv_file_path)\n",
    "    check_integrity(time_series_data_with_timestamp, csv_file_path)\n",
    "    # Fetch data every hour\n",
    "    time.sleep(3600)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
