# env - model-run-tf_env Python 3.10.16

import numpy as np
import pandas as pd
from scipy.stats import norm, gamma
import matplotlib.pyplot as plt
import scipy.stats as stats
from matplotlib.ticker import MaxNLocator # To ensure demand axis are integer.

from sklearn.model_selection import train_test_split
from sklearn.model_selection import TimeSeriesSplit
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

from prophet import Prophet
from prophet.diagnostics import cross_validation, performance_metrics
from prophet.diagnostics import cross_validation
from prophet.plot import plot_cross_validation_metric


import polars as pl

import os
from datetime import datetime
import time

import sqlite3

from pathlib import Path
import logging
import json


logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger('model_run_prophet')

PROJECT_ROOT = Path(__file__).resolve().parents[2]
CONFIG_PATH = PROJECT_ROOT / "sandeep" / "config" / "config.json"

# === CONFIG ===

if not CONFIG_PATH.exists():
    raise FileNotFoundError(f"âŒ Config not found at : {CONFIG_PATH}")
with open(CONFIG_PATH, "r") as f:
    config = json.load(f)

MODEL_RUN_RESULTS_DIR = config['ned']['ned_model_download_dir']

# Custom function for MAPE and sMAPE
def mean_absolute_percentage_error(y_true, y_pred): 
    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100

def symmetric_mape(y_true, y_pred):
    return np.mean(2 * np.abs(y_true - y_pred) / (np.abs(y_true) + np.abs(y_pred))) * 100

# Function to compute AIC for regression models
def compute_aic(y_true, y_pred, num_params):
    residuals = y_true - y_pred
    mse = np.mean(residuals**2)
    n = len(y_true)
    aic = n * np.log(mse) + 2 * num_params  # AIC formula
    return aic


# Connect to the SQLite database
db_path = '/Users/sgawde/work/eaisi-code/main-branch-11-may/ENEXIS/src/data/WARP.db'
conn = sqlite3.connect(db_path)
# Connect to the SQLite database using the existing db_path
conn = sqlite3.connect(db_path)
# Step 2: Read data from table
df_pd_orig = pd.read_sql_query("SELECT * FROM master_warp ORDER BY datetime DESC", conn)

# df_pd_orig = pd.read_sql_query("SELECT * FROM raw_entsoe_obs ORDER BY Timestamp DESC", conn)
#df_pd_orig["datetime"] = df_pd_orig["Timestamp"]
# Step 3: Close the connection
conn.close()

# Step 1: Convert 'validto' column to datetime
df_pd_orig['datetime'] = pd.to_datetime(df_pd_orig['datetime'])
# Step 2: Sort the DataFrame by 'validto' to avoid data leakage
df = df_pd_orig.sort_values(by='datetime')

# Step 1: Filter out negative Price and prepare data
# df = df[df['Price'] > 0].copy()                   # Keep only rows with positive Price
df['datetime'] = pd.to_datetime(df['datetime'])   # Ensure datetime column is datetime type
df['datetime'] = df['datetime'].dt.tz_localize(None)  # Remove timezone info if present

# Step 2: Define target and features
y = df[['datetime', 'Price']]                     # Target with timestamp
X = df.drop(columns=['Price'])                    # All other features, including datetime

# Step 3: Time Series Split
tscv = TimeSeriesSplit(n_splits=5)
for train_index, test_index in tscv.split(X):
    X_train, X_test = X.iloc[train_index], X.iloc[test_index]
    y_train, y_test = y.iloc[train_index], y.iloc[test_index]

# Print train/test datetime range
print("Train Date Range:")
print(f"Start: {X_train['datetime'].min()}")
print(f"End:   {X_train['datetime'].max()}")

print("\nTest Date Range:")
print(f"Start: {X_test['datetime'].min()}")
print(f"End:   {X_test['datetime'].max()}")

# Step 4: Merge X and y for Prophet input
train_prophet = pd.concat([y_train.reset_index(drop=True), 
                           X_train.drop(columns=['datetime']).reset_index(drop=True)], axis=1)
test_prophet = pd.concat([y_test.reset_index(drop=True), 
                          X_test.drop(columns=['datetime']).reset_index(drop=True)], axis=1)

# Step 5: Rename for Prophet
train_prophet.rename(columns={'datetime': 'ds', 'Price': 'y'}, inplace=True)
test_prophet.rename(columns={'datetime': 'ds', 'Price': 'y'}, inplace=True)

# Step 6: Ensure datetime has no timezone
train_prophet['ds'] = pd.to_datetime(train_prophet['ds']).dt.tz_localize(None)
test_prophet['ds'] = pd.to_datetime(test_prophet['ds']).dt.tz_localize(None)

# (Optional) Show the head of the training DataFrame for verification
print("\nProphet Training Data:")
print(train_prophet.head())

"""

# Parameter grid
param_grid = {
    'changepoint_prior_scale': [0.001, 0.01, 0.1, 0.5],
    'seasonality_mode': ['additive', 'multiplicative'],
    'seasonality_prior_scale': [1.0, 10.0, 20.0]
}
"""

# Best Model Param grid
param_grid = {
    'changepoint_prior_scale': [0.001],
    'seasonality_mode': ['additive'],
    'seasonality_prior_scale': [20.0]
}

# Create list of all parameter combinations
import itertools
all_params = [dict(zip(param_grid.keys(), v)) for v in itertools.product(*param_grid.values())]

best_params = None
best_mae = float('inf')

# Step 4: Train Prophet model
model_run_start_time = time.time()

for params in all_params:
    model = Prophet(
        changepoint_prior_scale=params['changepoint_prior_scale'],
        seasonality_mode=params['seasonality_mode'],
        seasonality_prior_scale=params['seasonality_prior_scale']
    )
    
    model.fit(train_prophet)
    
    forecast = model.predict(test_prophet[['ds']])
    
    y_true = test_prophet['y'].values
    y_pred = forecast['yhat'].values

    mae = mean_absolute_error(y_true, y_pred)

    print(f"Params: {params} â†’ MAE: {mae:.2f}")
    
    if mae < best_mae:
        best_mae = mae
        best_params = params

print("\nâœ… Best Parameters Found:")
print(best_params)
print(f"Best MAE: {best_mae:.2f}")

print("âœ… Forecast complete")
model_run_end_time = time.time()


# Step 6: Evaluation
y_true = test_prophet['y'].values
y_pred = forecast['yhat'].values
diff = y_true - y_pred

# Step 7: Plot Forecast
model.plot(forecast)
plt.title("Prophet Forecast")
plt.xlabel("Date")
plt.ylabel("Predicted Price")
plt.tight_layout()
plt.show()

# Step 8: Plot Forecast Components
model.plot_components(forecast)
plt.tight_layout()
plt.show()

# Step 9: Create Polars DataFrame for comparison
x_values = X_test['validto'].to_numpy() if 'validto' in X_test.columns else X_test.index.to_numpy()
df_pred = pl.DataFrame({
    "X_Values": x_values,
    "Actual": y_true,
    "Predicted": y_pred,
    "Diff": diff
})

# Step 10: Optional custom visualization
plt.figure(figsize=(10, 5))
plt.plot(test_prophet['ds'], y_true, label='Actual Price')
plt.plot(test_prophet['ds'], y_pred, label='Predicted Price')
plt.xlabel('Date')
plt.ylabel('Price')
plt.title('Actual vs Predicted Price')
plt.legend()
plt.tight_layout()
plt.show()


# Final: Print execution time
print(f"â±ï¸ Execution time: {model_run_end_time - model_run_start_time:.2f} seconds")

# Step 11: Evaluation Metrics
mae = mean_absolute_error(y_true, y_pred)
mse = mean_squared_error(y_true, y_pred)
rmse = np.sqrt(mse)
# mape = np.mean(np.abs((y_true - y_pred) / np.clip(y_true, a_min=1e-10, a_max=None))) * 100
# smape = np.mean(2 * np.abs(y_true - y_pred) / (np.abs(y_true) + np.abs(y_pred))) * 100

print("\nðŸ“Š Evaluation Metrics:")

r2 = r2_score(y_true, y_pred)

model_name = "Prophet"
print("model_name", "Prophet")
print(f"MAE   : {mae:.2f}")
print(f"MSE   : {mse:.2f}")
print(f"RMSE  : {rmse:.2f}")
print(f"RÂ²    : {r2:.4f}")

# print(f"MAPE  : {mape:.2f}%")
# print(f"sMAPE : {smape:.2f}%")

# Define the filename
model_run_timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')

results = []
model_name = "Prophet"
comments = "All data with hyperparameter tuning Model Run"
print("model_name", "Prophet")

model_execution_time = model_run_end_time - model_run_start_time

# Append results to results
results.append(["Prophet", mae, mse, rmse, r2, comments, model_execution_time, model_run_timestamp])

# Convert results to DataFrame
metrics_df = pd.DataFrame(results, columns=["Model", "MAE", "MSE", "RMSE", "R2", "Comments", "Execution Time", "Run At"])

# Display Table
print(metrics_df)


model_results_file_path = f'{MODEL_RUN_RESULTS_DIR}warp-prophet-model-results.csv'

# Step 10: Check if file exists, then append or create
if os.path.exists(model_results_file_path):
    # Append to existing file
    existing_results = pd.read_csv(model_results_file_path)
    updated_results = pd.concat([existing_results, metrics_df], ignore_index=True)
    updated_results.to_csv(model_results_file_path, index=False)
else:
    # Create new file
    metrics_df.to_csv(model_results_file_path, index=False)

print(f"âœ… Model evaluation saved to {model_results_file_path}")